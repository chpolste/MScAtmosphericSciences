Formulate retrieval problem.

Introduce optimal estimation and regression mention Kernel Methods, ... as
alternative regression methods.

Math is heavily influenced by Rodgers and Bishop, important calculations are
repeated to achieve consistent notation and naming conventions throughout.

\startsection[title={Bayesian Statistics}]

    The Bayesian view of statistics is ideal for the formulation of retrieval
    problems. 

    Conditional probability, diachronic interpreation, subjective priors,
    making assumptions explicit, focus on pdf from which values can be
    extracted due to different criteria, pdf carries uncertainty estimates,
    sampling from pdfs...

    Central role: Bayes' theorem.
    
    \placeformula[eq:bayes_theorem]
    \startformula
        P(x|y) = \frac{P(y|x) P(x)}{P(y)}
    \stopformula

    $P(x|y)$ likelihood

    $P(y|x)$ posterior

    $P(x)$ prior

    $P(y)$ normalization, can be obtained by integrating $P(y|x)P(x)$
    over all possible states $x$.

    \placefigure[bottom][fig:bayes_theorem]{Visualization of Bayes' theorem.} {\externalfigure[bayes_theorem][width=\textwidth]}

\stopsection

\startsection[title=The Multivariate Gaussian Distribution]

    Bayesian view relies heavily on probablility density functions. In a
    theoretical framework it is desireable to work with pdfs that allow
    analytic solutions for a given problem. The most commonly used pdf is the
    Gaussian distribution:

    \placeformula[eq:gaussian]
    \startformula
        \GAUSS{\VECX}{\MEANVEC}{\COVMAT}
        = \frac{1}{(2 \pi)^{N/2} \det(\COVMAT)^{1/2}}
          \exp \left( -\frac{1}{2}(\VECX - \MEANVEC)^\top \COVMAT^{-1} (\VECX - \MEANVEC) \right)
    \stopformula

    where $\VECX, \MEANVEC \in \REALS^N$ and $\COVMAT \in \REALS^{N
    \times N}$ and the exponenial function is applied component-wise.

    The assumption that a continuous variable follows a Gaussian distribution
    is not only popular due to its well understood analytic behavior. central
    limit theorem, maximum entropy.

    The mean vector $\MEANVEC$ of the Gaussian distribution is the
    location of its maximum. The covariance matrix $\COVMAT$ determines
    the shape of the distribution. For a proper Gaussian distribution, it is
    always a symmetric positive definite matrix. When all off-diagonal elements
    of $\COVMAT$ are zero, each component of the multidimensional
    distribution is independent from the others. If nonzero off-diagonal
    elements exist, some or all componentes are correlated.

    Refer to default Bayes/Gaussian plot.

    Because the normalization $((2 \pi)^{N/2} \det(\COVMAT)^{1/2})^{-1}$ can
    be determined from the content of the quadratic form in the exponent,
    i.e. the size of the input $N$ and the covariance matrix $\COVMAT$, a
    Gaussian distribution can be described completely in terms of the exponent.
    A common representation of a Gaussian distribution, that will also be used
    in the following, is

    \startformula
        -2 \ln(\GAUSS{\VECX}{\MEANVEC}{\COVMAT})
        = (\VECX - \MEANVEC)^\top \COVMAT^{-1} (\VECX - \MEANVEC) + c \EQCOMMA
    \stopformula

    where $c$ is a constant that contains the normalization. The right hand
    side of this representation is easier to deal with in analytical
    derivations but contains all the information of the distribution.

    Because the assumption of Gaussian {\PDF}s is persistent throughout this
    text, some general results are derived in the following sections. These
    will be used in subsequent sections.

    \startsubsection[title=Maximum Likelihood Estimators for the Parameters]

    \stopsubsection

    \startsubsection[title={Bayes' Theorem for Gaussians},reference=ch:bayes_gauss]

        A convenient property of Gaussian distributions is that the product of
        a Gaussian conditional distribution $\LIKELIHOOD$ (the likelihood
        function) and a Gaussian marginal distribution $\PRIOR$ (the prior) is
        again Gaussian \cite[authoryears][Bishop2006]. This allows the analytic
        calculation of the posterior $\POSTERIOR$ if two assumptions hold: the
        dependence of the conditional distribution's mean is linear with
        respect to the prior's argument ($\VECX$) and its covariance is
        independent of $\VECX$. Let

        \startformula
        \startalign[n=2,align{right,left}]
            \NC -2 \ln(\LIKELIHOOD) = \NC
                \GAUSSEXP{\VECY}{\FWDJAC\VECX - \VECB}{\COVMATERR} + c_1 \NR
            \NC -2 \ln(\PRIOR) = \NC
                \GAUSSEXP{\VECX}{\MEANVECA}{\COVMATA} + c_2 \EQSTOP \NR
        \stopalign
        \stopformula

        The normalization is contained in $c_1$ and $c_2$ respectively. These
        {\PDF}s are multivariate Gaussians fulfilling the above assumptions.
        The posterior distribution is given by:

        \placesubformula
        \startformula
        \startalign[n=2, align{right,left}]
            \NC -2 \ln(\POSTERIOR) = \NC -2 \ln(\LIKELIHOOD \PRIOR) \NR
            \NC = \NC -2 \ln(\LIKELIHOOD) - 2 \ln(\PRIOR) \NR
            \NC = \NC
                \GAUSSEXP{\VECY}{\FWDJAC - \VECB}{\COVMATERR}
                + \GAUSSEXP{\VECX}{\MEANVECX}{\COVMATA} + c_1 + c_2
                \NR
            \NC = \NC
                  \VECY^\top \COVMATERR^{-1} \VECY
                - \VECY^\top \COVMATERR^{-1} \FWDJAC \VECX
                - \VECY^\top \COVMATERR^{-1} \VECB
                \NR
            \NC \NC
                \mathbox{cornflowerblue}{- \VECX^\top \FWDJAC^\top \COVMATERR^{-1} \VECY}
                + \mathbox{yellowgreen}{\VECX^\top \FWDJAC^\top \COVMATERR^{-1} \FWDJAC \VECX}
                + \mathbox{cornflowerblue}{\VECX^\top \FWDJAC^\top \COVMATERR^{-1} \VECB}
                \NR
            \NC \NC
                - \VECB^\top \COVMATERR^{-1} \VECY
                + \VECB^\top \COVMATERR^{-1} \FWDJAC \VECX
                + \VECB^\top \COVMATERR^{-1} \VECB
                \NR[eq:gaussprodexpand]
            \NC \NC
                + \mathbox{yellowgreen}{\VECX^\top \COVMATA^{-1} \VECX}
                ~\mathbox{cornflowerblue}{- \VECX^\top \COVMATA^{-1} \MEANVECA}
                - \MEANVECA^\top \COVMATA^{-1} \VECX
                + \MEANVECA^\top \COVMATA^{-1} \MEANVECX
                \NR
            \NC \NC + c_1 + c_2 \EQCOMMA \NR
        \stopalign
        \stopformula
        
        where the last step is expanding the vector-matrix-vector product.
        Subscripts are chosen in anticipation of chapter
        \in[ch:optimalestimation]. The posterior has to be Gaussian, therefore

        \placesubformula
        \startformula
        \startalign[n=2,align={right,left}]
            \NC -2 \ln(\POSTERIOR) = \NC
                \GAUSSEXP{\VECX}{\MEANVECX}{\COVMATX} + c_3 \NR
            \NC = \NC
                 \mathbox{yellowgreen}{\VECX^\top \COVMATX^{-1} \VECX}
                 ~\mathbox{cornflowerblue}{- \VECX^\top \COVMATX^{-1} \MEANVECX}
                 - \MEANVECX^\top \COVMATX^{-1} \VECX
                 + \MEANVECX^\top \COVMATX^{-1} \MEANVECX
                 + c_3 \NR[eq:gausspostexpand]
        \stopalign
        \stopformula

        must be a equivalent representation. Noting that the equivalence must
        hold for all $\VECX$, an expression for $\COVMATX$ is foundy by
        matching terms quadratic in $\VECX$ of both forms
        \ineq{eq:gaussprodexpand} and \ineq{eq:gausspostexpand} (highlighted
        green):

        \placesubformula
        \startformula
        \startalign[n=3,align={left,right,left}]
            \NC \NC - \VECX^\top \COVMATX^{-1} \VECX = \NC
                \VECX^\top \FWDJAC^\top \COVMATERR^{-1} \FWDJAC \VECX
                + \VECX^\top \COVMATA^{-1} \VECX
             = \VECX^\top (\FWDJAC^\top \COVMATERR^{-1} \FWDJAC
                + \COVMATA^{-1}) \VECX \NR
            \NC \Rightarrow~~ \NC \COVMATX = \NC
                (\FWDJAC^\top \COVMATERR^{-1} \FWDJAC + \COVMATA^{-1})^{-1}
                \EQSTOP \NR[eq:gausspostcov]
        \stopalign
        \stopformula

        Matching the linear terms involving $\VECX^\top$ of both forms
        \ineq{eq:gaussprodexpand} and \ineq{eq:gausspostexpand} (highlighted
        blue) results in an expression for $\MEANVECX$:

        \placesubformula
        \startformula
        \startalign[n=3,align={left,right,left}]
            \NC \NC - \VECX^\top \COVMATX^{-1} \MEANVECX = \NC
                - \VECX^\top \FWDJAC^\top \COVMATERR^{-1} \VECY
                + \VECX^\top \FWDJAC^\top \COVMATERR^{-1} \VECB
                - \VECX^\top \COVMATA^{-1} \MEANVECA \NR
            \NC \NC  = \NC
                - \VECX^\top (\FWDJAC^\top \COVMATERR^{-1} (\VECY - \VECB)
                + \COVMATA^{-1} \MEANVECA) \NR
            \NC \Rightarrow~~ \NC \MEANVECX = \NC
                \COVMATX (\FWDJAC^\top \COVMATERR^{-1} (\VECY - \VECB)
                + \COVMATA^{-1} \MEANVECA)
                \EQCOMMA \NR[eq:gausspostmean]
        \stopalign
        \stopformula

        where $\COVMATX$ is given by \ineq{eq:gausspostcov}. Equating terms
        linear in $\VECX$ yields the same result.  An alternative derivation of
        \ineq{eq:gausspostcov} and \ineq{eq:gausspostmean} is given by
        \cite[Bishop2006] in chapter 2.3.3.

    \stopsubsection

    \startsubsection[title=Diagonalization of the Covariance Matrix]

        Transformation into eigenspace.

    \stopsubsection

    
    Plot stuff.

    cite Bishop, Rodgers

\stopsection


\startsection[title={Optimal Estimation},reference=ch:optimalestimation]

    Reference Rodgers, explain how it fits together.

    The optimal estimation approach directly uses Bayes' theorem
    \ineq{eq:bayes_theorem} to obtain the posterior probability distribution
    $\POSTERIOR$ using a forward model and prior information of the state
    vector.

    Call it physical retrieval because it uses the forward model.

    The assumptions for the prior {\PDF} and the likelihood function
    are that they are Gaussian.

    Justify assumptions.

    The prior $\PRIOR$ should contain all knowledge about the state vector
    before the measurement is evaluated. This knowledge might come from
    a climatology, output from a NWP model, information from other instruments
    and any combination of these sources. The construction of an optimal prior
    is not an easy task and will be discussed in section
    \in[ch:construct_prior]. In any case, due to the assumption that the
    {\PDF} is Gaussian it will have the form:

    \placeformula[eq:optimalest_prior]
    \startformula
        \PRIOR = \GAUSS{\VECX}{\MEANVECA}{\COVMATA}
    \stopformula

    The subscript stands for \quotation{a-priori}.

    The likelihood function $\LIKELIHOOD$ expresses the confidence that the
    observation $\VECY$ result of a state $\VECX$. This confidence is
    quantified with the help of the forward model, which can be formulated as:

    \placeformula[eq:forward_model]
    \startformula
        \VECY = \FWD(\VECX) + \ERR
    \stopformula

    Where $\FWD$ is the forward model operator and $\ERR$ is an error term,
    arising from measurement noise, inaccuracies of the forward model,
    interpolation errors and possibly other terms. Again, the assumption is
    that the errors are Gaussian, centered around a systematic bias
    $\MEANVECERR$ and with a convariance matrix $\COVMATERR$:

    \placeformula[eq:optimalest_errpdf]
    \startformula
        P(\ERR) = \GAUSS{\ERR}{\MEANVECERR}{\COVMATERR}
    \stopformula

    A discussion of how $\MEANVECERR$ and $\COVMATERR$ can be determined is
    given in section \in[rtm_errors]. Substituting \ineq{eq:forward_model}
    into \ineq{eq:optimalest_errpdf} and changing the independent variable of
    the {\PDF} results in the likelihood function:
    
    \placesubformula
    \startformula
    \startalign[n=3,align={right,middle,left}]
        \NC P(\VECY - \FWD(\VECX)) = \NC \GAUSS{\VECY - \FWD(\VECX)}{\MEANVECERR}{\COVMATERR} \NC \NR
        \NC = \NC \GAUSS{\VECY}{\FWD(\VECX)+\MEANVECERR}{\COVMATERR} \NC = \LIKELIHOOD \NR[eq:optimalest_likelihood][]
    \stopalign
    \stopformula

    Knowing \ineq{eq:optimalest_prior} and \ineq{eq:optimalest_likelihood} the
    posterior {\PDF} is given by:

    \placeformula[eq:optimalest_posterior]
    \startformula
        \POSTERIOR
        = \frac{\LIKELIHOOD \PRIOR}{\NORMALIZATION}
        = \frac{\GAUSS{\VECY}{\FWD(\VECX)+\MEANVECERR}{\COVMATERR}~\GAUSS{\VECX}{\MEANVECA}{\COVMATA}}{\NORMALIZATION}
    \stopformula

    Because the forward model is a nonlinear function, the product of the
    Gaussians cannot be carried out using the results from section
    \in[ch:bayes_gauss]. First, the forward model has to be linearized.

    \startsubsection[title=An Iterative Solution]

        Gaussian Error Assumption, linearize forward model â†’ jacobian/adjoint,
        analytic solution for cost function minimization.

        Show equivalence of MAP by finding maximum.

        Where does 1D-VAR come from?

    \stopsubsection

    \startsubsection[title={Constructing the Prior},reference=ch:constructing_prior]

        Choosing the prior (=background) state. Constructing the covariance
        matrix.

    \stopsubsection

    \startsubsection[title=A Generalization to Multiple Measurements]

        As shown by \cite[Rodgers2000] in chapter 4.1.1, there is a more
        general way to construct the posterior {\PDF} $\POSTERIOR$. If $n$
        measurements $\VECY_i$ are available that can be related to the state
        vector by forward models $\FWD_i$ and the forward model errors are
        pairwise independent, then Bayes' theorem takes the form

        \startformula
        \startalign[n=2,align={right,left}]
            \NC P(\VECX|\VECY_1,\dots,\VECY_n) =
                \NC \frac{P(\VECY_1,\dots,\VECY_n|\VECX)\PRIOR}{P(\VECY_1,\dots,\VECY_n)} \NR
            \NC = \NC \frac{\PRIOR}{P(\VECY_1,\dots,\VECY_n)} \prod_{i=1}^n P(\VECY_i|x) \NR
            \NC = \NC \frac{\PRIOR}{P(\VECY_1,\dots,\VECY_n)}
                \prod_{i=1}^n \GAUSS{\VECY_i}{\FWD_i(\VECX) + \MEANVEC_i}{\COVMAT_i} \EQCOMMA \NR
        \stopalign
        \stopformula

        where the independence property of the forward model errors has been
        used for the second identity. With this formulation, any measurements
        that give information on the state vector can be included in the
        optimal estimation procedure. This includes the a-priori knowledge,
        which might be interpreted as a \quotation{virtual measurement} having
        the identity function as a forward model and the prior state
        uncertainties as errors. In this interpretation, no prior knowledge
        is explicitly included in the model, i.e. $\PRIOR$ is constant, and the
        maximum a-posteriori solution is equivalent to a maximum likelihood
        solution.

        Is this used in the thesis? % TODO
        
    \stopsubsection

    \startsubsection[title=Introducing Constraints by Regularization]

        Refer to cost function version of MAP solution.

        Introduce penalties to cost function. Advantage: Simpler to set up
        than modifying prior, but no representation of additional knowledge
        in posterior.

        Example: \cite[Hewison2006], penalizing high LWC and references
        therein.

    \stopsubsection

\stopsection


\startsection[title=Linear Regression]

    Based on climatology, where to get climatology from. Idea: approximate
    the inverse model from climatology instead of introducting a numerical
    physical model.

    \startsubsection[title=Quantifying Uncertainty]

        Transformation into eigensystem, diagonal covariance.

    \stopsubsection

    \startsubsection[title=Including Additional Information]

        Adding regressors. No explicit forward model or error estimate needed.
        Who has done it? Problem: no constraint, rather a suggestion. Known
        elements of state vector are not enforced.

    \stopsubsection
    
\stopsection


\startsection[title=Neural Network Regression]

    Who has used it? Non-linearity. Extrapolation problems. No error estimates
    from common formulation. Choice of hidden layer and activation functions.
    Advantage in resolving low level inversions?

\stopsection


\startsection[title=Comparison of Techniques]

    Ease of use, computational efficiency, inclusion of additional information.

\stopsection

