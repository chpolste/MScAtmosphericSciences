The retrieval problem of atmospheric remote sensing can be stated as: given
a measurement of a sounding device, derive some part of the atmospheric state,
e.g. a vertical profile of temperature. The difficulty is that this derivation
requires the inversion of radiative transfer which is an ill-posed problem
due to the complex nature of electromagnetic wave propagation in the
atmosphere. When a passive instrument observes atmospheric radiation, its
measurement is affected by many emitting and absorbing layers of the atmosphere
and multiple states may exist which result in the same measurement. Therefore
additional information is needed to arrive at a unique solution which is
as close as possible to the true atmospheric state. Depending on the available
information and recources, different retrieval techniques are applicable. A
great variety of regression models exists which are purely statistical methods.
These directly model the inverse problem, predicting the atmospheric state
from a measurement based on a training data set, often a time series of past
measurements for which the corresponding atmospheric state is known by some
other method. Regression methods are generally easy to set up once such a data
set is available but they carry the risk of bad performance in situations not
adequately represented in the training data. Other methods make use of a
numerical model of the forward problem, which is used in combination with
boundary conditions given by additional measurements and/or statistical
knowledge to iteratively determine a solution that is consitent with both the
boundary conditions and the forward model. Such methods are called optimal
methods or physical retrievals.

Here, an optimal estimation method is derived and compared to the commonly used
linear regression method. This chapter will review the necessary mathematics
for both methods entirely in the framework of Bayesian statistics. All
following results have beed derived before. They are derived here again to
achieve consistent notation and to show the benefits of starting from a purely
Bayesian approach. The calculations are based on books by \cite[Rodgers2000]
and \cite[Bishop2006]. The former treats all aspects of the retrieval problem
specifically for atmospheric sounding from a mixed perspective of classical and
Bayesian statistics. His results are used for the optimal estimation technique.
The book by Bishop is the foundation for general results on Bayesian statistics
and the Gaussian distribution as well as for the Bayesian approach to linear
regression. The introduction to Bayesian statistics is additionally influenced
by \cite[Downey2013].

\startsection[title={Bayesian Statistics}]

    Probability always refers to some entity, which may be a hypothesis, an
    event or a value that some variable takes. A probability is a value between
    0 and 1 representing a continuum of certainty about the state of the
    entity. If the entity is a hypothesis, 0 stands for absolute certainty that
    the hypothesis is false, and 1 is absolute certainty that it is true. All
    numbers between 0 and 1 express certainties between these extremes.

    The entity that a probability refers to can be discrete or continuous.
    In both cases a \infull{PDF}, also called probability distribution,
    probability mass function or {\PDF} can be defined. For the retrieval
    problem the entity is always a continuous variable, e.g. $x \in U$, and its
    probability distribution is a function $U \rightarrow \REALS$ which is
    nonnegative almost everywhere. Integrating the {\PDF} over a subset of $U$
    results in the probability that $x$ takes a value in that subset. Therefore
    all probability distributions must fulfil

    \placeformula[eq:probaint]
    \startformula
        \int_U \PROB{x} \, \diff x = 1 \EQCOMMA
    \stopformula
    
    that is the probability that any value of all possible will be taken must
    be 1. A fundamental concept in Bayesian statistics is that of joint and
    conditional probability. A joint probability is the probability that two
    variables lie in a specified subset of their domain at the same time and
    a conditional probability is a probability of one variable being part of a
    set when it is known that the second variable takes a value in a given set.
    The joint {\PDF} is written as $\PROB{x, y}$ (read $x$ and $y$) and the
    conditional $\PROB{x \GIVEN y}$ (read $x$ given $y$). It is possible to
    remove the conditioning of a probability distribution by integrating over
    all possible values of the variable the provides the condition

    \startformula
        \PROB{y} = \int_U \PROB{x \GIVEN y} \, \PROB{y} \, \diff x \EQSTOP
    \stopformula

    This process is called marginalization. The extension of these concepts to
    more than two variables is straightforward.  To derive the fundamental
    theorem of Bayesian statistics, the product rule of probability is needed
    \cite[authoryears][Bishop2006] which reads

    \startformula
        \PROB{x, y} = \PROB{x \GIVEN y} \, \PROB{y} \EQCOMMA
    \stopformula

    i.e. the joint probability distribution of $x$ and $y$ is the product of
    the conditional probability distribution $x$ given $y$ and the {\PDF} of
    $y$ independent of $x$. Because the ordering of $x$ and $y$ is
    irrelevant, symmetry dictates that

    \startformula
        \PROB{x, y} = \PROB{y \GIVEN x} \, \PROB{x}
    \stopformula

    is valid too. Together both expressions for the joint {\PDF} can be equated
    and rearranged to yield Bayes' theorem:

    \placeformula[eq:bayes_theorem]
    \startformula
        \PROB{x \GIVEN y} = \frac{\PROB{y \GIVEN x} \, \PROB{x}}{\PROB{y}} 
            \sim \PROB{y \GIVEN x} \, \PROB{x} \EQSTOP
    \stopformula

    All probability distributions must fulfil \ineq{probaint}, therefore the
    denominator is just a normalization and generally does not need to be
    determined explicitly. Bayes' theorem allows to turn the conditioning
    around getting from $\PROB{y \GIVEN x}$ to $\PROB{x \GIVEN y}$ which else
    may be hard to compute directly. For retrieval applications it is useful
    to consider the diachronic interpretation of Bayes' theorem
    \cite[authoryears][Downey2013], which sees \ineq{bayes_theorem} as
    an update of the distribution of $x$ in light of some data $y$. At
    first only $\PROB{x}$ is known, this is the a-priori distribution of $x$ or
    just prior for short. Now some data $y$ is observed and $\PROB{x \GIVEN y}$
    expresses the likelihood of this observation given the prior knowlege $y$.
    Multiplying prior and likelihood and normalizing the result by $\PROB{y}$
    yields the so called posterior distribution $\PROB{y \GIVEN x}$
    representing the distribution of $x$ after having observed $y$. The
    application of Bayes' theorem updates the knowledge of $x$ with new
    knowlegde contained in $y$. This is visualized in Figure
    \in[fig:bayes_theorem] for a variable distributed in two dimensions.

    \placefigure[top][fig:bayes_theorem]
        {Visualization of a Bayesian update. A prior {\PDF} is combined
        with the likelihood {\PDF} according to Bayes' theorem resulting in
        a posterior distribution combining the knowledge contained in both.}
        {\externalfigure[bayes_theorem][width=\textwidth]}

    The Bayesian framework is valuable because it deals with entire probability
    distributions instead of just values. This means the uncertainty of a value
    or a fact is always quantified enabeling better decisions based on this
    data. Additionally a distribution allows sampling, i.e. generating new data
    from the distribution. This is useful for example when visualizing the
    data. But having to deal with probability distributions also complicates
    calculations and analytical solutions to a problem might not exist. One way
    to handle these situations is by discretization of the involved {\PDF}s and
    the use of Monte Carlo methods. As shown by \cite[Downey2013] these methods
    can work very well but they suffer from the high dimensionality of many
    problems and quickly become computationally expensive. Sometimes it
    sufficient to settle for a solution that is just values but still based on
    the underlying distributions. A common solution of the type is the
    determination of just the maximum of a probability distribution which often
    can be derived analytically even for complicated {\PDF}s. Depending on which
    distribution this solution is determined for, it is called the maximum
    likelihood (ML) or maximum a-posteriori (MAP) solution.

    There is another issue with the need for probability distributions in
    Bayesian statistics: sometimes one just has no data at hand to decide how
    the distribution of some variable should look like. At that point the
    science becomes subjective and distributions are chosen based on
    mathematical convenience or personal experience. Subjectivity in science
    sounds like a bad idea at first but here it is argued that the way it is
    treated in the Bayesian framework is actually a good thing. The reason is
    not that subjectivity is good in itself but unavoidable. Most of the time
    alternative methods starting from a non-Bayesian approach make the
    same subjective assumption only that they are implicit and sometimes
    downright hidden. This can be seen for example for the case of linear
    regression, which will be derived from a Bayesian approach in section
    \in[ch:linear_regression]. The assumption of Gaussian distributed target
    variables and regression parameters leads to the same solution as the
    classical approach of minimizing the sum of squared residuals with $L^2$
    regularization. The equivalence of solutions implies that both methods rely
    on the same assumptions but they are relatively hidden in the classical
    approach while they are clearly exposed in the Bayesian framework and the
    extension to different assumptions is obvious as well. Making a user
    explicitly aware of the assumptions that a model is based on allows him/her
    to include this information into the interpretation of results and the
    assessment of the applicability of a model which clearly is an advantage.

\stopsection

\startsection[title=The Multivariate Gaussian Distribution]

    Bayesian view relies heavily on probablility density functions. In
    a theoretical framework it is desireable to work with pdfs that allow
    analytic solutions for a given problem. The most commonly used pdf is the
    Gaussian distribution:

    \placeformula[eq:gaussian]
    \startformula
        \GAUSS{\VECX}{\MEANVEC}{\COVMAT}
        = \frac{1}{(2 \pi)^{d / 2} \det(\COVMAT)^{1/2}}
          \exp \left( -\frac{1}{2}(\VECX - \MEANVEC)^\top \COVMAT^{-1} (\VECX - \MEANVEC) \right)
    \stopformula

    where $\VECX, \MEANVEC \in \REALS^d$ and $\COVMAT \in \REALS^{d \times d}$
    and the exponential function is applied component-wise.

    The mean vector $\MEANVEC$ of the Gaussian distribution is the location of
    its maximum. The covariance matrix $\COVMAT$ determines the shape of the
    distribution. For a proper Gaussian distribution, it is always a symmetric
    positive definite matrix. When all off-diagonal elements of $\COVMAT$ are
    zero, each component of the multidimensional distribution is independent
    from the others. If nonzero off-diagonal elements exist, some or all
    componentes are correlated.

    TODO: Discuss possibility of determining mean of Gauss by search for extrema.

    Refer to default Bayes/Gaussian plot.

    Because the normalization $((2 \pi)^{d/2} \det(\COVMAT)^{1/2})^{-1}$ can be
    determined from the content of the quadratic form in the exponent, i.e. the
    size of the input $N$ and the covariance matrix $\COVMAT$, a Gaussian
    distribution can be described completely in terms of the exponent.
    A common representation of a Gaussian distribution, that will also be used
    in the following, is

    \startformula
        -2 \ln(\GAUSS{\VECX}{\MEANVEC}{\COVMAT})
        = (\VECX - \MEANVEC)^\top \COVMAT^{-1} (\VECX - \MEANVEC) + c \EQCOMMA % TODO: use \GAUSSEXP
    \stopformula

    where $c$ is a constant that contains the normalization. The right hand
    side of this representation is easier to deal with in analytical
    derivations but contains all the information of the distribution.

    Throughout this thesis it will be assumed again and again that continuous
    variables follow a Gaussian distribution. A major reason for choosing this
    assumption is that Gaussian distributions have very convenient analytic
    properties.  Particularly useful is that the marginalization of a Gaussian
    yields a Gaussian and that a conditional distribution of two Gaussian
    variables is again Gaussian if the conditioning is linear
    \cite[authoryears][Bishop2006].
    
    TODO The assumption that a continuous variable follows a Gaussian
    distribution is not only popular due to its well understood analytic
    behavior. central limit theorem, maximum entropy.

    Because the assumption of Gaussian {\PDF}s is persistent throughout this
    text, some general results are derived in the following sections. These
    will be used in subsequent sections.

\startsubsection[title=Parameter Estimation]
    
    For the description of a dataset in the form of a Gaussian distribution the
    mean and covariance parameters have to be estimated from the data.  This is
    done by finding the maximum likelihood solutions for $\MEANVEC$ and
    $\COVMAT$ given a dataset $\{\VECX_1, \dots, \VECX_N\}$, whose elements are
    assumed to be independent and identically distributed.  The results are
    found in most textbooks dealing with statistics
    \cite[alternative=authoryears,left={(e.g. }][Bishop2006]. Results are shown
    here for completeness but no derivations.

    The likelihood of a single datapoint $\VECX_i$ is
    $\GAUSS{\VECX_i}{\MEANVEC}{\COVMAT}$ and because of the assumption of
    independent data the likelihood of the entire dataset is

    \startformula
        \PROB{\{\VECX_1, \dots, \VECX_N\} \GIVEN \MEANVEC, \COVMAT}
        = \prod_{i=1}^N \GAUSS{\VECX_i}{\MEANVEC}{\COVMAT} \EQCOMMA
    \stopformula

    i.e. the product of each individual likelihood. The product of Gaussians is
    again Gaussian and mean of a Gaussian is its maximum therefore $\MEANVEC$
    is found by maximizing the likelihood function of the dataset or
    equivalently minimizing its negative natural logarithm.  Determination of
    the covariance matrix is more involved as it has to be symmetric and
    positive definite. The resulting ML estimates are

    \startformula
        \MEANVEC_{ML} = \frac{1}{N} \sum_{i=1}^N \VECX_i
    \stopformula

    and

    \startformula
        \COVMAT_{ML} = \frac{1}{N} \sum_{i=1}^N (\VECX_i - \MEANVEC_{ML})^\top
            (\VECX_i - \MEANVEC_{ML})
    \stopformula

    Notably the mean can be calculated independently from the covariance.  The
    ML estimator of the covariance systematically underestimates the true
    covariance but can be corrected by using

    \startformula
        \tilde\COVMAT_{ML} = \frac{1}{N-1} \sum_{i=1}^N (\VECX_i - \MEANVEC_{ML})^\top
            (\VECX_i - \MEANVEC_{ML})
    \stopformula

    instead of $\COVMAT_{ML}$.

\stopsubsection

\startsubsection[title={Bayesian Operations for Gaussians},reference=ch:bayes_gauss]

    In this section two important results for Gaussian distributions are
    derived which are used later for the optimal estimation method and linear
    regression with error estimates. A rigorous derivation of these results is
    found in \cite[Bishop2006]. Here, shortcuts are taken by taking the facts
    that the product of two Gaussians is Gaussian and that the marginalization
    of a Gaussian is again Gaussian as a given.

    The goal is to find the missing probability distributions in Bayes' Theorem
    when the prior $\PRIOR$ and likelihood function $\LIKELIHOOD$ are given by

    \startsubformulas[eq:gaussbayesinit]
    \placesubformula
    \startformula
    \startalign[n=2,align={right,left}]
        \NC \PRIOR = \NC \GAUSS{\VECX}{\VECA}{\MATPI} \EQCOMMA \NR[eq:gaussbayesinitx]
        \NC \LIKELIHOOD = \NC \GAUSS{\VECY}{\MATB \VECX + \VECB}{\MATQI}
            \EQSTOP \NR[eq:gaussbayesinityx]
    \stopalign
    \stopformula
    \stopsubformulas

    The mean of the conditional likelihood function is a linear function of
    $\VECX$ but the covariance is independent of $\VECX$. Covariances are given
    as inverted $\MATP$ and $MATQ$ making the notation of the Gaussians in
    their logarithm form less cluttered:

    \startsubformulas[eq:gaussbayesinitln]
    \placesubformula
    \startformula
    \startalign[n=2,align={right,left}]
        \NC -2 \ln(\PRIOR) = \NC (\VECX - \VECA)^\top \MATP (\VECX - \VECA)
            + \CONST \EQCOMMA \NR[eq:gaussbayesinitlnx]
        \NC -2 \ln(\LIKELIHOOD) = \NC (\VECY - \MATB \VECX - \VECB)^\top
            \MATQ (\VECY - \MATB \VECX - \VECB) + \CONST
            \EQSTOP \NR[eq:gaussbayesinitlnyx]
    \stopalign
    \stopformula
    \stopsubformulas

    The terms denoted \quote{const} contain the respective normalizations.
    Bayes' theorem is derived from the joint probability distribution

    \startformula
        \JOINT = \LIKELIHOOD \PRIOR = \POSTERIOR \NORMALIZATION \EQSTOP
    \stopformula

    Viewing $\LIKELIHOOD$ and $\PRIOR$ as likelihood and prior makes
    $\POSTERIOR$ the posterior distribution. It can be extracted from the joint
    probability distribution or derived by the product of $\LIKELIHOOD$ and
    $\PRIOR$ directly since $\NORMALIZATION$ only provides a constant
    normalization which can also be determined by the covariance of
    $\POSTERIOR$.

    \placesubformula
    \startformula
    \startalign[n=2,align={right,left}]
        \NC -2 \ln(\POSTERIOR) = \NC -2 \ln\left(\frac{\JOINT}{\NORMALIZATION}\right) \NR
        \NC = \NC -2 \ln\left(\frac{\LIKELIHOOD \PRIOR}{\NORMALIZATION}\right) \NR
        \NC = \NC -2 \ln(\LIKELIHOOD) - 2 \ln(\PRIOR) + \CONST \NR
        \NC = \NC \GAUSSEXP{\VECY}{\MATB \VECX - \VECB}{\MATQ}
            + \GAUSSEXP{\VECX}{\VECA}{\MATP} \NR
        \NC = \NC
            \VECYT \MATQ \VECY
            - \VECYT \MATQ \MATB \VECX
            - \VECYT \MATQ \VECB
            - \VECXT \MATBT \MATQ \VECY
            \NR
        \NC \NC
            + \VECXT \MATBT \MATQ \MATB \VECX
            + \VECXT \MATBT \MATQ \VECB
            - \VECBT \MATQ \VECY
            + \VECBT \MATQ \MATB \VECX
            \NR[eq:gaussprodexpand]
        \NC \NC
            + \VECXT \MATP \VECX
            - \VECXT \MATP \VECA
            - \VECAT \MATP \VECX
            + \CONST \EQCOMMA
            \NR
    \stopalign
    \stopformula

    where the last step is expanding the vector-matrix-vector products. The
    posterior is again a Gaussian distribution, therefore an equivalent
    representation of the form

    \placesubformula
    \startformula
    \startalign[n=2,align={right,left}]
        \NC -2 \ln(\POSTERIOR) = \NC
            \GAUSSEXP{\VECX}{\VECC}{\MATS} + \CONST \NR
        \NC = \NC
             \VECXT \MATS \VECX
             - \VECXT \MATS \VECC
             - \VECCT \MATS \VECX
             + \CONST \NR[eq:gausspostexpand]
    \stopalign
    \stopformula

    must exist. Noting that the equivalence must hold for all $\VECX$, an
    expression for $\COVMAT$ is found by matching terms quadratic in $\VECX$ of
    both forms \ineq{gaussprodexpand} and \ineq{gausspostexpand}:

    \startformula
    \startalign[n=3,align={left,right,left}]
        \NC \NC \VECXT \MATS \VECX = \NC
            \VECXT \MATBT \MATQ \MATB \VECX + \VECXT \MATP \VECX \NR
        \NC \NC = \NC
            \VECXT (\MATBT \MATQ \MATB + \MATP) \VECX \NR
        \NC \Rightarrow~~ \NC \MATS = \NC
            \MATBT \MATQ \MATB + \MATP
            \EQSTOP
    \stopalign
    \stopformula

    Matching the linear terms involving $\VECXT$ of both forms
    \ineq{gaussprodexpand} and \ineq{gausspostexpand} results in an expression
    for $\VECC$:

    \startformula
    \startalign[n=3,align={left,right,left}]
        \NC \NC - \VECXT \MATS \VECC = \NC
            - \VECXT \MATBT \MATQ \VECY
            + \VECXT \MATBT \MATQ \VECB
            - \VECXT \MATP \VECA \NR
        \NC \NC  = \NC
            - \VECXT (\MATBT \MATQ (\VECY - \VECB) + \MATP \VECA) \NR
        \NC \Rightarrow~~ \NC \VECC = \NC
            \MATSI (\MATBT \MATQ (\VECY - \VECB) + \MATP \VECA) \EQSTOP \NR
    \stopalign
    \stopformula

    Equating terms linear in $\VECX$ yields the same result. With $\VECC$
    (mean) and $\MATS$ (inverse covariance) determined,

    \placeformula[eq:gaussbayespost]
    \startformula
        \POSTERIOR = \GAUSS{\VECX}{\MATSI(\MATBT \MATQ (\VECY - \VECB) + \MATP \VECA)}{\MATSI}
    \stopformula

    is known. $\NORMALIZATION$ could now be obtained by

    \startformula
        \NORMALIZATION = \frac{\JOINT}{\POSTERIOR} \EQCOMMA
    \stopformula

    making the assumption that it is a Gaussian distribution

    \startformula
        -2 \ln(\NORMALIZATION) = \GAUSSEXP{\VECY}{\VECD}{\MATT}
    \stopformula

    and matching terms between both expressions as done for the posterior.  Due
    to the rather complex mean and covariance of $\POSTERIOR$ this is tedious.
    It is also possible to directly integrate the joint distribution with
    respect to $\VECX$
    
    \startformula
        \NORMALIZATION = \int \JOINT \, \diff \VECX = \int \LIKELIHOOD \, \PRIOR \, \diff \VECX \EQCOMMA
    \stopformula
    
    again a quite tedious procedure.  Starting from the joint distribution and
    rewriting it in blocked form, an intuitive argument is made which can be
    reinforced by rigorous derivation given by \cite[Bishop2006].  Let $\VECZ$
    be the vector obtained by stacking $\VECX$ and $\VECY$:

    \startformula
        \VECZ = \startpmatrix \VECX \NR \VECY \NR \stoppmatrix \EQSTOP
    \stopformula

    The joint distribution is Gaussian

    \placeformula
    \startformula
    \startalign
        \NC \JOINTZ = \NC \GAUSS{\VECZ}{\MEANVEC}{\MATRI} \EQCOMMA \NR
        \NC -2 \ln(\JOINTZ) = \NC \GAUSSEXP{\VECZ}{\MEANVEC}{\MATR} + \CONST \NR
        \NC = \NC \VECZT \MATR \VECZ - \VECZT \MATR \MEANVEC
            - \MEANVECT \MATR \VECZ + \CONST \NR[eq:gaussbayesjoint]
    \stopalign
    \stopformula

    and by matching terms with $\JOINT = \LIKELIHOOD \PRIOR$ \footnote{Which
    are the same terms as in \ineq{gaussprodexpand} with exception of the
    constant term containing the normalization $2 \ln(\NORMALIZATION)$.},
    $\MATR$ and $\MEANVEC$ can be determined. As before, the quadratic terms
    are used for the inverse of the covariance

    \placeformula
    \startformula
    \startalign[n=2,align={right,left}]
        \NC \NC
            \VECYT \MATQ \VECY
            - \VECYT \MATQ \MATB \VECX
            - \VECXT \MATBT \MATQ \VECY
            + \VECXT \MATBT \MATQ \MATB \VECX
            + \VECXT \MATP \VECX \NR
        \NC = \NC
            \VECZT \startpmatrix[n=2,align={middle,middle}]
                \NC \MATP + \MATBT \MATQ \MATB \NC - \MATBT \MATQ \NR
                \NC - \MATQ \MATB \NC \MATQ \NR
            \stoppmatrix \VECZ \NR[eq:gaussbayesjointcovi]
        \NC = \NC
            \VECZT \MATR \VECZ \NR
    \stopalign
    \stopformula

    and linear terms for determination of the mean (here shown with the
    transposed terms)

    \startformula
    \startalign[n=2,align={right,left}]
        \NC \NC
            - \VECYT \MATQ \VECB
            + \VECXT \MATBT \MATQ \VECB
            - \VECXT \MATP \VECA \NR
        \NC = \NC
            - \VECZT \startpmatrix
                \MATP \VECA - \MATBT \MATQ \VECB \NR
                \MATQ \VECB \NR
            \stoppmatrix \NR
        \NC = \NC
            - \VECZT \MATR \MEANVEC \EQSTOP \NR
    \stopalign
    \stopformula

    An expression for the inverse of a block matrix is given e.g. by
    \cite[Petersen2012] yielding

    \placeformula[eq:gaussbayesjointcov]
    \startformula
        \MATRI = \startpmatrix[n=2,align={middle,middle}]
            \NC \MATPI \NC \MATPI \MATBT \NR
            \NC \MATB \MATPI \NC \MATQI + \MATB \MATPI \MATBT \NR
        \stoppmatrix \EQCOMMA
    \stopformula

    for \ineq{gaussbayesjointcovi} while using the fact that covariance
    matrices are symmetric. Knowing \ineq{gaussbayesjointcov}, the mean is
    given by

    \placeformula[eq:gaussbayesjointmean]
    \startformula
        \MEANVEC = \MATRI \startpmatrix
                \MATP \VECA - \MATBT \MATQ \VECB \NR
                \MATQ \VECB \NR
            \stoppmatrix = \startpmatrix
                \VECA \NR
                \MATB \VECA + \VECB \NR
            \stoppmatrix \EQSTOP
    \stopformula

    Comparing \ineq{gaussbayesjointcov} and \ineq{gaussbayesjointmean} with the
    probability distribution $\PRIOR = \GAUSS{\VECX}{\VECA}{\MATPI}$ from
    \ineq{gaussbayesinit} one finds that the upper left block of the covariance
    matrix and the upper block of the mean vector of the joint distribution
    correspond directly to the covariance and mean of $\PRIOR$. This makes
    intuitive sense: When the distribution is evaluated, these are the blocks
    that interact with $\VECX$ exclusively while the off-diagonal blocks of
    $\MATRI$ model the interaction between $\VECX$ and $\VECY$. It would make
    sense that the former blocks are then the only ones neccesary to describe
    the (marginalized) distribution with respect to $\VECX$ alone. The same
    idea leads to the expectation that the lower left block of $\MATRI$ and the
    lower block of $\MEANVEC$ are the only ones neccessary to describe the
    (marginalized) distribution with respect to $\VECY$ only. For symmetry
    $\NORMALIZATION$ should therefore be given by
    
    \placeformula[eq:gaussbayesnorm]
    \startformula
        \NORMALIZATION = \GAUSS{\VECY}{\MATB \VECA + \VECB}{\MATQI + \MATB \MATPI \MATBT} \EQCOMMA
    \stopformula

    and this is indeed the result found in a rigorous derivation.

\stopsubsection

\stopsection


\startsection[title={Optimal Estimation},reference=ch:optimalestimation]

    This retrieval technique uses a Bayesian update to incorporate radiometer
    observations $\VECY$ into prior knowledge of the atmospheric state vector
    $\VECX$. The resulting state vector is consistent with the forward model
    (usually a numerical radiative transfer model) necessary for the technique
    and uses the a-priori information to solve the uniqueness problem
    associated with retrievals. Because the method employs a numerical forward
    model simulating the physical processes of measurement, it is often called
    a physical retrieval.

    Advantages: instruments, forward model and knowledge are included with
    their uncertainties and an uncertainty estimate comes out too. NWP models
    generally provide good priors for upper atmosphere but predict boundary
    layer processes less good, radiometer gets most information from lower
    atmosphere the upper atmosphere â†’ ideal combination of information. No
    climatology needed if prior and likelihood distributions are characterized.
    Disadvantages: forward model generally requires full description of
    atmospheric state although \cite[Cimini2011] separated the retrieval
    of temperature and humidity with success. Computational costs of forward
    model and its necessary linearization are often mentioned but some
    forward models can calculate linearizations effectively and computers are
    increasingly fast making this less of a problem than years ago. Barrier
    of entry higher as method is more complicated to set up than a regression.
    Software such as QPACK aims to solve this issue by providing routines
    for optimal estimation and a forward model (REF).

    The most often cited reference for the theory behind optimal estimation
    schemes is the book by \cite[Rodgers2000]. The derivation here also follows
    this book but only shows the purely Bayesian parts.  The usual assumption
    for all probability distributions involved in optimal estimation is that
    they are Gaussian. The reasons for this are mainly of mathematical nature
    as Gaussians can be dealt with analytically even for problems of high
    dimensionality where dicrete approaches are to costly.  While is assumption
    is good for variables like temperature (REF) some additional work will be
    neccesary for humidity and cloud liquid water.  These issues are discussed
    in section \in[ch:statevector]. For now the Gaussian assumption is taken as
    is and the involved {\PDF}s are derived.  The prior $\PRIOR$ should contain
    all knowledge about the state vector before the measurement is evaluated.
    This knowledge might come from a climatology, output from a NWP model,
    information from other instruments and any combination of these sources.
    The construction of an optimal prior is not an easy task and will be
    discussed in section \in[ch:construct_prior]. In any case, due to the
    assumption that the {\PDF} is Gaussian it is of the form:

    \placeformula[eq:optimalest_prior]
    \startformula
        \PRIOR = \GAUSS{\VECX}{\MEANVECA}{\COVMATA} \EQCOMMA
    \stopformula

    where the subscript \quote{a} stands for \quote{a-priori}.  The likelihood
    function $\LIKELIHOOD$ expresses the confidence that the observation
    $\VECY$ is the result of a state $\VECX$. This confidence is quantified
    with the help of the forward model, formulated as

    \placeformula[eq:forward_model]
    \startformula
        \VECY = \FWD(\VECX) + \ERR \EQCOMMA
    \stopformula

    where $\FWD$ is the forward model operator and $\ERR$ is an error term,
    arising from measurement noise, inaccuracies of the forward model,
    discretization errors and possibly other terms. Again, the assumption is
    that the error term follows a Gaussian distribution, centered around
    a systematic bias $\MEANVECERR$ with a convariance matrix $\COVMATERR$

    \placeformula[eq:optimalest_errpdf]
    \startformula
        \PROB{\ERR} = \GAUSS{\ERR}{\MEANVECERR}{\COVMATERR} \EQSTOP
    \stopformula

    A discussion of how $\MEANVECERR$ and $\COVMATERR$ can be determined is
    given in section \in[rtm_errors]. Substituting \ineq{forward_model}
    into \ineq{optimalest_errpdf} and changing the independent variable of
    the {\PDF} to $\VECY$ results in the likelihood function:
    
    \placesubformula
    \startformula
    \startalign[n=3,align={right,middle,left}]
        \NC \PROB{\VECY - \FWD(\VECX)} = \NC
            \GAUSS{\VECY - \FWD(\VECX)}{\MEANVECERR}{\COVMATERR} \NC \NR
        \NC = \NC \GAUSS{\VECY}{\FWD(\VECX)+\MEANVECERR}{\COVMATERR} \NC
            = \LIKELIHOOD \EQSTOP \NR[eq:optimalest_likelihood]
    \stopalign
    \stopformula

    Knowing \ineq{optimalest_prior} and \ineq{optimalest_likelihood}, the
    posterior {\PDF} is given by

    \placeformula[eq:optimalest_posterior]
    \startformula
        \POSTERIOR
        = \frac{\LIKELIHOOD \PRIOR}{\NORMALIZATION}
        = \frac{\GAUSS{\VECY}{\FWD(\VECX) + \MEANVECERR}{\COVMATERR}
            ~\GAUSS{\VECX}{\MEANVECA}{\COVMATA}}{\NORMALIZATION} \EQSTOP
    \stopformula

    It expresses the probability of an atmospheric state $\VECX$ given a
    measurement $\VECY$ and incorporates the forward model with its error
    characteristic and any a-priori knowledge of the atmospheric state before
    the measurement. As all involved distributions are assumed to be Gaussian,
    the $\POSTERIOR$ is also a Gaussian distribution therefore the
    normalization $\NORMALIZATION$ does not have to be determined explicitly
    but follows from the product in the numerator. The forward model operator
    $\FWD$ is non-linear, therefore this product cannot be carried out
    analytically. To use the results from section \in[ch:bayes_gauss], the
    model has to be linearized and an iterative approach is taken.

\startsubsection[title=Iterative Solutions]

    To linearize $\FWD$, a reference state to linearize at must be chosen.
    Let this state be $\ITER{\VECX}{i}$. Applying a Taylor series
    expansion to the forward model and chopping off all higher-than-linear
    order terms yields the approximation

    \startformula
        \FWD(\VECX) \approx \FWD(\ITER{\VECX}{i})
            + \ITER{\FWDJAC}{i} (\VECX - \ITER{\VECX}{i}) \EQSTOP
    \stopformula

    $\ITER{\FWDJAC}{i}$ is the Jacobian of $\FWD$ at $\ITER{\VECX}{i}$
    containing the derivatives of the forward model output with respect to each
    state vector variable. After replacing $\FWD$ in
    \ineq{optimalest_posterior} by this linearization, the likelihood function

    \startformula
        \LIKELIHOOD = \GAUSS{\VECY}{\ITER{\FWDJAC}{i} \VECX
            + \FWD(\ITER{\VECX}{i}) - \ITER{\FWDJAC}{i}
            \ITER{\VECX}{i} + \MEANVECERR}{\COVMATERR}
    \stopformula

    and the prior \ineq{optimalest_prior} fulfill the assumptions
    \ineq{gaussbayesinit} and the result \ineq{gaussbayespost} can be used with
    the substitutions

    \startformula
        \{ \VECA \rightarrow \MEANVECA,~
        \MATPI \rightarrow \COVMATA,~
        \MATB \rightarrow \ITER{\FWDJAC}{i},~
        \VECB \rightarrow \FWD(\ITER{\VECX}{i})
            - \ITER{\FWDJAC}{i} \ITER{\VECX}{i}
            + \MEANVECERR,~
        \MATQI \rightarrow \COVMATERR \}
    \stopformula

    to obtain an expression for the posterior:
    
    \startformula
    \startalign[n=2,align={right,left}]
        \NC \POSTERIOR = \NC \GAUSS{\VECX}{\MEANVEC}{\COVMAT} \NR
        \NC \MEANVEC = \NC \COVMAT (\ITER{\FWDJAC}{i}^\top
            \COVMATERR^{-1} (\VECY - \FWD(\ITER{\VECX}{i})
            + \ITER{\FWDJAC}{i} \ITER{\VECX}{i} - \MEANVECERR)
            + \COVMATA^{-1} \MEANVECA) \NR
        \NC \COVMAT = \NC 
            (\ITER{\FWDJAC}{i} \COVMATERR^{-1} \ITER{\FWDJAC}{i}^\top
            \COVMATA^{-1})^{-1} \NR
    \stopalign
    \stopformula
    
    Because of the linearization the solution for $\MEANVEC$ and $\COVMAT$ will
    not be optimal. Starting from a first guess $\ITER{\MEANVEC}{0}$, use
    fixed-point iteration to find the optimal solution, using the mean of the
    previous iteration as the linearization point

    \startsubformulas[eq:gausspostiter]
    \placesubformula
    \startformula
    \startalign[n=3,align={right,left,right}]
        \NC \ITER{\MEANVEC}{i+1} = \NC
            \ITER{\COVMAT}{i} (\ITER{\FWDJAC}{i}^\top
            \COVMATERR^{-1} (\VECY - \FWD(\ITER{\MEANVEC}{i})
            + \ITER{\FWDJAC}{i} \ITER{\MEANVEC}{i} - \MEANVECERR)
            + \COVMATA^{-1} \MEANVECA) \EQCOMMA \NC \NR[eq:gausspostmeaniter][a]
        \NC \ITER{\COVMAT}{i} = \NC
            (\ITER{\FWDJAC}{i}^\top \COVMATERR^{-1} \ITER{\FWDJAC}{i}
            + \COVMATA^{-1})^{-1}
            \EQSTOP \NC \NR[eq:gausspostcoviter][b]
    \stopalign
    \stopformula
    \stopsubformulas

    In practice more robust iteration schemes are needed to obtain good
    convergence properties. The search for the mean of the posterior can be
    translated into a minimization problem, for which many well studied
    numerical methods exist. This is because of the symmetry of Gaussian which
    implies that the mean of the distribution is also its maximum \footnote{In
    Bayesian terminology this is called the maximum a-posteriori solution or
    MAP for short. The equivalence of MAP and the expected value is one of the
    convenient analytic properties of the Gaussian distribution which make it
    such an attractive assumption.}. The natural logarithm is a strictly
    monotonic function therefore the maximum distribution is equal to the
    minimum of its negated quadratic form. The task is then to find the root of
    the gradient of the quadratic form of \ineq{optimalest_posterior} with
    respect to $\VECX$:

    \placeformula
    \startformula
    \startalign[n=2,align={right,left}]
        \NC 0 = \NC \nabla_{\VECX} (- 2 \ln(\POSTERIOR)) \NR
        \NC = \NC (\VECY - \FWD(\VECX) - \MEANVECERR)^\top \COVMAT^{-1}
            (\VECY - \FWD(\VECX) - \MEANVECERR) + (\VECX - \MEANVECA)^\top
            \COVMATA (\VECX - \MEANVECX)^\top \EQSTOP \NR[eq:gaussminimization]
    \stopalign
    \stopformula
    
    Usually one settles for a least squares solution to the minimization
    problem as methods for these problems are much simpler to implement (e.g.
    Newton's method for finding the root of a function requires the Hessian,
    which is often expensive to calculate). \cite[Rodgers2000] shows resulting
    iterative schemes for the Gauss-Newton method and a more robust version of
    it, the Levenberg-Marquard iteration. Using the identity

    \startformula
        \MEANVECA - \ITER{\COVMAT}{i} \ITER{\FWDJAC}{i}^\top
        \COVMATERR^{-1} \ITER{\FWDJAC}{i} \MEANVECA =
        \ITER{\COVMAT}{i} (\ITER{\COVMAT}{i}^{-1}
        - \ITER{\FWDJAC}{i}^\top \COVMATERR^{-1} \ITER{\FWDJAC}{i})
        \MEANVECA = \ITER{\COVMAT}{i} \COVMATA^{-1} \MEANVECA \EQCOMMA
    \stopformula

    it is easily seen that \ineq{gausspostmeaniter}, the solution obtained from
    fixed point iteration with the linearized forward model, is equivalent to
    equation 5.9 from \cite[Rodgers2000], which he obtained by applying the
    Gauss-Newton method. This underlines the equivalence of the two approaches.

    \placefigure[top][fig:iteralgo]
            {Visualization of the iterative retrieval algorithm. Missing
            is the forward model error distribution, which is used in the
            Levenberg-Marquard step. Square boxes represent numerical
            calculations, rounded boxes data input and octagonal boxes
            data being produced during the retrieval.}
            {\FLOWchart[flow:iteralgo]}

    The flowchart in Figure \in[fig:iteralgo] visualizes the resulting
    iterative retrieval scheme. Given the prior knowledge, the measurement and
    an initial guess of the profile, minimization steps with a linearized
    forward model are taken until convergence. The profile of the last
    iteration is then the result of the retrieval.

\stopsubsection

\startsubsection[title={Convergence Testing}]

    It is unnecessary to iterate until the atmospheric state vector has beed
    determined to machine precision and depending on the quality of the
    linearization of the forward model this might never happen. Therefore some
    criterion is needed that determines when the state vector has been
    determined with sufficient accuracy.

    Directly use value of cost function. Additionally use gradient of cost
    function. PROBLEM: cost function approach introduced in next section...

    Measure change in observation space.

    \startformula
        (\FWD(\ITER{\VECX}{i}) - \FWD(\ITER{\VECX}{i+1}))^\top
        \COVMATI_{\delta y}
        (\FWD(\ITER{\VECX}{i}) - \FWD(\ITER{\VECX}{i+1}))
    \stopformula

    where
    
    \startformula
        \COVMATI_{\delta y} = \COVMATERR (\ITER{\FWDJAC}{i}
            \COVMATA \ITER{\FWDJAC}{i}^\top)^{-1} \COVMATERR
    \stopformula

    Measure change in state vector space.

    \startformula
        (\ITER{\VECX}{i} - \ITER{\VECX}{i+1})^\top
        \ITER{\COVMATI}{i+1}
        (\ITER{\VECX}{i} - \ITER{\VECX}{i+1})
    \stopformula

    In both cases the squared distance is scaled by estimated error covariance
    which gives a sort of relative error.

    Covariance is positive definite therefore are these quadratic forms always
    positive.

    \cite[Rodgers2000] discusses the latter two convergence criteria in his
    chapter 5.6.3 and these appear to be the most popular.

    REF first converged on temperature with fixed humidity and V band channels
    only. Then humidity retrieval.

    Save computation cost by evaluation Jacobian every other step. Weakens
    convergence somewhat.

    Give some examples of iteration counts from the literature.

    The chosen convergence criterion, the included radiometer channels and
    angles, the minimization technique and the method of forward model
    linearization are likely to be major influences on convergence.

    Generally cloudy cases converge less often than clear-sky cases due to the
    stronger non-linearity of absorption at the cloud threshold
    \cite[authoryears][Hewison2006].

\stopsubsection

\startsubsection[title={1D-VAR, Cost Functions and Regularization},reference=ch:costfunction]

    There is another way to derive the optimal estimation retrieval scheme.
    Instead of working directly with probability distributions the retrieval
    problem can be approached as a minimization of the expected value of the
    retrieval error variance. In case of the assumption of Gaussian
    distributions this leads to the same solutions as the Bayesian approach.
    Because of the focus on minimization variance this approach is called
    \quote{variational} often abbreviated as 1D-VAR, since it is an applied to
    an atmospheric profile which represents one dimension of the atmosphere.
    A derivation from the variational perspective is found in chapter 4.2 in
    the book by \cite[Rodgers2000].

    The quadratic form minimized in the MAP solution \ineq{gaussminimization}
    is often referred to as a cost function, terminology that is common in
    regression problems. It is popular to add so-called regularization or
    penalty terms to this cost function in order to make the retrieval favor
    physically realistic solutions. In atmospheric profile retrieval such
    a penalty might be a term involving the lapse rate, raising the
    \quote{cost} of having superadiabatic layers in the atmosphere as these
    only occur near the surface (CITE).  REF and \cite[Hewison2006] added
    a term regulating the amount of LWC, with a higher penalty for higher cloud
    water content.
    
    There is a problem with including information in the retrieval by adding
    regularization terms: while the information is included in the retrieved
    profile, it is absent from its uncertainty estimate given by
    $\ITER{\COVMAT}{i}$. This issue can be resolved with the Bayesian
    perspective. Viewing the cost function as the product of the prior and
    likelihood function it becomes obvious that each additional penalty term is
    nothing else than a multiplication by a distribution representing prior
    knowledge. For a quadratic penalty term the associated distribution is
    Gaussian, making the posterior a Gaussian and an analytic form of the
    uncertainty can be derived as shown in the next section. For other terms
    the distribution might not permit an analytic form of the posterior. Then
    one has to resort to discretized distributions and/or Monte Carlo methods
    which are costly due to the high dimensionality of the retrieval problem.

    Regularization is a powerful tool for the inclusion of additional knowledge
    but penalities more complex than quadratic forms generally violate the
    Gaussian assumptions leading to inaccurate uncertainty estimates. Often
    these errors are tolerable since for many end users a more accuratly
    retrieved profile is more valuable than an inconsistent uncertainty
    assessment.

\stopsubsection

\stopsection


\startsection[title={Linear Regression},reference=ch:linear_regression]

    Linear regression is a computationally cheap retrieval method. It relies
    on an approximate model of the inversion process which is generated to fit
    a set of training data and then used to make predictions of the atmospheric
    state when given new brightness temperature measurements. Training data
    are often historic data from radiosonde ascents at a given location. If
    such data are not available, NWP analyses or forecasts may be used
    \cite[authoryears][Guldner2013]. Predictions of the regression model do not
    require the evaluation of a forward model, but a radiative transfer model
    is often used to simulate the brightness temperatures for training as
    sufficiently long time series of radiometer measurements are rare. The
    computational effort is concentrated in the training but then evaluation of
    the model is cheap.  This is true of all regression models and linear
    regression shares many other properties, advantages and disadvantages with
    most other regression methods (see also section \in[ch:otherretrievals]).
    When generating a linear regression model, three aspects affect its
    performance: the choice of basis functions, the training data set and the
    measures taken to avoid overfitting.

    The model in linear regression consists of a set of basis functions
    dependent on the input variables which are combined linearly to make
    a prediction. In case of retrieval the measured brightness temperatures are
    the input and the atmospheric profile of temperature, humidity and/or
    liquid water is the model's output. Linear regression models can
    approximate non-linear relationships if non-linear basis functions are
    chosen. The \quote{linear} in the name refers to the linearity in the model
    coefficients that are the weights with which the basis is combined. In
    retrieval applications it is common to use the basis functions that
    correspond to the individual brightness temperatures.
    \cite[Crewell2007,Massaro2015] included quadratic terms as well to better
    approximate the non-linear nature of the retrieval problem. Because the
    coefficients affect the model only linearly they can be determined from
    a training data set in one step without the need for iteration, resulting
    in a well understood, easy and cheap training process.

    In order to obtain a model that provides good predictions in many
    situations a large training data set is necessary that represents typical
    meteorological conditions at a measurement site well. Regression models in
    general are only as good as the data they were trained with and
    extrapolations to states not included in the training data are prone to
    errors and have no guarantee of physical reasonability
    \cite[authoryears][Cimini2006,Turner2007,Chan2010]. This behavior is one
    of the factors that limit the transferability of a trained regression model
    between different radiometer sites others being different altitudes of the
    measurement site or different channels used by the radiometers
    \cite[authoryears][Turner2007]. (TODO: there have been authors that used
    data from multiple sites, find and cite as exceptions)

    The set of basis functions and the number of training data pairs both
    affect if a model is in danger of overfitting. A model is overfitted if it
    performs well for the training data but makes poor predictions for any
    other data. Common practice is to have an independent test data set on
    which model performance is evaluated or to use cross validation techniques.
    So called regularization is used in linear regression to control
    overfitting during training. It is preferrable to the addition of noise to
    predictors during training as some authors have done
    \cite[alternative=authoryears,left={(e.g.  }][Churnside1994,Frate1998,Lohnert2004]
    since it is mathematically cleaner, works also with small data sets and is
    deterministic. In the following derivation of Bayesian linear regression
    regularization will appear naturally from the prior of the model
    parameters.

    Due to their simplicity linear regression models are commonly used.
    Statistical evaluations: rmse of ... K in the boundary layer. Performance
    of generally decreasing with height. This is mainly due to limited
    information content of radiometers which are usually most sensible to
    temperature and humidity at lower levels. The regression model then has to
    infer the upper level atmospheric state based on its correlation with
    the lower levels resulting in smooth profiles that have the quality of a
    climatological average. In terms of atmospheric structures it has been
    observed that predictions by linear regression models are rather smooth and
    models are able to resolve at most the lowest inversion of many
    \cite[authoryears][Crewell2007]. Studies with other regression models, most
    notably neural networks, suggest that these aspects might be improved upon
    by models better able to approximate the non-linear inversion problem
    \cite[alternative=authoryears,left={(see also section
    \in[ch:otherretrievals] or }][Churnside1994].

    The classical approach to linear regression is the least squares framework
    but the Bayesian framework provides an equivalent derivation presented
    here. In the subsequent derivation $x$ is a component of the atmospheric
    state vector and $\VECY$ is a  simultaneous radiometer measurement.

    Consider a model $f$ of a linear combination of $n$ fixed basis functions
    $\VECPHI = \startpmatrix[n=3] \NC \phi_1 \NC \dots \NC \phi_n \NR
    \stoppmatrix$ (a row vector) approximating a scalar
    function $t$ of an input $\VECY \in \REALS^d$

    \startformula
        t(\VECY) \approx f(\VECY, \VECW) = w_0 + \sum_{i=1}^n w_i \phi_i(\VECY)
            = \sum_{i=0}^n w_i \phi_i(\VECY) = \VECPHI(\VECY) \VECW
    \stopformula

    where an additional constant basis function $\phi_0(\VECY) = 1$ is
    introduced to $\VECPHI$ for modelling the constant term without the need to
    explicitly consider it in the following derivations. A common choice for
    basis functions in retrievals are indentity functions for individual
    components, i.e. $n = d$ and

    \startformula
        \phi_0(\VECY) = 1,~ \phi_1(\VECY) = y_1, ~ \dots, \phi_d = y_d
    \stopformula

    with $\VECY^\top = \startpmatrix[n=3] \NC y_1 \NC \dots \NC y_d \NR \stoppmatrix$,
    but basis is not restricted to linear functions. The objective of
    regression is to find a a parameter vector $\VECW$ from a given set of
    $(t(\VECY), \VECY)$ pairs that is in some sense optimal. The Bayesian
    approach even results in an entire probability distribution for $\VECW$
    although it will be shown that the usefulness of this distribution is
    limited.

    As usual, the assumption is that the deviations between the true function
    $t$ and its approximation $f$ follows a Gaussian distribution with zero
    mean and covariance $\BETAI$

    \placeformula
    \startformula
    \startalign[n=3,align={left,right,left}]
        \NC \NC \PROB{t(\VECY) - f(\VECY, \VECW) \GIVEN \beta} = \NC
                \GAUSS{t(\VECY) - f(\VECY, \VECW)}{0}{\BETAI} \NR
        \NC \Rightarrow~~ \NC \PROB{x \GIVEN \VECY, \VECW, \beta} = \NC
            \GAUSS{x}{\VECPHI(\VECY) \VECW}{\BETAI} \EQCOMMA \NR[eq:reg_indlikelihood]
    \stopalign
    \stopformula

    where $x = t(\VECY)$ is now understood as the target variable and its
    dependence on $\VECY$ is implied by the conditional variables of the
    probability distribution. Under the assumption that data pairs from
    corresponding datasets $\SETY = \{ \VECY_1, \dots, \VECY_m \}$ and $\SETX
    = \{ x_1, \dots, x_m \}$ are independent, the likelihood function of
    is given by

    \startformula
        \RLIKELIHOOD = \prod_{i=1}^{m} \GAUSS{x_i}{\VECPHI(\VECY_i) \VECW}{\BETAI}
    \stopformula
    
    using \ineq{reg_indlikelihood} or alternatively by

    \startformula
        \RLIKELIHOOD = \GAUSS{\VECX}{\MATPHI \VECW}{\BETAI \MATID}
    \stopformula
    
    with the definitions

    \startformula
        \VECX = \startpmatrix x_1 \NR \vdots\NR x_m \NR \stoppmatrix
        {\rm ~~and~~}
        \MATPHI = \startpmatrix[n=3,align={middle,middle,middle}]
            \NC \phi_0(\VECY_1) \NC \dots \NC \phi_n(\VECY_1) \NR
            \NC \vdots \NC \ddots \NC \vdots \NR
            \NC \phi_0(\VECY_m) \NC \dots \NC \phi_n(\VECY_m) \NR
        \stoppmatrix = \startpmatrix
            \VECPHI(\VECY_1) \NR \vdots \NR \VECPHI(\VECY_m) \NR
        \stoppmatrix
        \EQSTOP
    \stopformula
    
    In order to use Bayes' theorem to obtain the probability distribution of
    $\VECW$ given the data pairs, a prior assumption for the parameter vector
    is necessary. One could just assume that any value of $\VECW$ is equally
    likely since no information about $\VECW$ are available at this point,
    resulting in a maximum likelihood solution.  However, experience shows that
    it is useful to make the regression favor smaller values of $\VECW$ in
    order to obtain a model that is less prone to overfitting and numerically
    more stable. An isotropic Gaussian with mean $\VECZERO$ is therefore
    considered

    \placeformula[eq:reg_prior]
    \startformula
        \RPRIOR = \GAUSS{\VECW}{\VECZERO}{\ALPHAI \MATID}
    \stopformula

    where $\alpha$ governs the covariance of the distribution. With likelihood
    function and prior determined, Bayes' theorem reads

    \startformula
        \RPOSTERIOR = \frac{\RLIKELIHOOD \, \RPRIOR}{\RNORMALIZATION} \EQSTOP
    \stopformula

    The denominator is only a constant normalization, applying the result
    \ineq{gaussbayespost} with substitutions

    \startformula
        \{ \VECX \rightarrow \VECW,~
            \VECA \rightarrow \VECZERO,~
            \MATP \rightarrow \alpha \MATID,~
            \VECY \rightarrow \VECX,~
            \MATB \rightarrow \MATPHI,~
            \VECB \rightarrow \VECZERO,~
            \MATQ \rightarrow \beta \MATID
            \} \EQCOMMA
    \stopformula

    identified from \ineq{gaussbayesinit}, yields the desired posterior
    probability distribution for the parameter vector:

    \startsubformulas[eq:reg_posterior]
    \placesubformula
    \startformula
    \startalign[n=2,align={right,left}]
        \NC \RPOSTERIOR = \NC \GAUSS{\VECW}{\MEANVEC}{\COVMAT} \NR[eq:reg_posterior_gauss]
        \NC \COVMAT = \NC (\beta \MATPHIT \MATPHI + \alpha \MATID)^{-1} \NR[eq:reg_posterior_cov]
        \NC \MEANVEC = \NC \beta \COVMAT \MATPHIT \VECX \EQSTOP \NR[eq:reg_posterior_mean]
    \stopalign
    \stopformula
    \stopsubformulas

    Once the regression model is trained predictions for new measurements
    $\VECY$ are made by evaluating the predictive distribution

    \startformula
    \startalign[n=2,align={right,left}]
        \NC \RPREDICT = \NC
            \int_{\REALS^{n+1}} \PROB{x \GIVEN \VECY, \VECW, \beta} \,
            \RPOSTERIOR \, \diff \VECW \NR
        \NC = \NC
            \int_{\REALS^{n+1}} \GAUSS{x}{\VECPHI(\VECY) \VECW}{\BETAI} \,
                \GAUSS{\VECW}{\MEANVEC}{\COVMAT} \,\diff \VECW \NR
    \stopalign
    \stopformula
    
    which is the likelihood of the measurement given the model
    \ineq{reg_indlikelihood}, marginalized with respect to the posterior
    parameter vector distribution \ineq{reg_posterior_gauss}. With the
    substitution

    \startformula
        \{ \VECX \rightarrow \VECW,~
            \VECA \rightarrow \MEANVEC,~
            \MATP \rightarrow \COVMATI,~
            \VECY \rightarrow x,~
            \MATB \rightarrow \VECPHI(\VECY),~
            \VECB \rightarrow 0,~
            \MATQ \rightarrow \beta \}
    \stopformula

    result \ineq{gaussbayesnorm} is applicable\footnote{The reduction in
    dimension in \ineq{gaussbayesinityx} does not invalidate the result as the
    dependece of the conditional distribution's mean is still linear.},
    yielding

    \startformula
        \RPREDICT = \GAUSS{x}{\VECPHI(\VECY) \MEANVEC}{\BETAI + \VECPHI(\VECY) \COVMAT \VECPHIT(\VECY)}
    \stopformula

    as the predictive distribution. The prediction mean and therefore most
    probable value is the sum of products between the basis functions evaluated
    for the predictors and the the parameter vector's MAP solution, a result
    that is consistent with the expectation from a non-Bayesian approach to
    linear regression. The covariance of the predicted value is a combination
    of the uncertainty inherent in the target variable and uncertainty in the
    model parameters.

    \placefigure[top][fig:bayesian_regression]
        {Predictive distributions of Bayesian regression models trained on 12
        data points obtained from a sine signal with added Gaussian noise of
        standard deviation 0.2 (circles). The line is the mean of the
        predictive distribution, the gray and light gray shaded areas mark
        one and three standard deviations around this mean. The left and center
        models are fifth and first order polynomials respectively, the right
        model uses seven Gaussian basis functions centered between -3 and 3.}
        {\externalfigure[bayesian_regression][width=\textwidth]}

    On first sight, a full description of the predictive distribution seems
    useful for the estimation of uncertainty in predictions.  But care has to
    be taken in its interpretation because the distribution describes
    uncertainty in the model parameters but not the choice of model itself,
    i.e. the choice of basis functions. The uncertainty obtained from the
    predictive distribution is just one term in the overall error estimate and
    on its own only really useful if the chosen model can accurately describe
    the variability in the data.  Figure \in[fig:bayesian_regression]
    illustrates how the choice of basis functions affects the predictive
    distribution. The model shown in the left frame is a polynomial able
    to describe the variability of the data and gives a good fit inside the
    data domain and an appropriate uncertainty range outside the domain. The
    model in the middle is a line with just slope and offset as parameters. It
    is not only a bad fit for the data in- and outside the domain but the
    predictive distribution massively overstates its accuracy. For the given
    model this is a good fit and the parameters can be determined with high
    accuracy but considering all possible models it is a bad fit because the
    model cannot express variability sufficiently. On the right seven Gaussian
    basis functions are used, giving a good fit inside the data domain with
    appropriate uncertainty estimates. Because the basis functions are
    localized, the error estimate from the predictive distribution however
    reduces to the inherent uncertainty of the target data and no contribution
    from the model exists anymore (the covariance is dominated by $\BETAI$
    since $\VECPHI(\VECY)$ vanishes away from the basis functions).

    The treatment so far only considered multiple predictors but only a scalar
    target. For the retrieval of an atmospheric profile from radiometer
    measurements, each atmospheric layer is a target variable resulting in a
    one dimensional target vector. The common approach in this situation is to
    use the same basis functions for each target component and derive a
    parameter matrix instead of a parameter vector $\VECW$. In the derivation
    of this matrix it becomes apparent that the problem decouples into
    independent, scalar regression problems for each target component
    \cite[Bishop2006]. The results from above are therefore directly applicable
    to multiple targets if the assumption holds that the errors of each target
    variable are identically and independently distributed, an assumption that
    is reasonable for the random errors of the sensors of a radiosonde.

\stopsection


\startsection[title={Other Retrieval Techniques},reference=ch:otherretrievals]

    Physical retrievals: optimal estimation standard but also experiments with
    2D-VAR (REF) and efforts to directly assimilate measured brightness
    temperatures into NWP models.

    Aside from linear regression models, neural network have a history of use
    in retrieval. Non-linear in its parameters, theoretically able to
    approximate any complex function. Training phase is computationally more
    expensive than that of linear regression but evaluation is again cheap once
    the model parameters have been found. Most neural networks for the
    retrieval of thermodynamic profiles are rather simple, typically with one
    hidden layer and only a single type of activation function. The overall
    performance of neural networks in terms of the root mean square error has
    not been found to be consistently better than that of linear regression.
    There are indications that retrieval performance is better in
    meteorological extremes such as inversion cases despite worse root mean
    square errors overall \cite[authoryears][Churnside1994]. Non-linear
    regression models have been found to react stronger to a bias
    \cite[authoryears][Martinet2015] and noise \cite[authoryears][Cimini2006]
    in the input data.  Research has been focused on linear regression and
    neural networks so far but many other regression models exist and have not
    been used yet.  All regression models are generally easy to use but
    measures have to be taken to avoid overfitting. They often lack methods to
    estimate uncertainty and, as shown for the case of linear regression
    (Figure \in[fig:bayesian_regression]), even if an uncertainty estimate can
    be derived it might have very limited information content. In contrast to
    optimal estimation retrievals which explicitly use a forward model,
    profiles determined by regression methods need not be consistent with the
    observed brightness temperatures they are retrieved from.

    Aside from the method itself there are additional components of the
    retrieval that can be tweaked. \cite[Frate1998,Tan2011] have used principal
    component analysis (PCA)/empirical orthogonal functions (EOF) to reduce the
    dimensionality of the retrieval problem. This is possible because
    simultaneous measurements from different radiometer channels are correlated
    and contain less independent pieces of information than the number of
    channels REF. It is also possible to use dimension reduction techniques on
    the atmospheric state or to apply variable transformations. The choice of
    variables that quantify the atmospheric state is generally of importance.
    While regression techniques are able to retrieve temperature, humidity or
    liquid water isolated from one another, optimal estimation techniques are
    more restricted. This is discussed further in section \in[ch:statevector].
    Regression methods are able to retrieve only partial profiles, while
    optimal estimation is dependent on a forward model which often needs
    a comprehensive description of the atmospheric state in order to make
    accurate predictions.

\stopsection


\startsection[title={Priors and Training Data Sets}]

    Choosing the prior (=background) state. Constructing the covariance
    matrix.

    Choosing prior faces same challenges as chossing a training data set
    for regression.

    Important since radiometer information mainly limited to lower
    atmosphere \cite[Caddedu2013,Cimini006,Lohnert2012].

    Fixed points ....

    Completely synthetic training data by \cite[Frate1998].

    A-priori knowledge may produce bias, pulling retrievals towards statistical
    mean \cite[Caddedu2002]. Possible solution for optimal estmation not based
    on model output and regression techniques: specialization into seasons
    (e.g. \cite[Chan2010,Tan2011,Massaro2015,Peckam2000]) or even monthly
    \cite[Turner2007]. Cloud-free sky can often be classified befor retrieval
    (ceilometer, possibly camera) and incorporated into prior/training dataset,
    e.g. \cite[Solheim1998,Lohnert2012,Martinet2015,Xu2015]. Transition
    between different priors may not be smooth and has to be managed
    \cite[Massaro2015].

\stopsection

\startsection[title={Inclusion of Additional Information}]

    Current and important topic of research. Many sites that operate
    a radiometer also operate other instruments such as infrared
    spectrometers, ceilometers, cloud radars or surface in-situ sensors.
    Radiosonde climatologies and NWP forecasts discussed in the previous
    section are also information not obtained by the radiometer that are added
    to the retrieval. Most radiometers have built in pressure, humidity and
    temperature sensors.  Naturally there is a desire to use information from
    other instruments to improve the retrieval based on radiometer
    observations.

    Regression methods make it easy to include such measurements as additional
    regressors but the retrieved atmospheric state is not necessarily
    constrained to conform to additional information. While additional
    information have an obvious place in the prior of optimal estimation
    retrievals it generally not that obvious how the prior should be set up
    to accomodate various kinds of additional information. A NWP forecast
    directly provides a complete profile as needed for the prior but a cloud
    base height or surface measurement does not directly lead to an entire
    profile from which a prior can be determined. Such isolated measurements
    may be included as fixed points of the prior by modifying its mean and
    covariance matrix explicitly \cite[authoryears][Bleisch2012]. In-situ
    measurements of the atmospheric state can be directly used to set a value
    of the prior while cloud base heights may be translated to a saturated
    layer and cloud radar observations can be used to explicity prescribe the
    vertical distribution of liquid water
    \cite[authoryears][Lohnert2004,Turner2007]. Modification of the prior is
    generally not trivial and care has to be taken not to make the covariance
    matrix singular. If there is no easy way to include additional information
    into the prior it might be sufficient to use them for the first guess of
    the optimal estimation iterative procedure in order to encourage
    convergence to a profile that is consistent with the additional knowledge.
    A similar effect has the use of regularization terms as discussed in
    section \in[ch:costfunction]. Profiles known to be cloud-free may be
    retrieved with a penalty on cloud liquid water or superadiabatic layers may
    be supressed by lapse rate penalties.

    For instruments from which entire atmospheric profiles can be retrieved
    and for which numerical forward models exists, multiple Bayesian updates
    in the optimal estimation framework are the possible. These updates can
    either be performed sequentially one model after the other or
    simulateously. The latter case is treated by \cite[Rodgers2000] in chapter
    4.1.1 where a general way to construct the posterior {\PDF} $\POSTERIOR$
    from multiple measurements is given. If $n$ measurements $\VECY_i$ are
    available that can be related to the state vector by forward models
    $\FWD_i$ and the forward model errors are pairwise independent, then Bayes'
    theorem takes the form

    \startformula
    \startalign[n=2,align={right,left}]
        \NC \PROB{\VECX \GIVEN \VECY_1,\dots,\VECY_n} =
            \NC \frac{\PROB{\VECY_1,\dots,\VECY_n \GIVEN \VECX}\PRIOR}{\PROB{\VECY_1,\dots,\VECY_n}} \NR
        \NC = \NC \frac{\PRIOR}{\PROB{\VECY_1,\dots,\VECY_n}} \prod_{i=1}^n \PROB{\VECY_i \GIVEN x} \NR
        \NC = \NC \frac{\PRIOR}{\PROB{\VECY_1,\dots,\VECY_n}}
            \prod_{i=1}^n \GAUSS{\VECY_i}{\FWD_i(\VECX) + \MEANVEC_i}{\COVMAT_i} \EQCOMMA \NR
    \stopalign
    \stopformula

    where the independence property of the forward model errors has been
    used for the second identity. With this formulation, any measurements
    that give information on the state vector can be included in the
    optimal estimation procedure. This includes the a-priori knowledge,
    which might be interpreted as a \quotation{virtual measurement} having
    the identity function as a forward model and the prior state
    uncertainties as errors. In this interpretation, no prior knowledge
    is explicitly included in the model ($\PRIOR$ is constant) and the
    maximum a-posteriori solution is equivalent to a maximum likelihood
    solution.

    There is some controversy regarding the usefulness of surface observations
    with some authors claiming they are too heavily influenced by inhomogeneous
    surface layer processes and are not representative of the air volume that
    is responsible for the radiometer observations
    \cite[authoryears][Crewell2007]. Research in Innsbruck has been focused on
    the inclusion of in-situ measurements from surface stations at different
    altitudes as they are available in mountainous locations and found that
    these do have a positive effect as additional regressors in a linear model
    despite not directly probing the volume that affects the radiometer
    \cite[authoryears][Massaro2013,Meyer2016].

\stopsection

