Formulate retrieval problem.

Introduce optimal estimation and regression mention Kernel Methods, ... as
alternative regression methods.

Math is heavily influenced by Rodgers and Bishop, important calculations are
repeated to achieve consistent notation and naming conventions throughout.

\startsection[title={Bayesian Statistics}]

    The Bayesian view of statistics is ideal for the formulation of retrieval
    problems. 

    Conditional probability, diachronic interpreation, subjective priors,
    making assumptions explicit, focus on pdf from which values can be
    extracted due to different criteria, pdf carries uncertainty estimates,
    sampling from pdfs...

    Central role: Bayes' theorem.
    
    \placeformula[eq:bayes_theorem]
    \startformula
        \PROB{x|y} = \frac{\PROB{y|x} \PROB{x}}{\PROB{y}}
    \stopformula

    $\PROB{x|y}$ likelihood

    $\PROB{y|x}$ posterior

    $\PROB{x}$ prior

    $\PROB{y}$ normalization, can be obtained by integrating $\PROB{y|x}\PROB{x}$
    over all possible states $x$.

    \placefigure[bottom][fig:bayes_theorem]{Visualization of Bayes' theorem.} {\externalfigure[bayes_theorem][width=\textwidth]}

    Describe different kinds of solutions: maximum likelihood, MAP, ...

    Mention Monte Carlo methods.

\stopsection

\startsection[title=The Multivariate Gaussian Distribution]

    Bayesian view relies heavily on probablility density functions. In a
    theoretical framework it is desireable to work with pdfs that allow
    analytic solutions for a given problem. The most commonly used pdf is the
    Gaussian distribution:

    \placeformula[eq:gaussian]
    \startformula
        \GAUSS{\VECX}{\MEANVEC}{\COVMAT}
        = \frac{1}{(2 \pi)^{d / 2} \det(\COVMAT)^{1/2}}
          \exp \left( -\frac{1}{2}(\VECX - \MEANVEC)^\top \COVMAT^{-1} (\VECX - \MEANVEC) \right)
    \stopformula

    where $\VECX, \MEANVEC \in \REALS^d$ and $\COVMAT \in \REALS^{d
    \times d}$ and the exponential function is applied component-wise.

    The assumption that a continuous variable follows a Gaussian distribution
    is not only popular due to its well understood analytic behavior. central
    limit theorem, maximum entropy.

    The mean vector $\MEANVEC$ of the Gaussian distribution is the
    location of its maximum. The covariance matrix $\COVMAT$ determines
    the shape of the distribution. For a proper Gaussian distribution, it is
    always a symmetric positive definite matrix. When all off-diagonal elements
    of $\COVMAT$ are zero, each component of the multidimensional
    distribution is independent from the others. If nonzero off-diagonal
    elements exist, some or all componentes are correlated.

    Discuss possibility of determining mean of Gauss by search for extrema.

    Refer to default Bayes/Gaussian plot.

    Because the normalization $((2 \pi)^{d/2} \det(\COVMAT)^{1/2})^{-1}$ can
    be determined from the content of the quadratic form in the exponent,
    i.e. the size of the input $N$ and the covariance matrix $\COVMAT$, a
    Gaussian distribution can be described completely in terms of the exponent.
    A common representation of a Gaussian distribution, that will also be used
    in the following, is

    \startformula
        -2 \ln(\GAUSS{\VECX}{\MEANVEC}{\COVMAT})
        = (\VECX - \MEANVEC)^\top \COVMAT^{-1} (\VECX - \MEANVEC) + c \EQCOMMA
    \stopformula

    where $c$ is a constant that contains the normalization. The right hand
    side of this representation is easier to deal with in analytical
    derivations but contains all the information of the distribution.

    Because the assumption of Gaussian {\PDF}s is persistent throughout this
    text, some general results are derived in the following sections. These
    will be used in subsequent sections.

    \startsubsection[title=Maximum Likelihood Estimators for the Parameters]
        
        How best to determine mean and cov from a given dataset.

    \stopsubsection

    \startsubsection[title={Bayes' Theorem for Gaussians},reference=ch:bayes_gauss]

        A convenient property of Gaussian distributions is that the product of
        a Gaussian conditional distribution $\LIKELIHOOD$ (the likelihood
        function) and a Gaussian marginal distribution $\PRIOR$ (the prior) is
        again Gaussian \cite[authoryears][Bishop2006]. This allows the analytic
        calculation of the posterior $\POSTERIOR$ if two assumptions hold: the
        dependence of the conditional distribution's mean is linear with
        respect to the prior's argument ($\VECX$) and its covariance is
        independent of $\VECX$. Let

        \startformula
        \startalign[n=2,align{right,left}]
            \NC -2 \ln(\LIKELIHOOD) = \NC
                \GAUSSEXP{\VECY}{\FWDJAC\VECX - \VECB}{\COVMATERR} + c_1 \NR
            \NC -2 \ln(\PRIOR) = \NC
                \GAUSSEXP{\VECX}{\MEANVECA}{\COVMATA} + c_2 \EQSTOP \NR
        \stopalign
        \stopformula

        The normalization is contained in $c_1$ and $c_2$ respectively. These
        {\PDF}s are multivariate Gaussians fulfilling the above assumptions.
        The posterior distribution is given by:

        \placesubformula
        \startformula
        \startalign[n=2, align{right,left}]
            \NC -2 \ln(\POSTERIOR) = \NC -2 \ln(\LIKELIHOOD \PRIOR) \NR
            \NC = \NC -2 \ln(\LIKELIHOOD) - 2 \ln(\PRIOR) \NR
            \NC = \NC
                \GAUSSEXP{\VECY}{\FWDJAC - \VECB}{\COVMATERR}
                + \GAUSSEXP{\VECX}{\MEANVECA}{\COVMATA} + c_1 + c_2
                \NR
            \NC = \NC
                  \VECY^\top \COVMATERR^{-1} \VECY
                - \VECY^\top \COVMATERR^{-1} \FWDJAC \VECX
                - \VECY^\top \COVMATERR^{-1} \VECB
                \NR
            \NC \NC
                \mathbox{cornflowerblue}{- \VECX^\top \FWDJAC^\top \COVMATERR^{-1} \VECY}
                + \mathbox{yellowgreen}{\VECX^\top \FWDJAC^\top \COVMATERR^{-1} \FWDJAC \VECX}
                + \mathbox{cornflowerblue}{\VECX^\top \FWDJAC^\top \COVMATERR^{-1} \VECB}
                \NR
            \NC \NC
                - \VECB^\top \COVMATERR^{-1} \VECY
                + \VECB^\top \COVMATERR^{-1} \FWDJAC \VECX
                + \VECB^\top \COVMATERR^{-1} \VECB
                \NR[eq:gaussprodexpand]
            \NC \NC
                + \mathbox{yellowgreen}{\VECX^\top \COVMATA^{-1} \VECX}
                ~\mathbox{cornflowerblue}{- \VECX^\top \COVMATA^{-1} \MEANVECA}
                - \MEANVECA^\top \COVMATA^{-1} \VECX
                + \MEANVECA^\top \COVMATA^{-1} \MEANVECA
                \NR
            \NC \NC + c_1 + c_2 \EQCOMMA \NR
        \stopalign
        \stopformula
        
        where the last step is expanding the vector-matrix-vector product.
        Subscripts are chosen in anticipation of chapter
        \in[ch:optimalestimation]. The posterior has to be Gaussian, therefore

        \placesubformula
        \startformula
        \startalign[n=2,align={right,left}]
            \NC -2 \ln(\POSTERIOR) = \NC
                \GAUSSEXP{\VECX}{\MEANVEC}{\COVMAT} + c_3 \NR
            \NC = \NC
                 \mathbox{yellowgreen}{\VECX^\top \COVMAT^{-1} \VECX}
                 ~\mathbox{cornflowerblue}{- \VECX^\top \COVMAT^{-1} \MEANVEC}
                 - \MEANVEC^\top \COVMAT^{-1} \VECX
                 + \MEANVEC^\top \COVMAT^{-1} \MEANVEC
                 + c_3 \NR[eq:gausspostexpand]
        \stopalign
        \stopformula

        must ben a equivalent representation. Noting that the equivalence must
        hold for all $\VECX$, an expression for $\COVMAT$ is foundy by
        matching terms quadratic in $\VECX$ of both forms
        \ineq{gaussprodexpand} and \ineq{gausspostexpand} (highlighted
        green):

        \placesubformula
        \startformula
        \startalign[n=3,align={left,right,left}]
            \NC \NC - \VECX^\top \COVMAT^{-1} \VECX = \NC
                \VECX^\top \FWDJAC^\top \COVMATERR^{-1} \FWDJAC \VECX
                + \VECX^\top \COVMATA^{-1} \VECX
             = \VECX^\top (\FWDJAC^\top \COVMATERR^{-1} \FWDJAC
                + \COVMATA^{-1}) \VECX \NR
            \NC \Rightarrow~~ \NC \COVMAT = \NC
                (\FWDJAC^\top \COVMATERR^{-1} \FWDJAC + \COVMATA^{-1})^{-1}
                \EQSTOP \NR[eq:gausspostcov]
        \stopalign
        \stopformula

        Matching the linear terms involving $\VECX^\top$ of both forms
        \ineq{gaussprodexpand} and \ineq{gausspostexpand} (highlighted
        blue) results in an expression for $\MEANVEC$:

        \placesubformula
        \startformula
        \startalign[n=3,align={left,right,left}]
            \NC \NC - \VECX^\top \COVMAT^{-1} \MEANVEC = \NC
                - \VECX^\top \FWDJAC^\top \COVMATERR^{-1} \VECY
                + \VECX^\top \FWDJAC^\top \COVMATERR^{-1} \VECB
                - \VECX^\top \COVMATA^{-1} \MEANVECA \NR
            \NC \NC  = \NC
                - \VECX^\top (\FWDJAC^\top \COVMATERR^{-1} (\VECY - \VECB)
                + \COVMATA^{-1} \MEANVECA) \NR
            \NC \Rightarrow~~ \NC \MEANVEC = \NC
                \COVMAT (\FWDJAC^\top \COVMATERR^{-1} (\VECY - \VECB)
                + \COVMATA^{-1} \MEANVECA)
                \EQCOMMA \NR[eq:gausspostmean]
        \stopalign
        \stopformula

        where $\COVMAT$ is given by \ineq{gausspostcov}. Equating terms
        linear in $\VECX$ yields the same result.  An alternative derivation of
        \ineq{gausspostcov} and \ineq{gausspostmean} is given by
        \cite[Bishop2006] in chapter 2.3.3.

    \stopsubsection

    \startsubsection[title=Diagonalization of the Covariance Matrix]

        Transformation into eigenspace. Used for uncertainty estimates of
        linear regression technique.

    \stopsubsection

    
    Plot stuff.

    cite Bishop, Rodgers

\stopsection


\startsection[title={Optimal Estimation},reference=ch:optimalestimation]

    Reference Rodgers, explain how it fits together.

    The optimal estimation approach directly uses Bayes' theorem
    \ineq{bayes_theorem} to obtain the posterior probability distribution
    $\POSTERIOR$ using a forward model and prior information of the state
    vector.

    Call it physical retrieval because it uses the forward model.

    The assumptions for the prior {\PDF} and the likelihood function
    are that they are Gaussian.

    Justify assumptions.

    The prior $\PRIOR$ should contain all knowledge about the state vector
    before the measurement is evaluated. This knowledge might come from
    a climatology, output from a NWP model, information from other instruments
    and any combination of these sources. The construction of an optimal prior
    is not an easy task and will be discussed in section
    \in[ch:construct_prior]. In any case, due to the assumption that the
    {\PDF} is Gaussian it will have the form:

    \placeformula[eq:optimalest_prior]
    \startformula
        \PRIOR = \GAUSS{\VECX}{\MEANVECA}{\COVMATA}
    \stopformula

    The subscript stands for \quotation{a-priori}.

    The likelihood function $\LIKELIHOOD$ expresses the confidence that the
    observation $\VECY$ is the result of a state $\VECX$. This confidence is
    quantified with the help of the forward model, which can be formulated as:

    \placeformula[eq:forward_model]
    \startformula
        \VECY = \FWD(\VECX) + \ERR
    \stopformula

    Where $\FWD$ is the forward model operator and $\ERR$ is an error term,
    arising from measurement noise, inaccuracies of the forward model,
    interpolation errors and possibly other terms. Again, the assumption is
    that the errors are Gaussian, centered around a systematic bias
    $\MEANVECERR$ and with a convariance matrix $\COVMATERR$:

    \placeformula[eq:optimalest_errpdf]
    \startformula
        \PROB{\ERR} = \GAUSS{\ERR}{\MEANVECERR}{\COVMATERR}
    \stopformula

    A discussion of how $\MEANVECERR$ and $\COVMATERR$ can be determined is
    given in section \in[rtm_errors]. Substituting \ineq{forward_model}
    into \ineq{optimalest_errpdf} and changing the independent variable of
    the {\PDF} results in the likelihood function:
    
    \placesubformula
    \startformula
    \startalign[n=3,align={right,middle,left}]
        \NC \PROB{\VECY - \FWD(\VECX)} = \NC
            \GAUSS{\VECY - \FWD(\VECX)}{\MEANVECERR}{\COVMATERR} \NC \NR
        \NC = \NC \GAUSS{\VECY}{\FWD(\VECX)+\MEANVECERR}{\COVMATERR} \NC
            = \LIKELIHOOD \NR[eq:optimalest_likelihood][]
    \stopalign
    \stopformula

    Knowing \ineq{optimalest_prior} and \ineq{optimalest_likelihood} the
    posterior {\PDF} is given by

    \placeformula[eq:optimalest_posterior]
    \startformula
        \POSTERIOR
        = \frac{\LIKELIHOOD \PRIOR}{\NORMALIZATION}
        = \frac{\GAUSS{\VECY}{\FWD(\VECX) + \MEANVECERR}{\COVMATERR}
            ~\GAUSS{\VECX}{\MEANVECA}{\COVMATA}}{\NORMALIZATION} \EQCOMMA
    \stopformula

    but because the forward model is a nonlinear function, the product of the
    Gaussians cannot be carried out using the results from section
    \in[ch:bayes_gauss] yet.

    \startsubsection[title=An Iterative Solution]

        Taylor series around a state $\ITER{\MEANVEC}{i}$, throw away higher order
        terms

        \startformula
            \FWD(\VECX) \approx \FWD(\ITER{\MEANVEC}{i})
                + \ITER{\FWDJAC}{i} (\VECX - \ITER{\MEANVEC}{i})
        \stopformula

        Insert into \ineq{optimalest_posterior} to obtain an expression for
        the posterior where the mean of the likelihood function is linear in
        $\VECX$ and \ineq{gausspostcov} and \ineq{gausspostmean} can be used to
        determine the posterior distribution parameters. Because of the
        linearization the solutions for $\MEANVEC$ and $\COVMAT$ will not be
        optimal. Use fixed-point iteration to find the optimal solution,
        leading to an iterative scheme

        \startsubformulas[eq:gausspostiter]
        \placesubformula
        \startformula
        \startalign[n=3,align={right,left,right}]
            \NC \ITER{\MEANVEC}{i+1} = \NC
                \ITER{\COVMAT}{i} (\ITER{\FWDJAC}{i}^\top \COVMATERR^{-1}
                (\VECY - \MEANVECERR - \FWD(\ITER{\MEANVEC}{i})
                + \ITER{\FWDJAC}{i} \ITER{\MEANVEC}{i}) + \COVMATA^{-1} \MEANVECA)
                \EQCOMMA \NC \NR[eq:gausspostmeaniter][a]
            \NC \ITER{\COVMAT}{i} = \NC
                (\ITER{\FWDJAC}{i}^\top \COVMATERR^{-1} \ITER{\FWDJAC}{i}
                + \COVMATA^{-1})^{-1}
                \EQSTOP \NC \NR[eq:gausspostcoviter][b]
        \stopalign
        \stopformula
        \stopsubformulas

        Using the identity

        \startformula
            \MEANVECA - \ITER{\COVMAT}{i} \ITER{\FWDJAC}{i}^\top
            \COVMATERR^{-1} \ITER{\FWDJAC}{i} \MEANVECA =
            \ITER{\COVMAT}{i} (\ITER{\COVMAT}{i}^{-1}
            - \ITER{\FWDJAC}{i}^\top \COVMATERR^{-1} \ITER{\FWDJAC}{i})
            \MEANVECA = \ITER{\COVMAT}{i} \COVMATA^{-1} \MEANVECA \EQCOMMA
        \stopformula

        it is easily seen that \ineq{gausspostmeaniter} is equivalent to
        equation 5.9 from \cite[Rodgers2000], which he obtained by finding
        for the maximum of the posterior {\PDF} with the Gauss-Newton method.
        The next section expands on this approach by introducing an iterative
        procedure that is more robust than basic fixed point iteration.

    \stopsubsection

    \startsubsection[title=A Cost Function Approach]

        MAP = finding maximum of a cost function (refer to property of Gaussian
        that maximum=mean). Argue that one still obtains full information about
        distribution in case of Gaussian assumption. Solve nonlinear problem by
        Levenberg-Marquart, show equivalence to previous result by fixing gamma
        parameter.

        Explain where 1D-VAR comes from: starting with minimization of
        variance.
        
    \stopsubsection

    \startsubsection[title=Introducing Constraints by Regularization]

        Refer to cost function version of MAP solution.

        Introduce penalties to cost function. Explain how is nothing else than
        adding additional prior information (interpret each penalty term as its
        own distribution). Advantage: Simpler to set up than modifying prior,
        but no representation of additional knowledge in posterior since MAP
        solution does not give uncertainty and a general analytic solution for
        the posterior is impossible when PDFs more complicated than Gaussian
        appear. Mention Monte Carlo as solution but very high associated
        compuational costs.

        Example: \cite[Hewison2006], penalizing high LWC and references
        therein.

    \stopsubsection

    \startsubsection[title=A Generalization to Multiple Measurements]

        As shown by \cite[Rodgers2000] in chapter 4.1.1, there is a more
        general way to construct the posterior {\PDF} $\POSTERIOR$. If $n$
        measurements $\VECY_i$ are available that can be related to the state
        vector by forward models $\FWD_i$ and the forward model errors are
        pairwise independent, then Bayes' theorem takes the form

        \startformula
        \startalign[n=2,align={right,left}]
            \NC \PROB{\VECX|\VECY_1,\dots,\VECY_n} =
                \NC \frac{\PROB{\VECY_1,\dots,\VECY_n|\VECX}\PRIOR}{\PROB{\VECY_1,\dots,\VECY_n}} \NR
            \NC = \NC \frac{\PRIOR}{\PROB{\VECY_1,\dots,\VECY_n}} \prod_{i=1}^n \PROB{\VECY_i|x} \NR
            \NC = \NC \frac{\PRIOR}{\PROB{\VECY_1,\dots,\VECY_n}}
                \prod_{i=1}^n \GAUSS{\VECY_i}{\FWD_i(\VECX) + \MEANVEC_i}{\COVMAT_i} \EQCOMMA \NR
        \stopalign
        \stopformula

        where the independence property of the forward model errors has been
        used for the second identity. With this formulation, any measurements
        that give information on the state vector can be included in the
        optimal estimation procedure. This includes the a-priori knowledge,
        which might be interpreted as a \quotation{virtual measurement} having
        the identity function as a forward model and the prior state
        uncertainties as errors. In this interpretation, no prior knowledge
        is explicitly included in the model, i.e. $\PRIOR$ is constant, and the
        maximum a-posteriori solution is equivalent to a maximum likelihood
        solution.

        Give formulas for combined map and cov solution from Rodgers, this
        might actually be used for the combination of COSMO7 and Nordkette
        measurements.
        
    \stopsubsection

    \startsubsection[title={Constructing the Prior},reference=ch:construct_prior]

        Choosing the prior (=background) state. Constructing the covariance
        matrix.

    \stopsubsection

\stopsection


\startsection[title=Linear Regression]

    Based on climatology, where to get climatology from. Idea: approximate
    the inverse model from climatology instead of introducting a numerical
    physical model.

    \startsubsection[title=Quantifying Uncertainty]

        Transformation into eigensystem, diagonal covariance.

    \stopsubsection

    \startsubsection[title=Including Additional Information]

        Adding regressors. No explicit forward model or error estimate needed.
        Who has done it? Problem: no constraint, rather a suggestion. Known
        elements of state vector are not enforced.

    \stopsubsection
    
\stopsection


\startsection[title=Neural Network Regression]

    Who has used it? Non-linearity. Extrapolation problems. No error estimates
    from common formulation. Choice of hidden layer and activation functions.
    Advantage in resolving low level inversions?

\stopsection


\startsection[title=Comparison of Techniques]

    Ease of use, computational efficiency, inclusion of additional information.
    Information content of output: uncertainty.

\stopsection

