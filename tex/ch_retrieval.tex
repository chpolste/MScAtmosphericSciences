The retrieval problem of atmospheric remote sensing can be stated as: given
a measurement of a sounding device, derive some part of the atmospheric state,
for example a vertical profile of temperature. The inherent difficulty of this
procedure is that the required inversion of radiative transfer is an ill-posed
problem due to the complex nature of electromagnetic wave propagation in a
medium like air. When a passive remote sensing instrument observes atmospheric
radiation, its measurement is the result of emission and extinction by all
layers of the atmosphere. In general, multiple atmospheric states exist that
result in very similar observations by the instrument and additional information
is necessary to constrain the state space such that a unique solution, ideally
the true atmospheric state, exists. These constraints can be a climatology,
a numerical forecast or measurements from other instruments.

Depending on the available information and resources, different retrieval
techniques for the inversion are be applicable. A variety of statistical
regression models exists which try to directly describe the inversion problem.
These models approximate the atmospheric state based on an observation by
a remote sensing instrument after being trained on a data set representative of
the relationship which the model tries to imitate. Regression methods are
generally easy to set up if such a data set is available, but they a prone to
large errors in situations not adequately represented in the training data.
Other retrieval methods use a numerical model of the forward problem
to iteratively determine a solution consistent with given constraints on the
atmospheric state and physical processes as simulated by the forward model.
These methods are called optimal or physical methods.

In this chapter, an optimal retrieval technique and the linear regression
method are derived from a purely Bayesian approach to statistics. Although all
following results have been given before, their derivations are repeated here
in order to achieve consistent notation and discuss the benefits of operating
in a purely Bayesian framework. The mathematics are based on the work of
\cite[Rodgers2000], who treats all aspects of the retrieval problem of
atmospheric sounding partially in the Bayesian framework. His results are used
for the derivation of the optimal estimation technique. Fundamental results
of Bayesian statistics, properties of the Gaussian distribution and the
Bayesian approach to linear regression are taken from \cite[Bishop2006]. The
introduction to Bayesian statistics is additionally influenced by
\cite[Downey2013].

\startsection[title={Bayesian Statistics}]

    A probability always refers to some entity, which may be a hypothesis, an
    event or the value of some physical variable. By convention, probability is
    scaled to the interval $[0, 1]$ representing a continuum of certainty about
    the state of the associated entity. For the example of a hypothesis,
    a probability of 0 stands for the absolute certainty that the hypothesis is
    false, 1 stands for the absolute certainty that it is true and values
    between 0 and 1 express certainties in between these extremes.

    A probablity always has an associated probability density function, also
    called probability distribution, probability mass function or just PDF.
    Let $x \in U$ be a continuous variable. Its probability distribution is
    a function $U \rightarrow \REALS$ which is non-negative almost everywhere.
    Integrating the PDF over a subset of $U$ yields the probability that
    $x$ is an element of that subset. Therefore all probability distributions
    must fulfil

    \placeformula[eq:probaint]
    \startformula
        \int_U \PROB{x} \, \diff x = 1 \EQCOMMA
    \stopformula
    
    i.e. the probability that x has any value of all possible values must
    be 1.

    A fundamental concept in Bayesian statistics is that of joint and
    conditional probability. A joint probability is the probability that two
    variables are each elements of specified subsets of their respective
    domains at the same time. A conditional probability is the probability of
    one variable being an element of a given set when it is known that the
    second variable is an element of another given set. Let $x$, $y$ be two
    variables of a statistical scenario. The PDFs associated with joint and
    conditional probabilities of $x$ and $y$ are written as $\PROB{x, y}$ (read
    $x$ and $y$) and $\PROB{x \GIVEN y}$ (read $x$ given $y$) or $\PROB{y
    \GIVEN x}$, respectively. It is possible to extract the probability
    distribution of a variable from a joint distribtion by integrating
    over all possible values of the other variables. For example:

    \startformula
        \PROB{y} = \int_U \PROB{x, y} \, \diff x
    \stopformula

    if $x \in U$. This process is called marginalization.

    The fundamental theorem of Bayesian statistics (aptly named \quote{Bayes'
    theorem}), can be derived from the product rule of probability
    \cite[authoryears][Bishop2006] which reads

    \startformula
        \PROB{x, y} = \PROB{x \GIVEN y} \, \PROB{y} \EQCOMMA
    \stopformula

    i.e. the joint probability distribution of $x$ and $y$ is the product of
    the conditional probability distribution $x$ given $y$ and the PDF of
    $y$ independent of $x$. Because $x$ and $y$ are interchangeable,
    symmetry dictates that

    \startformula
        \PROB{x, y} = \PROB{y \GIVEN x} \, \PROB{x}
    \stopformula

    is valid too. Equating both expressions of the joint PDF and rearranging
    yields Bayes' theorem:

    \placeformula[eq:bayes_theorem]
    \startformula
        \PROB{x \GIVEN y} = \frac{\PROB{y \GIVEN x} \, \PROB{x}}{\PROB{y}} 
            \sim \PROB{y \GIVEN x} \, \PROB{x} \EQSTOP
    \stopformula

    Because all probability distributions fulfil \ineq{probaint}, the
    denominator acts as merely as a normalization and often does not need to be
    determined explicitly. Bayes' theorem allows to change the conditioning
    of a probability distribution from $\PROB{y \GIVEN x}$ to $\PROB{x \GIVEN
    y}$ which otherwise may be hard to compute directly. For retrieval
    applications it is useful to consider the diachronic interpretation of
    Bayes' theorem in which \ineq{bayes_theorem} is viewed as an update of the
    distribution of $x$ in light of some data $y$
    \cite[authoryears][Downey2013]. At first only $\PROB{x}$ is known,
    containing a-priori knowledge of $x$. This PDF is called prior of x. Then,
    some new data $y$ is observed and $\PROB{y \GIVEN x}$ expresses the
    likelihood of this observation under consideration of prior knowlege of
    $x$.  Multiplying prior and likelihood and normalizing the result with
    $\PROB{y}$ yields the so called posterior distribution $\PROB{x \GIVEN y}$,
    which is the now updated distribution of $x$ after having observed $y$. By
    applying Bayes' theorem, the prior knowledge of $x$ is combined with new
    information contained in the distribution of $y$. This process is
    visualized in Figure \in[fig:bayes_theorem] for two two-dimensionally
    distributed variables.

    \placefigure[top][fig:bayes_theorem]
        {Visualization of a Bayesian update. A circular prior PDF is combined
        with an elliptical likelihood function according to Bayes' theorem,
        resulting in a posterior distribution combining the knowledge contained
        in both. Combining both information on the state of the considered
        variable has significantly constrained the high-probability state space
        (shaded) of the posterior relative to the prior.}
        {\externalfigure[bayes_theorem][width=\textwidth]}

    The Bayesian framework is valuable because it deals with entire probability
    distributions instead of plain values. Quantifying the uncertainty of
    a value or a fact by such distributions enables a user to make better
    decisions and obtain an accurate description of the state of some entity.
    A probability distribution also allows sampling, which is useful for the
    generation of synthetic data based on the distribution and for
    visualization of error margins of higher-dimensional variables. However,
    having to deal with probability distributions complicates calculations and
    analytical solutions to specific problems do not always exist. Such
    problems can be treated with discrete representations of the involved PDFs
    and with the use of Monte Carlo methods. \cite[Downey2013] shows that
    these methods can work well but they suffer under the high dimensionality
    of many problems which causes them to be too computationally expensive to
    be practical. Another way to deal with complicated problems in the Bayesian
    framework is by settling for a solution that is just values but still based
    on the underlying distribution of the result. For example, determination
    of the maximum of a probability distribution is often possible with
    analytic methods even for complicated PDFs. Depending on which distribution
    the maximum solution is derived for, it is called the maximum likelihood
    (ML) or maximum a-posteriori (MAP) solution.

    Another issue with the need for probability distributions in Bayesian
    statistics arises when no data are at hand with which to decide how the
    distribution of some variable should look like. At that point, decisions
    become subjective and distributions are chosen based on mathematical
    convenience or personal experience. Allowing subjectivity in a scientific
    method initially sounds like a bad idea, but here it is argued that the way
    this issue is treated in the Bayesian framework is favorable over its
    alternatives. The reason is not that subjectivity itself is good but that
    it is unavoidable. Most alternative methods starting from a non-Bayesian
    approach make the same subjective assumptions as a substitute Bayesian
    technique, only that the assumptions are often made implicitly.
    This can be seen in the case of linear regression, which will be derived
    from a Bayesian approach in section \in[ch:linear_regression]. The
    assumption of Gaussian-distributed target variables and regression
    parameters leads to the same solution in the Bayesian framework as the
    classical approach of minimizing the sum of squared residuals with an $L^2$
    regularization term. The equivalence of solutions implies that both methods
    rely on the same assumptions. In the Bayesian framework they ar explicit
    and exposed and the extension to different assumptions is obvious. The
    classical approach hides these assumptions behind a sum of squares whose
    connection to the Gaussian distribution is not directly obvious. If a user
    is explicitly aware of the assumptions on which a model is built, he/she
    can include this information into the interpretation of results and
    assessments of applicability of the model to various scenarios, which
    clearly is an advantage.

\stopsection

\startsection[title=The Multivariate Gaussian Distribution]

    For theoretical derivations in the Bayesian framework it is desireable to
    work with probability distributions for which analytic solutions to common
    problems exist. The most prominent example of such a distribution is the
    Gaussian distribution

    \placeformula[eq:gaussian]
    \startformula
        \GAUSS{\VECX}{\MEANVEC}{\COVMAT}
        = \frac{1}{(2 \pi)^{d / 2} \det(\COVMAT)^{1/2}}
            \exp \left( -\frac{1}{2}(\VECX - \MEANVEC)^\top \COVMAT^{-1}
            (\VECX - \MEANVEC) \right)
    \stopformula

    with $\VECX, \, \MEANVEC \in \REALS^d$ and $\COVMAT \in \REALS^{d \times d}$.
    $\MEANVEC$ is the mean of this distribution and it is also the location of
    the only extremum. This is particularly useful for the determination of
    $\MEANVEC$ from a given Gaussian since a simple extremum search directly
    yields the unique maximum. The symmetric and positive definite covariance
    matrix $\COVMAT$ governs the shape of the distribution. If all off-diagonal
    elements of $\COVMAT$ are zero, all components of the multidimensional
    distribution are independent. If non-zero off-diagonal elements exist, some
    or all components are correlated. The two-dimensional example distributions
    shown in Figure \in[fig:bayes_theorem] are all Gaussian distributions. The
    prior has a diagonal covariance matrix and is therefore circular. The
    likelihood function has non-zero off-diagonal elements resulting in tilted,
    ellipsoidal isolines.

    A Gaussian distribution is characterized by its mean and covariance.
    In particular, the normalization $((2 \pi)^{d/2} \det(\COVMAT)^{1/2})^{-1}$
    can be determined from $\COVMAT$ alone. This simplifies derivations where
    it is known that the result is a Gaussian distribution, because the
    normalization can be inferred directly once $\COVMAT$ is known.
    Bayesian operations benefit from this simplification since the
    marginalization of a Gaussian yields a Gaussian and a conditional
    distribution of two Gaussian variables is again Gaussian if the
    conditioning is linear \cite[authoryears][Bishop2006].

    With the normalization out of the way it is furthermore practical to
    operate with Gaussian distributions in logarithmic space. There,
    \ineq{gaussian} is given by the quadratic form

    \startformula
        -2 \ln(\GAUSS{\VECX}{\MEANVEC}{\COVMAT})
        = \GAUSSEXP{\VECX}{\MEANVEC}{\COVMAT} + c
    \stopformula

    where $c \in \REALS$ is a constant that contains the normalization. This
    form still contains all information from \ineq{gaussian} and is used
    extensively in subsequent derivations.

    Throughout this thesis it will be assumed again and again that the values
    of a continuous variable can be described by a Gaussian distribution. Its
    convenient analytic properties are a major reason for this choice but
    there are other arguments supporting this assumption. From theoretical
    considerations it is known that the Gaussian distribution is
    the best distribution that can be chosen in terms of information content
    when only the mean and variance of a variable are known because it
    comes with the least assumptions about the unknown properties of the
    variable \cite[authoryears][Rodgers2000]. Furthermore, due to the central
    limit theorem, the sum of a set of random variables has a distribution
    approaching a Gaussian when the number of terms in the sum increases
    \cite[authoryears][Bishop2006]. Physical measurements are often time
    averages of observations of higher frequency. The central limit theorem
    causes such averages to have error distributions that are to good
    approximation Gaussian.

\startsubsection[title=Parameter Estimation]
    
    To approximate a data set with the a Gaussian distribution, the mean and
    covariance parameters have to be estimated from the data.  For a given set
    $\{\VECX_1, \dots, \VECX_N\}$ of data, whose elements are assumed to be
    independent and identically distributed, appropriate values are found in
    the maximum likelihood estimators of $\MEANVEC$ and $\COVMAT$. The
    respective expressions are repeated here from the book of
    \cite[Bishop2006] without derivations.

    The likelihood of a single data point $\VECX_i$ given the mean $\MEANVEC$
    and covariance $\COVMAT$ of the distribution it is assumed to be drawn from
    is $\GAUSS{\VECX_i}{\MEANVEC}{\COVMAT}$. Because the data are assumed to be
    independent and identically distributed, the likelihood of the entire
    data set $\{\VECX_1, \dots, \VECX_N\}$ given $\MEANVEC$ and $\COVMAT$ is

    \startformula
        \PROB{\{\VECX_1, \dots, \VECX_N\} \GIVEN \MEANVEC, \COVMAT}
        = \prod_{i=1}^N \GAUSS{\VECX_i}{\MEANVEC}{\COVMAT} \EQCOMMA
    \stopformula

    i.e. the product of all individual likelihood functions. This product of
    Gaussians is again Gaussian and its mean can be found at its maximum
    or equivalently at the minimum of its negative naturam logarithm.
    Determination of the covariance matrix is more involved as it has to be
    symmetric and positive definite. The resulting ML estimates are

    \startformula
        \MEANVEC_{ML} = \frac{1}{N} \sum_{i=1}^N \VECX_i
    \stopformula

    and

    \startformula
        \COVMAT_{ML} = \frac{1}{N} \sum_{i=1}^N (\VECX_i - \MEANVEC_{ML})^\top
            (\VECX_i - \MEANVEC_{ML}) \EQSTOP
    \stopformula

    Notably, the mean is calculated independently from the covariance. The ML
    estimator of the covariance can be proven to systematically underestimate
    the true covariance. The correct estimator is

    \startformula
        \tilde\COVMAT_{ML} = \frac{1}{N-1} \sum_{i=1}^N (\VECX_i - \MEANVEC_{ML})^\top
            (\VECX_i - \MEANVEC_{ML}) \EQSTOP
    \stopformula

\stopsubsection

\startsubsection[title={Bayesian Operations for Gaussians},reference=ch:bayes_gauss]

    Two important results for Gaussian distributions are derived in this
    section which are subsequently used in the optimal estimation and
    linear regression methods.  The derivations are shortened by taking the
    facts that a product of two Gaussians and the marginalization of a Gaussian
    are again Gaussian for granted.  Rigorous derivations of these results are
    given by \cite[Bishop2006].

    The goal of this section is to find the missing probability distributions
    $\POSTERIOR$ and $\NORMALIZATION$ in Bayes' theorem when the prior $\PRIOR$
    and likelihood function $\LIKELIHOOD$ are given by

    \startsubformulas[eq:gaussbayesinit]
    \placesubformula
    \startformula
    \startalign[n=2,align={right,left}]
        \NC \PRIOR = \NC \GAUSS{\VECX}{\VECA}{\MATPI} \EQCOMMA \NR[eq:gaussbayesinitx]
        \NC \LIKELIHOOD = \NC \GAUSS{\VECY}{\MATB \VECX + \VECB}{\MATQI}
            \NR[eq:gaussbayesinityx]
    \stopalign
    \stopformula
    \stopsubformulas

    respectively. The mean of the conditional likelihood
    \ineq{gaussbayesinityx} is a linear function of $\VECX$ but the covariance
    is independent of $\VECX$. To reduce clutter in the logarithmic forms of
    these distributions their covariances have been specified as the inverse of
    matrices $\MATP$ and $\MATQ$:

    \startsubformulas[eq:gaussbayesinitln]
    \placesubformula
    \startformula
    \startalign[n=2,align={right,left}]
        \NC -2 \ln(\PRIOR) = \NC (\VECX - \VECA)^\top \MATP (\VECX - \VECA)
            + \CONST \EQCOMMA \NR[eq:gaussbayesinitlnx]
        \NC -2 \ln(\LIKELIHOOD) = \NC (\VECY - \MATB \VECX - \VECB)^\top
            \MATQ (\VECY - \MATB \VECX - \VECB) + \CONST
            \EQSTOP \NR[eq:gaussbayesinitlnyx]
    \stopalign
    \stopformula
    \stopsubformulas

    Terms denoted \quote{const} are containers for any constant expressions
    independent of $\VECX$ repeatedly be used as such in following
    calculations. Consider Bayes' theorem in logarithmic space

    \placesubformula
    \startformula
    \startalign[n=2,align={right,left}]
        \NC -2 \ln(\POSTERIOR) = \NC -2 \ln\left(\frac{\JOINT}{\NORMALIZATION}\right) \NR
        \NC = \NC -2 \ln\left(\frac{\LIKELIHOOD \PRIOR}{\NORMALIZATION}\right) \NR
        \NC = \NC -2 \ln(\LIKELIHOOD) - 2 \ln(\PRIOR) + \CONST \NR
        \NC = \NC \GAUSSEXP{\VECY}{\MATB \VECX - \VECB}{\MATQ}
            + \GAUSSEXP{\VECX}{\VECA}{\MATP} + \CONST\NR
        \NC = \NC
            \VECYT \MATQ \VECY
            - \VECYT \MATQ \MATB \VECX
            - \VECYT \MATQ \VECB
            - \VECXT \MATBT \MATQ \VECY
            \NR
        \NC \NC
            + \VECXT \MATBT \MATQ \MATB \VECX
            + \VECXT \MATBT \MATQ \VECB
            - \VECBT \MATQ \VECY
            + \VECBT \MATQ \MATB \VECX
            \NR[eq:gaussprodexpand]
        \NC \NC
            + \VECXT \MATP \VECX
            - \VECXT \MATP \VECA
            - \VECAT \MATP \VECX
            + \CONST
            \NR
    \stopalign
    \stopformula

    where the last step expands the vector-matrix-vector products. The
    posterior is known to be a Gaussian distribution, therefore an equivalent
    representation of the form

    \placesubformula
    \startformula
    \startalign[n=2,align={right,left}]
        \NC -2 \ln(\POSTERIOR) = \NC
            \GAUSSEXP{\VECX}{\VECC}{\MATS} + \CONST \NR
        \NC = \NC
             \VECXT \MATS \VECX
             - \VECXT \MATS \VECC
             - \VECCT \MATS \VECX
             + \CONST \NR[eq:gausspostexpand]
    \stopalign
    \stopformula

    must exist. Because that the equivalence must hold for all $\VECX$, an
    expression for $\COVMAT$ is found by matching quadratic terms of $\VECX$
    between both forms \ineq{gaussprodexpand} and \ineq{gausspostexpand}:

    \startformula
    \startalign[n=3,align={left,right,left}]
        \NC \NC \VECXT \MATS \VECX = \NC
            \VECXT \MATBT \MATQ \MATB \VECX + \VECXT \MATP \VECX \NR
        \NC \NC = \NC
            \VECXT (\MATBT \MATQ \MATB + \MATP) \VECX \NR
        \NC \Rightarrow~~ \NC \MATS = \NC
            \MATBT \MATQ \MATB + \MATP
            \EQSTOP
    \stopalign
    \stopformula

    Matching the linear terms involving $\VECXT$ between both forms
    \ineq{gaussprodexpand} and \ineq{gausspostexpand} results in an expression
    for the posterior mean $\VECC$:

    \startformula
    \startalign[n=3,align={left,right,left}]
        \NC \NC - \VECXT \MATS \VECC = \NC
            - \VECXT \MATBT \MATQ \VECY
            + \VECXT \MATBT \MATQ \VECB
            - \VECXT \MATP \VECA \NR
        \NC \NC  = \NC
            - \VECXT (\MATBT \MATQ (\VECY - \VECB) + \MATP \VECA) \NR
        \NC \Rightarrow~~ \NC \VECC = \NC
            \MATSI (\MATBT \MATQ (\VECY - \VECB) + \MATP \VECA) \EQSTOP \NR
    \stopalign
    \stopformula

    Equating terms linear in $\VECX$ yields the same result. With $\VECC$
    (mean) and $\MATS$ (inverse covariance) determined, the posterior

    \placeformula[eq:gaussbayespost]
    \startformula
        \POSTERIOR = \GAUSS{\VECX}{\MATSI(\MATBT \MATQ (\VECY - \VECB) + \MATP \VECA)}{\MATSI}
    \stopformula

    is known. $\NORMALIZATION$ could now be obtained by

    \startformula
        \NORMALIZATION = \frac{\JOINT}{\POSTERIOR} \EQCOMMA
    \stopformula

    a subtraction in linear space, assuming that $\NORMALIZATION$ is a Gaussian
    distribution

    \startformula
        -2 \ln(\NORMALIZATION) = \GAUSSEXP{\VECY}{\VECD}{\MATT}
    \stopformula

    and matching terms between both expressions like is was done for the
    posterior. Due to the rather complex mean and covariance of $\POSTERIOR$
    this is tedious. Alternatively, it is possible to directly integrate the
    joint distribution with respect to $\VECX$
    
    \startformula
        \NORMALIZATION = \int \JOINT \, \diff \VECX = \int \LIKELIHOOD \, \PRIOR \, \diff \VECX \EQCOMMA
    \stopformula
    
    again a quite tedious procedure. Instead, starting from the joint
    distribution and rewriting it in a blocked form, an intuitive argument is
    made which can be reinforced by rigorous derivation
    \cite[alternative=authoryears,left={(given e.g. by }][Bishop2006]. Let
    $\VECZ$ be the vector obtained by stacking $\VECX$ and $\VECY$

    \startformula
        \VECZ = \startpmatrix \VECX \NR \VECY \NR \stoppmatrix \EQSTOP
    \stopformula

    The joint distribution $\JOINTZ$ is Gaussian

    \placeformula
    \startformula
    \startalign
        \NC \JOINTZ = \NC \GAUSS{\VECZ}{\MEANVEC}{\MATRI} \EQCOMMA \NR
        \NC -2 \ln(\JOINTZ) = \NC \GAUSSEXP{\VECZ}{\MEANVEC}{\MATR} + \CONST \NR
        \NC = \NC \VECZT \MATR \VECZ - \VECZT \MATR \MEANVEC
            - \MEANVECT \MATR \VECZ + \CONST \NR[eq:gaussbayesjoint]
    \stopalign
    \stopformula

    and by matching terms with $\JOINT = \LIKELIHOOD \PRIOR$ which was
    evaluated previously in \ineq{gaussprodexpand} $\MEANVEC$ and $\MATR$
    can be determined. Again, the quadratic terms are matched to obtain the
    inverse of the covariance

    \placeformula
    \startformula
    \startalign[n=2,align={right,left}]
        \NC \NC
            \VECYT \MATQ \VECY
            - \VECYT \MATQ \MATB \VECX
            - \VECXT \MATBT \MATQ \VECY
            + \VECXT \MATBT \MATQ \MATB \VECX
            + \VECXT \MATP \VECX \NR
        \NC = \NC
            \VECZT \startpmatrix[n=2,align={middle,middle}]
                \NC \MATP + \MATBT \MATQ \MATB \NC - \MATBT \MATQ \NR
                \NC - \MATQ \MATB \NC \MATQ \NR
            \stoppmatrix \VECZ \NR[eq:gaussbayesjointcovi]
        \NC = \NC
            \VECZT \MATR \VECZ \NR
    \stopalign
    \stopformula

    and linear terms are matched to determine the mean (here shown for the
    transposed terms)

    \startformula
    \startalign[n=2,align={right,left}]
        \NC \NC
            - \VECYT \MATQ \VECB
            + \VECXT \MATBT \MATQ \VECB
            - \VECXT \MATP \VECA \NR
        \NC = \NC
            - \VECZT \startpmatrix
                \MATP \VECA - \MATBT \MATQ \VECB \NR
                \MATQ \VECB \NR
            \stoppmatrix \NR
        \NC = \NC
            - \VECZT \MATR \MEANVEC \EQSTOP \NR
    \stopalign
    \stopformula

    Inversion of the block matrix $\MATR$ from \ineq{gaussbayesjointcovi}
    \cite[alternative=authoryears,left={(e.g. }][Petersen2012] yields the
    covariance matrix

    \placeformula[eq:gaussbayesjointcov]
    \startformula
        \MATRI = \startpmatrix[n=2,align={middle,middle}]
            \NC \MATPI \NC \MATPI \MATBT \NR
            \NC \MATB \MATPI \NC \MATQI + \MATB \MATPI \MATBT \NR
        \stoppmatrix \EQSTOP
    \stopformula

    Knowing \ineq{gaussbayesjointcov}, the mean is found to be

    \placeformula[eq:gaussbayesjointmean]
    \startformula
        \MEANVEC = \MATRI \startpmatrix
                \MATP \VECA - \MATBT \MATQ \VECB \NR
                \MATQ \VECB \NR
            \stoppmatrix = \startpmatrix
                \VECA \NR
                \MATB \VECA + \VECB \NR
            \stoppmatrix \EQSTOP
    \stopformula

    Comparison with the probability distribution $\PRIOR
    = \GAUSS{\VECX}{\VECA}{\MATPI}$ from \ineq{gaussbayesinit} reveals that the
    upper left block of the covariance matrix \ineq{gaussbayesjointcov} and the
    upper block of the mean vector \ineq{gaussbayesjointmean} of the joint
    distribution $\JOINTZ$ correspond directly to the covariance and mean of
    $\PRIOR$. This makes intuitive sense: When the distribution is evaluated,
    these are the blocks that interact with $\VECX$ exclusively while the
    off-diagonal blocks of $\MATRI$ result in interaction of $\VECX$ and
    $\VECY$. It seems reasonable that the blocks interacting exclusively with
    $\VECX$ are the only ones neccesary to describe the (marginalized)
    distribution with respect to $\VECX$ alone. The same idea leads to the
    expectation that the lower left block of $\MATRI$ and the lower block of
    $\MEANVEC$ are the only ones neccessary to describe the (marginalized)
    distribution with respect to $\VECY$ only. For symmetry, $\NORMALIZATION$
    should therefore be given by
    
    \placeformula[eq:gaussbayesnorm]
    \startformula
        \NORMALIZATION = \GAUSS{\VECY}{\MATB \VECA + \VECB}{\MATQI + \MATB \MATPI \MATBT}
    \stopformula

    and this is indeed the result found in a rigorous derivation.

\stopsubsection

\stopsection


\startsection[title={Optimal Estimation},reference=ch:optimalestimation]

    This retrieval technique incorporates radiometer observations into a prior
    estimate of the atmospheric state with a Bayesian update. Because this
    update explicitly makes use of the physics behind radiative transfer as
    simulated by a numerical model, optimal estimation is often called a
    physical retrieval technique
    \cite[alternative=authoryears,left={(e.g. }][Guldner2001,Turner2007].
    With knowledge of the uncertainties of the forward model and the prior
    information which are essential to solve the non-uniqueness problem of
    retrieval, a atmospheric states consistent with these error characteristics
    are derived by the method \cite[authoryears][Lohnert2004]. Because
    uncertainties are propagated in a Bayesian approach, an estimate of
    the error of the retrieved state is derived as well.

    Due to the requirements of error estimates and a forward model, the optimal
    estimation method is more complex to set up than regression techniques
    and more expensive in terms of computational time. However, the
    computational cost has been offset by increasing hardware capabilites and
    retrievals with a frequency suitable for real-time operational use are
    possible with any modern desktop computer. Setup can be simplified by
    software such as Qpack \cite[authoryears][Eriksson2005]
    which providing not only a forward model (ARTS) but also the routines
    necessary for the optimal estimation procedure.

    Recent research involving optimal estimation methods is often connected to
    the desire of assimilating retrieved profiles into numerical
    weather prediction models \cite[alternative=authoryears,right={ and
    references therein)}][Cimini2014]. \cite[Martinet2015] employed a two step
    assimilation procedure: vertical atmospheric profiles are retrieved
    from radiometer observations first and then assimiliated into a weather
    prediction model in a second step. \cite[DeAngelis2016] recently presented
    a new radiative transfer model called RTTOV-gb providing the capability to
    assimilate radiometer observations directly into an NWP model fields.

    The standard reference for the theory behind optimal estimation
    schemes is a book by \cite[Rodgers2000]. The presentation here also follows
    this book but is restricted to a purely Bayesian approach.

    All probability distributions involved in the optimal estimation procedure
    are assumed to be Gaussian allowing analytical derivations. The result is
    a retrieval scheme efficient even for atmospheric state vectors of high
    dimensionality for which discrete approaches are very costly.

    All knowledge about the state vector $\VECX$ independent of a radiometer
    observation is contained in the prior distribution $\PRIOR$.
    This knowledge might originate from climatological records, output from
    a NWP model, other instruments than the radiometer or any combination of
    these sources. The construction of an optimal prior is not an easy task and
    will be discussed in sections \in[ch:construct_prior] and
    \in[ch:additionalinfo]. Wherever from, due to the assumption that the prior
    PDF is Gaussian it is of the form

    \placeformula[eq:optimalest_prior]
    \startformula
        \PRIOR = \GAUSS{\VECX}{\MEANVECA}{\COVMATA} \EQCOMMA
    \stopformula

    where the subscript \quote{a} stands for \quote{a-priori}. The likelihood
    function $\LIKELIHOOD$ expresses the probability that an observation
    $\VECY$ is the result of some state $\VECX$. It is quantified with the help
    of a forward model of the radiative transfer governing the radiometer
    observation $\VECY$

    \placeformula[eq:forward_model]
    \startformula
        \VECY = \FWD(\VECX) + \ERR \EQCOMMA
    \stopformula

    where $\FWD$ is the forward model operator and $\ERR$ is an error term,
    arising from measurement noise, inaccuracies of the forward model,
    discretization errors and possibly other terms (see section
    \in[ch:rtm_errors]. The distribution of the error term is Gaussian,
    centered around a systematic bias $\MEANVECERR$ with a convariance matrix
    $\COVMATERR$

    \placeformula[eq:optimalest_errpdf]
    \startformula
        \PROB{\ERR} = \GAUSS{\ERR}{\MEANVECERR}{\COVMATERR} \EQSTOP
    \stopformula

    Substituting \ineq{forward_model} into \ineq{optimalest_errpdf} and
    changing the independent variable of the {\PDF} to $\VECY$ yields in the
    likelihood function
    
    \placesubformula
    \startformula
    \startalign[n=3,align={right,middle,left}]
        \NC \PROB{\VECY - \FWD(\VECX)} = \NC
            \GAUSS{\VECY - \FWD(\VECX)}{\MEANVECERR}{\COVMATERR} \NC \NR
        \NC = \NC \GAUSS{\VECY}{\FWD(\VECX)+\MEANVECERR}{\COVMATERR} \NC
            = \LIKELIHOOD \EQSTOP \NR[eq:optimalest_likelihood]
    \stopalign
    \stopformula

    Knowing \ineq{optimalest_prior} and \ineq{optimalest_likelihood}, the
    posterior PDF (which is the result of the retrieval) is

    \placeformula[eq:optimalest_posterior]
    \startformula
        \POSTERIOR
        = \frac{\LIKELIHOOD \PRIOR}{\NORMALIZATION}
        = \frac{\GAUSS{\VECY}{\FWD(\VECX) + \MEANVECERR}{\COVMATERR}
            ~\GAUSS{\VECX}{\MEANVECA}{\COVMATA}}{\NORMALIZATION} \EQSTOP
    \stopformula

    It expresses the probability of an atmospheric state $\VECX$ given a
    measurement $\VECY$ and incorporates the forward model with its error
    characteristics and a-priori knowledge of the atmospheric state independent
    of the observation. All involved distributions are Gaussians, therefore
    $\POSTERIOR$ is also a Gaussian distribution and the normalization
    $\NORMALIZATION$ does not have to be determined explicitly
    but can be inferred from the product in the numerator.
    The results from section \in[ch:bayes_gauss] cannot be used yet with
    \ineq{optimalest_posterior}, because the forward model operator $\FWD$ is
    non-linear which is why the product of $\LIKELIHOOD$ and $\PRIOR$ cannot
    be carried out analytically. 
    The model operator $\FWD$ has to be linearized first resulting in an
    iterative approach.

\startsubsection[title=Iterative Solutions]

    To linearize $\FWD$, a reference state to linearize at must be chosen.
    Let this state be $\ITER{\VECX}{i}$. Applying a Taylor series
    expansion to the forward model and chopping off all non-linear
    terms yields the linear approximation

    \startformula
        \FWD(\VECX) \approx \FWD(\ITER{\VECX}{i})
            + \ITER{\FWDJAC}{i} (\VECX - \ITER{\VECX}{i}) \EQSTOP
    \stopformula

    $\ITER{\FWDJAC}{i}$ is the Jacobian of $\FWD$ at $\ITER{\VECX}{i}$
    containing the derivatives of the forward model output with respect to each
    element of the state vector $\VECX$. After replacing $\FWD$ in
    \ineq{optimalest_posterior} by this linearization, the likelihood function

    \startformula
        \LIKELIHOOD = \GAUSS{\VECY}{\ITER{\FWDJAC}{i} \VECX
            + \FWD(\ITER{\VECX}{i}) - \ITER{\FWDJAC}{i}
            \ITER{\VECX}{i} + \MEANVECERR}{\COVMATERR}
    \stopformula

    and the prior \ineq{optimalest_prior} fulfill the assumptions of
    \ineq{gaussbayesinit} and the result \ineq{gaussbayespost} can be used by
    substituting

    \startformula
        \{ \VECA \rightarrow \MEANVECA,~
        \MATPI \rightarrow \COVMATA,~
        \MATB \rightarrow \ITER{\FWDJAC}{i},~
        \VECB \rightarrow \FWD(\ITER{\VECX}{i})
            - \ITER{\FWDJAC}{i} \ITER{\VECX}{i}
            + \MEANVECERR,~
        \MATQI \rightarrow \COVMATERR \}
    \stopformula

    to obtain an expression for the posterior:
    
    \startformula
    \startalign[n=2,align={right,left}]
        \NC \POSTERIOR = \NC \GAUSS{\VECX}{\MEANVEC}{\COVMAT} \EQCOMMA \NR
        \NC \MEANVEC = \NC \COVMAT (\ITER{\FWDJAC}{i}^\top
            \COVMATERR^{-1} (\VECY - \FWD(\ITER{\VECX}{i})
            + \ITER{\FWDJAC}{i} \ITER{\VECX}{i} - \MEANVECERR)
            + \COVMATA^{-1} \MEANVECA) \EQCOMMA \NR
        \NC \COVMAT = \NC 
            (\ITER{\FWDJAC}{i} \COVMATERR^{-1} \ITER{\FWDJAC}{i}^\top
            + \COVMATA^{-1})^{-1} \EQSTOP \NR
    \stopalign
    \stopformula
    
    Due to the linear approximation of $\FWD$ the solution for $\MEANVEC$ and
    $\COVMAT$ will not be optimal. Instead, starting from a first guess
    $\ITER{\MEANVEC}{0}$, the optimal solution is found by fixed-point
    iteration, using the mean of the previous iteration as the linearization
    point in each step

    \startsubformulas[eq:gausspostiter]
    \placesubformula
    \startformula
    \startalign[n=3,align={right,left,right}]
        \NC \ITER{\MEANVEC}{i+1} = \NC
            \ITER{\COVMAT}{i} (\ITER{\FWDJAC}{i}^\top
            \COVMATERR^{-1} (\VECY - \FWD(\ITER{\MEANVEC}{i})
            + \ITER{\FWDJAC}{i} \ITER{\MEANVEC}{i} - \MEANVECERR)
            + \COVMATA^{-1} \MEANVECA) \EQCOMMA \NC \NR[eq:gausspostmeaniter][a]
        \NC \ITER{\COVMAT}{i} = \NC
            (\ITER{\FWDJAC}{i}^\top \COVMATERR^{-1} \ITER{\FWDJAC}{i}
            + \COVMATA^{-1})^{-1}
            \EQSTOP \NC \NR[eq:gausspostcoviter][b]
    \stopalign
    \stopformula
    \stopsubformulas

    In practice more robust iteration schemes are required to obtain good
    convergence properties. The easiest approach to find such schemes is to
    replace \ineq{gausspostmeaniter} by an algorithm searching for the position
    of the global maximum of \ineq{optimalest_posterior} which is the same as
    the mean due to the posterior being Gaussian. This search is best carried
    out in logarithmic space where the location of the extremum is found at the
    the root of the gradient of the quadratic form of
    \ineq{optimalest_posterior} with respect to $\VECX$

    \placeformula
    \startformula
    \startalign[n=2,align={right,left}]
        \NC 0 = \NC \nabla_{\VECX} (- 2 \ln(\POSTERIOR)) \NR
        \NC = \NC (\VECY - \FWD(\VECX) - \MEANVECERR)^\top \COVMAT^{-1}
            (\VECY - \FWD(\VECX) - \MEANVECERR) + (\VECX - \MEANVECA)^\top
            \COVMATA (\VECX - \MEANVECX)^\top \EQSTOP \NR[eq:gaussminimization]
    \stopalign
    \stopformula
    
    Usually one settles for a least squares solution to this minimization
    problem\footnote{The maximum search is translated to a minimum search after
    application of the logarithm and multiplication by -2.} as methods for
    these problems are simpler to implement (e.g. Newton's method for finding
    the root of a function requires the Hessian, which is often expensive to
    calculate). \cite[Rodgers2000] shows resulting iterative schemes for the
    Gauss-Newton method and a more robust variant, the Levenberg-Marquard
    iteration. With the identity

    \startformula
        \MEANVECA - \ITER{\COVMAT}{i} \ITER{\FWDJAC}{i}^\top
        \COVMATERR^{-1} \ITER{\FWDJAC}{i} \MEANVECA =
        \ITER{\COVMAT}{i} (\ITER{\COVMAT}{i}^{-1}
        - \ITER{\FWDJAC}{i}^\top \COVMATERR^{-1} \ITER{\FWDJAC}{i})
        \MEANVECA = \ITER{\COVMAT}{i} \COVMATA^{-1} \MEANVECA \EQCOMMA
    \stopformula

    it is easily seen that \ineq{gausspostmeaniter}, the solution obtained from
    fixed point iteration with the linearized forward model, is equivalent to
    equation 5.9 from \cite[Rodgers2000], which he obtained by applying the
    Gauss-Newton method to \ineq{gaussminimization}. This underlines the
    equivalence of the two approaches to determine the posterior mean.

    \placefigure[top][fig:iteralgo]
            {Visualization of the iterative retrieval algorithm. Missing
            is the forward model error distribution, which is used in the
            Levenberg-Marquard step. Square boxes represent numerical
            calculations, rounded boxes data input and octagonal boxes
            data being produced during the retrieval.}
            {\FLOWchart[flow:iteralgo]}

    The flowchart in Figure \in[fig:iteralgo] visualizes the resulting
    iterative retrieval scheme. Given prior knowledge, a measurement and
    an initial guess of the state vector, minimization steps with
    linearizations of the forward model are taken until the state vector has
    converged. The mean and covariance matrix of the last iteration are the
    result of the retrieval.

\stopsubsection

\startsubsection[title={Convergence Testing}]

    Considering the magnitude of errors expected from a retrieval, it is
    unnecessary to iterate until the atmospheric state vector has been
    determined with machine precision. Therefore, a criterion which indicates
    if the state vector has been determined with sufficient accuracy is
    required. \cite[Rodgers2000] presents multiple possibilities which are
    repeated here.

    One possibility is to assess convergence directly with the value of the
    posterior distribution \ineq{optimalest_posterior} evaluated in logarithmic
    space (the cost function)

    \placeformula[eq:itercostfun]
    \startformula
        \GAUSSEXP{\VECY}{\FWD(\ITER{\MEANVEC}{i}) - \MEANVECERR}{\COVMATERR^{-1}}
        + \GAUSSEXP{\ITER{\MEANVEC}{i}}{\MEANVECA}{\COVMATA^{-1}} + \CONST \EQSTOP
    \stopformula

    The constant term arising from the normalization can safely be ignored for
    the purposes of convergence detection. The value of the gradient
    \ineq{gaussminimization} can be incorporated in this criterion too
    \cite[authoryears][Hewison2006]. Also used are criteria based on the
    change between two iteration steps measured in observation or state space.
    It is important to scale such measures to account for the naturally
    different magnitudes of the elements of the observation or state vector.
    The error covariance matrices of measured vs. simulated brightness
    temperatures,

    \startformula
        \COVMATI_{\delta y} = \COVMATERR (\ITER{\FWDJAC}{i}
            \COVMATA \ITER{\FWDJAC}{i}^\top)^{-1} \COVMATERR \EQCOMMA
    \stopformula

    \cite[authoryears][Rodgers2000] and of the state vector,
    $\ITER{\COVMAT}{i+1}$ given by \ineq{gausspostcoviter}, provide appropriate
    scalings. Because covariance matrices are positive definite the resulting
    measures are always positive. They are

    \placeformula[eq:convergenceobs]
    \startformula
        (\FWD(\ITER{\VECX}{i}) - \FWD(\ITER{\VECX}{i+1}))^\top
        \COVMATI_{\delta y}
        (\FWD(\ITER{\VECX}{i}) - \FWD(\ITER{\VECX}{i+1}))
    \stopformula

    in observation space and

    \placeformula[eq:convergencestate]
    \startformula
        (\ITER{\VECX}{i} - \ITER{\VECX}{i+1})^\top
        \ITER{\COVMATI}{i+1}
        (\ITER{\VECX}{i} - \ITER{\VECX}{i+1})
    \stopformula

    in state space. When these terms are sufficiently small iteration can be
    stopped and the retrieved atmospheric state returned.

    In practice, the issue of convergence is more complicated because not only
    the choice of criterion affects convergence is important. The minimization
    technique, method of forward model linearization and included radiometer
    channels are all relevant components affecting convergence. The first guess
    of the iteration procedure also influences the convergence properties.
    Different strategies can be applied to increase the rate and likelihood of
    convergence.  \cite[Cimini2011] found that retrieving temperature first and
    then humidity in a second step improved convergence efficency.
    \cite[Hewison2006] relaxed the chosen criterion after the first 10
    iterations to encourage convergence. \cite[Turner2014] additionally coupled
    the criterion to a fixed sequence of parameters that controlling their
    minimization algorithm.

\stopsubsection

\startsubsection[title={1D-VAR, Cost Functions and Regularization},reference=ch:costfunction]

    There is an alternative way to derive the optimal estimation retrieval
    scheme. Instead of directly working with probability distributions, the
    retrieval problem can be set up as the minimization of the expected value
    of the retrieval error variance. This leads to the same solution as the
    Bayesian approach assuming all variables are Gaussian-distributed.
    Because of the minimization of a variance in the alternative approach,
    optimal estimation is also called a \quote{variational} method, often
    abbreviated as 1D-VAR, due to its application to a vertical profile
    representing one dimension of the atmospheric state.  A derivation starting
    from the variational perspective is found in chapter 4.2 of the book by
    \cite[Rodgers2000].

    The quadratic form $-2 \ln(\POSTERIOR)$ minimized during the iteration
    is often referred to as a cost function, terminology that is also common in
    regression problems. So-called regularization or penalty terms can be added
    to this cost function in order to make the retrieval favor physically
    realistic solutions. In atmospheric profile retrieval such a penalty might
    be a term involving the lapse rate \cite[authoryears][Peckham2000].
    \cite[Hewison2006] discussed penalizing superadiabatic layers and also
    considered a regularization of excessively high liquid water content in
    clouds.
    
    There is a problem with including information in the retrieval in the form
    of regularization terms: while the information is included in the retrieved
    mean profile, it is absent from its uncertainty estimate given by
    $\ITER{\COVMAT}{i}$. This issue can be resolved with the Bayesian
    perspective. Viewing the cost function as the combination of the likelihood
    function with prior distribution, it appears that each additional penalty
    term corresponds to another prior distribution involved in the Bayesian
    update. Quadratic penalty terms are associated with Gaussian distributions.
    For these, an analytic form of the uncertainty can be derived. This is
    further discussed in section \in[ch:additionalinfo]. Other types of
    distributions may not permit an analytic form of the posterior.

    Regularization is a powerful tool for the inclusion of additional knowledge
    but penalities more complex than quadratic forms generally violate the
    Gaussian assumptions leading to inaccurate uncertainty estimates. For many
    end users a more accurately retrieved mean profile is more valuable than a
    theoretically consistent uncertainty assessment and such errors are
    tolerable. Other ways to include additional knowledge of the atmospheric
    state are discussed in section \in[ch:additionalinfo].

\stopsubsection

\stopsection


\startsection[title={Linear Regression},reference=ch:linear_regression]

    Linear regression is a simple and computationally cheap retrieval method.
    It uses an approximate model of the inversion process of radiative transfer
    to predict the atmospheric state based on a radiometer observation.
    This model is fitted to a representative set of training data, for example
    historic data from radiosonde ascents paired with collocated radiometer measurements.
    Predictions of a regression model do not require the evaluation of
    a forward model during retrieval and are therefore quickly evaluated.
    The computational effort is concentrated in the training phase where the
    model parameters are determined. When generating a linear regression
    model, three aspects affect its subsequent performance: the choice of basis
    functions, the quality of the training data set and the measures taken to
    avoid overfitting.

    The linear regression model consists of a set of basis functions
    dependent on the input variables (predictors) which are combined linearly
    to make a prediction of the target variables. Applied to the retrieval
    problem, the observed brightness temperatures are
    the model's input and an atmospheric profile of temperature, humidity
    or liquid water is the output. A linear regression model can
    approximate a non-linear relationship if non-linear basis functions are
    chosen. The \quote{linear} in its name refers to the linearity in the model
    coefficients acting as weights with which the evaluated basis functions are
    combined. In retrieval applications it is common to use a basis directly
    corresponding to individual brightness temperatures.
    \cite[Crewell2007,Massaro2015] also included quadratic terms to better
    approximate the non-linear nature of the inversion. problem Because the
    coefficients affect the model only linearly they can be determined from
    a training data set in a single fitting step without the need for
    iteration.

    In order to obtain a model that provides good predictions in many
    situations a large training data set is necessary that represents the
    relationship which is to be approximated well.  Regression models in
    general are only as good as the data they are trained with and
    extrapolations to states not included in the training data are prone to
    errors and have no guarantee of physical reasonability
    \cite[authoryears][Cimini2006,Turner2007,Chan2010]. This behavior is one of
    the factors limiting the transferability of a trained regression model
    between different radiometer sites.

    The set of basis functions and the number of training data pairs both
    affect if a model is in danger of overfitting. A model is overfitted if it
    performs well for the training data but makes poor predictions for any
    other reasonable input data. It is common practice is to have a
    test data set independent of the training data for the evaluation of model
    performance or to use cross validation techniques.
    Overfitting of a linear regression model can be controlled by
    regularization. This is preferrable to the addition of noise to predictors
    during training like some authors have done
    \cite[alternative=authoryears,left={(e.g.  }][Churnside1994,Frate1998,Lohnert2004]
    since it works also with small data sets and is mathematically more elegant
    and deterministic. In the following derivation of Bayesian linear
    regression regularization will appear naturally from the prior of the model
    parameters.

    Due to their simplicity linear regression models are commonly used.
    Statistical evaluation of linear regression models used for retrievals
    of temperature and humidity have shown that their accurracy generally
    decreases with increasing altitude
    \cite[alternative=authoryears,left={(e.g. }][Solheim1998].  This is mainly
    due to the information content of radiometer observations which is lower
    at greater heights (see section \in[ch:weightingfuns]).  Atmospheric
    structures predicted by linear regression models are often much smoother
    than the actual structures and models have been found to be able to resolve
    at most the lowest temperature inversion of many
    \cite[authoryears][Crewell2007]. Studies with other regression models, most
    notably neural networks, suggest that these aspects might be improved upon
    by models better able to approximate the non-linearity of the inversion
    problem \cite[alternative=authoryears,left={(see also section
    \in[ch:otherretrievals] or }][Churnside1994].

    The classical approach to linear regression is the least squares framework
    but the Bayesian framework provides an equivalent derivation. It is
    presented here based on the book by \cite[Bishop2006]. Let $x$ be
    a component of the atmospheric state vector and $\VECY \in \REALS^d$
    a vector containing a radiometer observation. Consider a model $f$,
    a linear combination of $n$ fixed basis functions
    $\VECPHI = \startpmatrix[n=3] \NC \phi_1 \NC \dots \NC \phi_n \NR
    \stoppmatrix$, approximating a scalar function $t$ of the input $\VECY$

    \startformula
        t(\VECY) \approx f(\VECY, \VECW) = w_0 + \sum_{i=1}^n w_i \phi_i(\VECY)
            = \sum_{i=0}^n w_i \phi_i(\VECY) = \VECPHI(\VECY) \VECW \EQCOMMA
    \stopformula

    where $\VECW \in \REALS^n$ and an additional constant basis function
    $\phi_0(\VECY) = 1$ is introduced in the third step to account for constant
    term $w_0$ without the need to explicitly consider it in the following
    derivations. A common choice for basis functions in retrievals are
    indentity functions for individual components, i.e. $n = d$ and

    \placeformula[eq:regression_basis]
    \startformula
        \phi_0(\VECY) = 1,~ \phi_1(\VECY) = y_1, ~ \dots, \phi_d(\VECY) = y_d
    \stopformula

    with $\VECY^\top = \startpmatrix[n=3] \NC y_1 \NC \dots \NC y_d \NR
    \stoppmatrix$, but in general any set of linear and non-linear functions
    can be chosen. The objective when fitting the regression model is to find
    a parameter vector $\VECW$ from a given set of $(t(\VECY), \VECY)$
    pairs that results in a good approximation of $t$ by $f$.
    optimal. The Bayesian approach provides a distribution for these
    parameters.

    In order to obtain an analytical result for the distribution of $\VECW$
    deviations between the true function $t$ and its approximation $f$ are
    assumed to follow a Gaussian distribution with zero mean and covariance
    $\BETAI$

    \placeformula
    \startformula
    \startalign[n=3,align={left,right,left}]
        \NC \NC \PROB{t(\VECY) - f(\VECY, \VECW) \GIVEN \beta} = \NC
                \GAUSS{t(\VECY) - f(\VECY, \VECW)}{0}{\BETAI} \NR
        \NC \Rightarrow~~ \NC \PROB{x \GIVEN \VECY, \VECW, \beta} = \NC
            \GAUSS{x}{\VECPHI(\VECY) \VECW}{\BETAI} \EQCOMMA \NR[eq:reg_indlikelihood]
    \stopalign
    \stopformula

    where $x = t(\VECY)$ is the so-called target variable and its
    dependency on $\VECY$ is implied by the conditioning of its probability
    distribution. If data from pairwise corresponding datasets $\SETY = \{
    \VECY_1, \dots, \VECY_m \}$ and $\SETX = \{ x_1, \dots, x_m \}$ are
    independent, the likelihood function of the target variables is given by

    \startformula
        \RLIKELIHOOD = \prod_{i=1}^{m} \GAUSS{x_i}{\VECPHI(\VECY_i) \VECW}{\BETAI}
    \stopformula
    
    using \ineq{reg_indlikelihood} or equivalently by

    \startformula
        \RLIKELIHOOD = \GAUSS{\VECX}{\MATPHI \VECW}{\BETAI \MATID}
    \stopformula
    
    with the definitions

    \startformula
        \VECX = \startpmatrix x_1 \NR \vdots\NR x_m \NR \stoppmatrix
        {\rm ~~and~~}
        \MATPHI = \startpmatrix[n=3,align={middle,middle,middle}]
            \NC \phi_0(\VECY_1) \NC \dots \NC \phi_n(\VECY_1) \NR
            \NC \vdots \NC \ddots \NC \vdots \NR
            \NC \phi_0(\VECY_m) \NC \dots \NC \phi_n(\VECY_m) \NR
        \stoppmatrix = \startpmatrix
            \VECPHI(\VECY_1) \NR \vdots \NR \VECPHI(\VECY_m) \NR
        \stoppmatrix
        \EQSTOP
    \stopformula
    
    In order to use Bayes' theorem to obtain the probability distribution of
    $\VECW$ given the training data pairs, a prior for the parameter vector is
    necessary. It is possible to assume that any value of $\VECW$ is equally
    likely since no information about $\VECW$ is available at this point.
    Experience shows however that it is benfical to make the regression favor
    values of $\VECW$ close to $\VECZERO$ as this results in a model less prone
    to overfitting and with better numerically stability. An isotropic Gaussian
    with mean $\VECZERO$ is therefore considered

    \placeformula[eq:reg_prior]
    \startformula
        \RPRIOR = \GAUSS{\VECW}{\VECZERO}{\ALPHAI \MATID}
    \stopformula

    where $\alpha > 0$ governs the covariance of the prior distribution. With
    likelihood function and prior determined, Bayes' theorem can be written
    down:

    \startformula
        \RPOSTERIOR = \frac{\RLIKELIHOOD \, \RPRIOR}{\RNORMALIZATION} \EQSTOP
    \stopformula

    Application of the result \ineq{gaussbayespost} with the substitutions

    \startformula
        \{ \VECX \rightarrow \VECW,~
            \VECA \rightarrow \VECZERO,~
            \MATP \rightarrow \alpha \MATID,~
            \VECY \rightarrow \VECX,~
            \MATB \rightarrow \MATPHI,~
            \VECB \rightarrow \VECZERO,~
            \MATQ \rightarrow \beta \MATID
            \}
    \stopformula

    identified from \ineq{gaussbayesinit} yields the desired posterior
    probability distribution for the parameter vector $\VECW$:

    \startsubformulas[eq:reg_posterior]
    \placesubformula
    \startformula
    \startalign[n=2,align={right,left}]
        \NC \RPOSTERIOR = \NC \GAUSS{\VECW}{\MEANVEC}{\COVMAT} \NR[eq:reg_posterior_gauss]
        \NC \COVMAT = \NC (\beta \MATPHIT \MATPHI + \alpha \MATID)^{-1} \NR[eq:reg_posterior_cov]
        \NC \MEANVEC = \NC \beta \COVMAT \MATPHIT \VECX \EQSTOP \NR[eq:reg_posterior_mean]
    \stopalign
    \stopformula
    \stopsubformulas

    This concludes the training of the regression model. Predictions of
    a target $x$ based on a new input $\VECY$ are obtained from the predictive
    distribution

    \startformula
    \startalign[n=2,align={right,left}]
        \NC \RPREDICT = \NC
            \int_{\REALS^{n+1}} \PROB{x \GIVEN \VECY, \VECW, \beta} \,
            \RPOSTERIOR \, \diff \VECW \NR
        \NC = \NC
            \int_{\REALS^{n+1}} \GAUSS{x}{\VECPHI(\VECY) \VECW}{\BETAI} \,
                \GAUSS{\VECW}{\MEANVEC}{\COVMAT} \,\diff \VECW \NR
    \stopalign
    \stopformula
    
    which is the likelihood of the measurement given the model
    \ineq{reg_indlikelihood}, marginalized with respect to the posterior
    parameter vector distribution \ineq{reg_posterior_gauss}. Result
    \ineq{gaussbayesnorm} is applicable \footnote{The reduction in dimension in
    \ineq{gaussbayesinityx} does not invalidate the result. The distribution's
    mean is still linear.} with the substitution

    \startformula
        \{ \VECX \rightarrow \VECW,~
            \VECA \rightarrow \MEANVEC,~
            \MATP \rightarrow \COVMATI,~
            \VECY \rightarrow x,~
            \MATB \rightarrow \VECPHI(\VECY),~
            \VECB \rightarrow 0,~
            \MATQ \rightarrow \beta \}
    \stopformula

    yielding

    \startformula
        \RPREDICT = \GAUSS{x}{\VECPHI(\VECY) \MEANVEC}{\BETAI + \VECPHI(\VECY) \COVMAT \VECPHIT(\VECY)}
    \stopformula

    for the predictive distribution. The predicted mean is the sum of products
    between the basis functions evaluated for the predictors and the the
    parameter vector's MAP solution. This result is consistent with the
    expectation of a prediction from a non-Bayesian approach to linear
    regression. The covariance of the predicted value is a combination of the
    uncertainty inherent in the target variable and uncertainty in the model
    parameters.

    \placefigure[top][fig:bayesian_regression]
        {Predictive distributions of Bayesian regression models trained on 12
        data points obtained from a sine signal with added Gaussian noise of
        standard deviation 0.2 (circles). The black line is the mean of the
        predictive distribution, the gray and light gray shaded areas mark
        ± one and three standard deviations around this mean. The left and center
        models are fifth and first order polynomials respectively, the right
        model uses seven Gaussian basis functions located between -3 and 3.}
        {\externalfigure[bayesian_regression][width=\textwidth]}

    At first sight, a full description of the predictive distribution seems
    useful for error estimates of predictions.  But care has to
    be taken in its interpretation because the distribution describes
    uncertainty of the model parameters but not of the choice of model itself,
    i.e. the choice of basis functions. Uncertainty obtained from the
    predictive distribution is just one term in the overall error estimate of
    predictions and on its own only useful if the chosen model can
    accurately reproduce the variability of the target data. Figure
    \in[fig:bayesian_regression] illustrates how the choice of basis functions
    affects the predictive distribution with a constructed example in one
    dimension. The model shown in the left frame is a polynomial of order five
    and able to describe the variability of the target data well. It
    gives a good fit inside the data domain and an appropriate uncertainty
    range outside the domain can be derived from the predictive distribution.
    The model in the middle is a line with ony slope and offset as parameters.
    Not only is it a bad fit for the data inside the domain but the predictive
    distribution massively overstates the model's accuracy. For a line
    the fit is good and the parameters can be determined with high
    accuracy but considering all possible models it is a bad fit because a line
    cannot express variability of the target data sufficiently. On the right,
    seven Gaussian basis functions are used in a linear regression model,
    resulting in a good fit inside the data domain with appropriate uncertainty
    estimates. However, the error estimate derived from the predictive
    distribution reduces to the inherent uncertainty of the target data outside
    the traingin domain. No contribution to the estimate from the model exists
    anymore due to the localized bases functions (the covariance is dominated
    by $\BETAI$ since $\VECPHI(\VECY)$ vanishes). As for the line model, this
    obviously does not represent the true uncertainty of the model predictions.

    The treatment of linear regression so far considered multiple
    predictors but was restricted to a scalar target. For the retrieval of an
    atmospheric profile from radiometer measurements, each atmospheric layer is
    a target variable resulting in a one dimensional target vector. The common
    approach in this situation is to use the same basis functions for each
    target component and derive a parameter matrix instead of a parameter
    vector $\VECW$. During the derivation of this matrix it becomes apparent
    that the problem decouples into independent, scalar regression problems for
    each target component \cite[authoryears][Bishop2006]. The results from
    above are therefore directly applicable to multiple targets.

\stopsection


\startsection[title={Other Retrieval Techniques},reference=ch:otherretrievals]

    The optimal estimation method has been the starting point for integrated
    approaches, that combine observations from multiple instruments.
    For example, the integrated profiling technique by \cite[Lohnert2004]
    applies optimal estimation to combine data from a microwave radiometer,
    a cloud radar, a lidar-ceilometer, radiosonde ascents and results from
    a microphysical cloud model. \cite[Lopez2006] used observations from
    a ground-based microwave radiometer in a two-dimensional variational
    scheme together with a cloud radar to retrieve cloud liquid water content.
    Recent efforts by \cite[DeAngelis2016] aim at the direct use of brightness
    temperature observations in a 4D-VAR assimilation scheme of a numerical
    weather prediction model.

    Besides linear regression models, neural networks are used frequently to
    retrieve vertical profiles of thermodynamic variables from radiometer
    observations \cite[alternative=authoryears,left={(e.g. }][Churnside1994,Solheim1998,Xu2015].
    Neural networks are non-linear models. Their training phase is more
    complicated than that of linear regression models but evaluation is easy
    once the model parameters have been found. Most neural networks for
    retrieval applications are rather simple, typically with a single hidden
    layer and one type of activation function. Their overall performance in
    terms of the root mean square error has not been found to be consistently
    better than that of linear regression models. There are indications
    that retrieval performance is better in terms of the representation of
    atmospheric structures such as temperature inversions despite worse root
    mean square errors overall \cite[authoryears][Churnside1994]. However,
    non-linear regression models have been found to react stronger to a bias
    \cite[authoryears][Martinet2015] and noise \cite[authoryears][Cimini2006]
    in the input data.  Many other regression models exist that have not been
    used for retrievals yet. All regression models are generally easy to use
    but measures have to be taken to avoid overfitting. They often lack methods
    to estimate uncertainty and, as shown for the case of linear regression
    (Figure \in[fig:bayesian_regression]), even if an uncertainty estimate can
    be derived, its information content has to be studied carefully. In
    contrast to optimal estimation retrievals which explicitly use a forward
    model, atmospheric states determined by regression methods are in general
    not consistent with the brightness temperatures they are derived from.

    Individual components of the retrieval can be tweaked nest to the method
    itself. \cite[Frate1998,Tan2011] have used principal component
    analysis/empirical orthogonal functions to reduce the dimensionality of
    the retrieval problem. Such dimensionality reducing techniques can be
    applied to the radiometer measurements or the atmospheric state vector.
    It can also be benefitial for the retrieval accuracy to apply variable
    transformations, e.g. to enforce postitivity. The choice of variables that
    quantifying the atmospheric state is of importance too.  While regression
    model are able to retrieve temperature, humidity or liquid water isolated
    from one another, optimal estimation techniques are more restricted. This
    is discussed further in section \in[ch:statevector]. Regression methods are
    also able to retrieve only partial profiles, while optimal estimation is
    dependent on a forward model which often requires a comprehensive
    description of the atmospheric state in order to be accurate.

\stopsection


\startsection[title={Priors and Training Data Sets}]

    The performance of optimal estimation and regression methods strongly
    depends on the data that drive them. Error estimates and the prior
    distribution used in optimal estimation are important for the uncertainty
    assessment of the retrieved atmospheric state and constrain the ill-posed
    inversion problem of retrieval. A first guess for the iteration procedure
    can be derived from the prior distribution. The information on the
    atmospheric state contained in an observation of a ground-based microwave
    radiometer originates mainly from the lower troposphere
    \cite[authoryears][Cadeddu2013,Cimini2006,Lohnert2012]. The prior therefore
    governs the retrieval performance at higher altitudes. This makes numerical
    weather forecasts a good source of prior information as these are generally
    more accurate at higher levels where larger scales of high short-term
    predictability dominate the atmospheric state than in the boundary layer.
    Thus, a NWP model prior provides complementary information to the
    radiometer observation and the retrieval additionally benefits from all
    data sources assimilated into the NWP model \cite[authoryears][Cimini2015].
    The result is good accuracy throughout the entire troposphere
    \cite[alternative=authoryears,left={(see e.g. results from }][Cimini2011].
    The covariance of a NWP model prior can be extracted from the model's
    assimilation procedure or be obtained from a comparison of radiosonde
    ascents with previous forecasts. In complex terrain where the model
    topography often significantly differs from the actual topography,
    additional processing is required before model output can be used in an
    optimal estimation framework.

    Priors may also be determined directly from a radiosonde climatology. This
    generally leads to distributions with a large uncertainty because the
    atmospheric state can vary greatly over the course of a year at a fixed
    location. The atmospheric state space accessible to the optimal estimation
    procedure is therefore only weakly constrained and a good first guess
    should be chosen to ensure convergence on an appropriate state.
    A possible choice is are profiles retrieved by a regression model in
    a first step. It is worthwhile to consider using regression retrievals also
    for the construction of the prior distribution. This likely yields
    a smaller uncertainty compared to the climatology. A disadvantage of this
    approach is that the independent estimation of the regression model
    parameters and the prior covariance requires a large climatological data
    set. The covariance of a data-based prior can further be reduced by
    categorization of the data and subsequent calculation of specialized
    priors. It is possible to specialize on seasons
    \cite[alternative=authoryears,left={(e.g. }][Peckham2000,Chan2010,Tan2011,Massaro2015],
    months \cite[alternative=authoryears,left={(e.g. }][Turner2007],
    time of day \cite[alternative=authoryears,left={(e.g. }][Massaro2015] or on
    meteorological situations such as a distinction between clear, cloudy and
    rainy cases or atmospheric stability \cite[authoryears][Meyer2016].
    Meteorological categorization requires that the category can be determined
    prior to the retrieval.

    Regression models relate brightness temperature measurements to atmospheric
    states based on the data pairs that were used for the training of the
    model. The representativity of actual atmospheric states of a training data
    set is of high importance because the extrapolation behavior of
    a regression model is not guaranteed to be physically reasonable.
    A model can become bias toward a certain meteorological situation if it is
    overrepresented in the data set which has to be considered when collecting
    training data \cite[authoryears][Cadeddu2002]. A climatology of radiosonde
    ascents and brightness temperature measurements simulated by a radiative
    transfer model from these ascents are commonly used for training.
    \cite[Frate1998] used an entirely synthetic training data set from an
    atmospheric profile generator. Few sites have radiometer measurements over
    a long enough time period to allow measurement-based regression, i.e.
    training with collocated and simultaneous measurements by ballon and
    radiometer. \cite[Guldner2001,Cimini2006] found that measurement-based
    regression models have significantly less bias due to the absent bias of
    the radiative transfer model. Where radiosonde data are not available,
    numerical weather prediction model output can be used alternatively
    \cite[authoryears][Guldner2013]. Regression models based on specialized
    training data sets may exhibit \quote{jumps} in the the time evolution of
    retrieved profiles when a procedure managing the transition between
    categories is missing \cite[authoryears][Massaro2015]. In section
    \in[ch:linear_regression] it was mentioned that regression models are not
    arbitrarily tranferable due to the dange of extrapolation. Models for
    measurement sites at different altitudes or in different climates must
    therefore be trained with different data sets. Most authors only use
    data directly from the radiometer site to train their models. An exception
    is a study by \cite[Sanchez2013], who combined radiosonde ascents from
    Denver (USA), Madrid and Coruña (both Spain) which are stations located at
    similar latitude and altitude.

\stopsection

\startsection[title={Inclusion of Additional Information},reference=ch:additionalinfo]

    Many sites that operate a microwave radiometer also operate other
    instruments such as infrared spectrometers, ceilometers, cloud radars or
    surface in-situ sensors. Radiosonde climatologies and NWP model forecasts
    can provide external information for retrievals. How best to include
    such sources of additional information into retrieval methods is a current
    and important topic of research.

    Regression methods make it easy to include other data as additional
    regressors. Most regression models however do not consider the
    uncertainties of their predictors and additional information about the
    atmospheric state may not be used optimally. A high number of regressors
    also increases the possibility of overfitting.

    While additional information should contribute to the prior of optimal
    estimation retrievals, it is often not immediately obvious how exactly the
    prior should has to be set up to accomodate various kinds of such
    information. A NWP forecast provides complete profiles of atmospheric
    variables which can be used directly in a prior. A cloud base height or
    surface measurement does not lead to an entire profile from which a prior
    can be determined. Such isolated measurements may be included as fixed
    points of the prior by modifying the mean and covariance matrix
    \cite[authoryears][Bleisch2012]. In-situ measurements of the atmospheric
    state can be used to explicitly set a value of the prior whereas a cloud
    base height may be translated to a saturated layer and cloud radar
    observations can be used to prescribe the vertical distribution of liquid
    water \cite[authoryears][Lohnert2004,Turner2007]. When modifying the prior,
    care has to be taken not to make the covariance matrix singular. If no easy
    way of including an additional information into the prior distribution can
    be found, it might be sufficient to use such information for the first
    guess of the optimal estimation iterative procedure in order to encourage
    convergence to a profile that is consistent with the additional knowledge.
    Using regularization terms as discussed in section \in[ch:costfunction],
    has a similar effect.

    Instruments from which entire atmospheric profiles can be retrieved
    and for which numerical forward models exists can be included in the
    optimal estimation framework by multiple Bayesian updates. These updates
    can either be performed sequentially one model after the other or
    simultaneously. The latter case is discussed in the book by
    \cite[Rodgers2000] in chapter 4.1.1. He presents a general approach for the
    determination of the posterior PDF from multiple measurements with
    associated forward models. If $n$ observations $\VECY_i$ are available that
    can be related to the state vector by forward models $\FWD_i$ and the
    forward model errors are pairwise independent, then Bayes' theorem takes
    the form

    \startformula
    \startalign[n=2,align={right,left}]
        \NC \PROB{\VECX \GIVEN \VECY_1,\dots,\VECY_n} =
            \NC \frac{\PROB{\VECY_1,\dots,\VECY_n \GIVEN \VECX}\PRIOR}{\PROB{\VECY_1,\dots,\VECY_n}} \NR
        \NC = \NC \frac{\PRIOR}{\PROB{\VECY_1,\dots,\VECY_n}} \prod_{i=1}^n \PROB{\VECY_i \GIVEN x} \NR
        \NC = \NC \frac{\PRIOR}{\PROB{\VECY_1,\dots,\VECY_n}}
            \prod_{i=1}^n \GAUSS{\VECY_i}{\FWD_i(\VECX) + \MEANVEC_i}{\COVMAT_i} \EQCOMMA \NR
    \stopalign
    \stopformula

    where the independence of forward model errors is used for the second
    identity.

    There is some controversy regarding the usefulness of surface observations
    as additional information. Some authors claim these are too strongly
    influenced by surface layer processes and not representative of the air
    volume responsible for the radiometer observations
    \cite[alternative=authoryears,left={(e.g. }][Crewell2007]. Research in
    Innsbruck has been focused on the inclusion of in-situ measurements from
    surface stations at different altitudes as they are available in
    mountainous locations. \cite[Massaro2013,Meyer2016] found that such
    observations have a positive effect on the retrieval accuracy if included
    as additional regressors in a linear regression model despite not directly
    probing the volume that affects the radiometer.

\stopsection

