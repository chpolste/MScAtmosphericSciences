Formulate retrieval problem.

Introduce optimal estimation and regression mention Kernel Methods, ... as
alternative regression methods.

Math is heavily influenced by Rodgers and Bishop, important calculations are
repeated to achieve consistent notation and naming conventions throughout.

\startsection[title={Bayesian Statistics}]

    The Bayesian view of statistics is ideal for the formulation of retrieval
    problems. 

    Conditional probability, diachronic interpreation, subjective priors,
    making assumptions explicit, focus on pdf from which values can be
    extracted due to different criteria, pdf carries uncertainty estimates,
    sampling from pdfs...

    Central role: Bayes' theorem.
    
    \placeformula[eq:bayes_theorem]
    \startformula
        \PROB{x|y} = \frac{\PROB{y|x} \PROB{x}}{\PROB{y}}
    \stopformula

    $\PROB{x|y}$ likelihood

    $\PROB{y|x}$ posterior

    $\PROB{x}$ prior

    $\PROB{y}$ normalization, can be obtained by integrating $\PROB{y|x}\PROB{x}$
    over all possible states $x$.

    \placefigure[bottom][fig:bayes_theorem]{Visualization of Bayes' theorem.} {\externalfigure[bayes_theorem][width=\textwidth]}

    Describe different kinds of solutions: maximum likelihood, MAP, ...

    Mention Monte Carlo methods.

\stopsection

\startsection[title=The Multivariate Gaussian Distribution]

    Bayesian view relies heavily on probablility density functions. In a
    theoretical framework it is desireable to work with pdfs that allow
    analytic solutions for a given problem. The most commonly used pdf is the
    Gaussian distribution:

    \placeformula[eq:gaussian]
    \startformula
        \GAUSS{\VECX}{\MEANVEC}{\COVMAT}
        = \frac{1}{(2 \pi)^{d / 2} \det(\COVMAT)^{1/2}}
          \exp \left( -\frac{1}{2}(\VECX - \MEANVEC)^\top \COVMAT^{-1} (\VECX - \MEANVEC) \right)
    \stopformula

    where $\VECX, \MEANVEC \in \REALS^d$ and $\COVMAT \in \REALS^{d
    \times d}$ and the exponential function is applied component-wise.

    The assumption that a continuous variable follows a Gaussian distribution
    is not only popular due to its well understood analytic behavior. central
    limit theorem, maximum entropy.

    The mean vector $\MEANVEC$ of the Gaussian distribution is the
    location of its maximum. The covariance matrix $\COVMAT$ determines
    the shape of the distribution. For a proper Gaussian distribution, it is
    always a symmetric positive definite matrix. When all off-diagonal elements
    of $\COVMAT$ are zero, each component of the multidimensional
    distribution is independent from the others. If nonzero off-diagonal
    elements exist, some or all componentes are correlated.

    Discuss possibility of determining mean of Gauss by search for extrema.

    Refer to default Bayes/Gaussian plot.

    Because the normalization $((2 \pi)^{d/2} \det(\COVMAT)^{1/2})^{-1}$ can
    be determined from the content of the quadratic form in the exponent,
    i.e. the size of the input $N$ and the covariance matrix $\COVMAT$, a
    Gaussian distribution can be described completely in terms of the exponent.
    A common representation of a Gaussian distribution, that will also be used
    in the following, is

    \startformula
        -2 \ln(\GAUSS{\VECX}{\MEANVEC}{\COVMAT})
        = (\VECX - \MEANVEC)^\top \COVMAT^{-1} (\VECX - \MEANVEC) + c \EQCOMMA
    \stopformula

    where $c$ is a constant that contains the normalization. The right hand
    side of this representation is easier to deal with in analytical
    derivations but contains all the information of the distribution.

    Because the assumption of Gaussian {\PDF}s is persistent throughout this
    text, some general results are derived in the following sections. These
    will be used in subsequent sections.

    \startsubsection[title=Parameter Estimation]
        
        For the description of a dataset in the form of a Gaussian distribution
        the mean and covariance parameters have to be estimated from the data.
        This is done by finding the maximum likelihood solutions for $\MEANVEC$
        and $\COVMAT$ given a dataset $\{\VECX_1, \dots, \VECX_N\}$, whose
        elements are assumed to be independent and identically distributed.
        The results are found in most textbooks dealing with statistics
        \cite[alternative=authoryears,left={(e.g. }][Bishop2006]. Results
        are shown here for completeness but no derivations.

        The likelihood of a single datapoint $\VECX_i$ is
        $\GAUSS{\VECX_i}{\MEANVEC}{\COVMAT}$ and because of the assumption of
        independent data the likelihood of the entire dataset is

        \startformula
            \PROB{\{\VECX_1, \dots, \VECX_N\}|\MEANVEC,\COVMAT}
            = \prod_{i=1}^N \GAUSS{\VECX_i}{\MEANVEC}{\COVMAT} \EQCOMMA
        \stopformula

        i.e. the product of each individual likelihood. The product of
        Gaussians is again Gaussian and mean of a Gaussian is its maximum
        therefore $\MEANVEC$ is found by maximizing the likelihood function of
        the dataset or equivalently minimizing its negative natural logarithm.
        Determination of the covariance matrix is more involved as it has to be
        symmetric and positive definite. The resulting ML estimates are

        \startformula
            \MEANVEC_{ML} = \frac{1}{N} \sum_{i=1}^N \VECX_i
        \stopformula

        and

        \startformula
            \COVMAT_{ML} = \frac{1}{N} \sum_{i=1}^N (\VECX_i - \MEANVEC_{ML})^\top
                (\VECX_i - \MEANVEC_{ML})
        \stopformula

        Notably the mean can be calculated independently from the covariance.
        The ML estimator of the covariance systematically underestimates the
        true covariance but can be corrected by using

        \startformula
            \tilde\COVMAT_{ML} = \frac{1}{N-1} \sum_{i=1}^N (\VECX_i - \MEANVEC_{ML})^\top
                (\VECX_i - \MEANVEC_{ML})
        \stopformula

        instead of $\COVMAT_{ML}$.

    \stopsubsection

    \startsubsection[title={Bayes' Theorem for Gaussians},reference=ch:bayes_gauss]

        A convenient property of Gaussian distributions is that the product of
        a Gaussian conditional distribution $\LIKELIHOOD$ (the likelihood
        function) and a Gaussian marginal distribution $\PRIOR$ (the prior) is
        again Gaussian \cite[authoryears][Bishop2006]. This allows the analytic
        calculation of the posterior $\POSTERIOR$ if two assumptions hold: the
        dependence of the conditional distribution's mean is linear with
        respect to the prior's argument ($\VECX$) and its covariance is
        independent of $\VECX$. Let

        \startformula
        \startalign[n=2,align{right,left}]
            \NC -2 \ln(\LIKELIHOOD) = \NC
                \GAUSSEXP{\VECY}{\FWDJAC\VECX - \VECB}{\COVMATERR} + c_1 \NR
            \NC -2 \ln(\PRIOR) = \NC
                \GAUSSEXP{\VECX}{\MEANVECA}{\COVMATA} + c_2 \EQSTOP \NR
        \stopalign
        \stopformula

        The normalization is contained in $c_1$ and $c_2$ respectively. These
        {\PDF}s are multivariate Gaussians fulfilling the above assumptions.
        The posterior distribution is given by:

        \placesubformula
        \startformula
        \startalign[n=2, align{right,left}]
            \NC -2 \ln(\POSTERIOR) = \NC -2 \ln(\LIKELIHOOD \PRIOR) \NR
            \NC = \NC -2 \ln(\LIKELIHOOD) - 2 \ln(\PRIOR) \NR
            \NC = \NC
                \GAUSSEXP{\VECY}{\FWDJAC \VECX - \VECB}{\COVMATERR}
                + \GAUSSEXP{\VECX}{\MEANVECA}{\COVMATA} + c_1 + c_2
                \NR
            \NC = \NC
                  \VECY^\top \COVMATERR^{-1} \VECY
                - \VECY^\top \COVMATERR^{-1} \FWDJAC \VECX
                - \VECY^\top \COVMATERR^{-1} \VECB
                \NR
            \NC \NC
                \mathbox{cornflowerblue}{- \VECX^\top \FWDJAC^\top \COVMATERR^{-1} \VECY}
                + \mathbox{yellowgreen}{\VECX^\top \FWDJAC^\top \COVMATERR^{-1} \FWDJAC \VECX}
                + \mathbox{cornflowerblue}{\VECX^\top \FWDJAC^\top \COVMATERR^{-1} \VECB}
                \NR
            \NC \NC
                - \VECB^\top \COVMATERR^{-1} \VECY
                + \VECB^\top \COVMATERR^{-1} \FWDJAC \VECX
                + \VECB^\top \COVMATERR^{-1} \VECB
                \NR[eq:gaussprodexpand]
            \NC \NC
                + \mathbox{yellowgreen}{\VECX^\top \COVMATA^{-1} \VECX}
                ~\mathbox{cornflowerblue}{- \VECX^\top \COVMATA^{-1} \MEANVECA}
                - \MEANVECA^\top \COVMATA^{-1} \VECX
                + \MEANVECA^\top \COVMATA^{-1} \MEANVECA
                \NR
            \NC \NC + c_1 + c_2 \EQCOMMA \NR
        \stopalign
        \stopformula
        
        where the last step is expanding the vector-matrix-vector product.
        Subscripts are chosen in anticipation of chapter
        \in[ch:optimalestimation]. The posterior has to be Gaussian, therefore

        \placesubformula
        \startformula
        \startalign[n=2,align={right,left}]
            \NC -2 \ln(\POSTERIOR) = \NC
                \GAUSSEXP{\VECX}{\MEANVEC}{\COVMAT} + c_3 \NR
            \NC = \NC
                 \mathbox{yellowgreen}{\VECX^\top \COVMAT^{-1} \VECX}
                 ~\mathbox{cornflowerblue}{- \VECX^\top \COVMAT^{-1} \MEANVEC}
                 - \MEANVEC^\top \COVMAT^{-1} \VECX
                 + \MEANVEC^\top \COVMAT^{-1} \MEANVEC
                 + c_3 \NR[eq:gausspostexpand]
        \stopalign
        \stopformula

        must ben a equivalent representation. Noting that the equivalence must
        hold for all $\VECX$, an expression for $\COVMAT$ is foundy by
        matching terms quadratic in $\VECX$ of both forms
        \ineq{gaussprodexpand} and \ineq{gausspostexpand} (highlighted
        green):

        \placesubformula
        \startformula
        \startalign[n=3,align={left,right,left}]
            \NC \NC - \VECX^\top \COVMAT^{-1} \VECX = \NC
                \VECX^\top \FWDJAC^\top \COVMATERR^{-1} \FWDJAC \VECX
                + \VECX^\top \COVMATA^{-1} \VECX
             = \VECX^\top (\FWDJAC^\top \COVMATERR^{-1} \FWDJAC
                + \COVMATA^{-1}) \VECX \NR
            \NC \Rightarrow~~ \NC \COVMAT = \NC
                (\FWDJAC^\top \COVMATERR^{-1} \FWDJAC + \COVMATA^{-1})^{-1}
                \EQSTOP \NR[eq:gausspostcov]
        \stopalign
        \stopformula

        Matching the linear terms involving $\VECX^\top$ of both forms
        \ineq{gaussprodexpand} and \ineq{gausspostexpand} (highlighted
        blue) results in an expression for $\MEANVEC$:

        \placesubformula
        \startformula
        \startalign[n=3,align={left,right,left}]
            \NC \NC - \VECX^\top \COVMAT^{-1} \MEANVEC = \NC
                - \VECX^\top \FWDJAC^\top \COVMATERR^{-1} \VECY
                + \VECX^\top \FWDJAC^\top \COVMATERR^{-1} \VECB
                - \VECX^\top \COVMATA^{-1} \MEANVECA \NR
            \NC \NC  = \NC
                - \VECX^\top (\FWDJAC^\top \COVMATERR^{-1} (\VECY - \VECB)
                + \COVMATA^{-1} \MEANVECA) \NR
            \NC \Rightarrow~~ \NC \MEANVEC = \NC
                \COVMAT (\FWDJAC^\top \COVMATERR^{-1} (\VECY - \VECB)
                + \COVMATA^{-1} \MEANVECA)
                \EQCOMMA \NR[eq:gausspostmean]
        \stopalign
        \stopformula

        where $\COVMAT$ is given by \ineq{gausspostcov}. Equating terms
        linear in $\VECX$ yields the same result.  An alternative derivation of
        \ineq{gausspostcov} and \ineq{gausspostmean} is given by
        \cite[Bishop2006] in chapter 2.3.3.

    \stopsubsection

    \startsubsection[title=Diagonalization of the Covariance Matrix]

        Transformation into eigenspace. Used for uncertainty estimates of
        linear regression technique.

    \stopsubsection

    
    Plot stuff.

    cite Bishop, Rodgers

\stopsection


\startsection[title={Optimal Estimation},reference=ch:optimalestimation]

    Reference Rodgers, explain how it fits together.

    The optimal estimation approach directly uses Bayes' theorem
    \ineq{bayes_theorem} to obtain the posterior probability distribution
    $\POSTERIOR$ using a forward model and prior information of the state
    vector.

    Call it physical retrieval because it uses the forward model.

    The assumptions for the prior {\PDF} and the likelihood function
    are that they are Gaussian.

    Justify assumptions.

    The prior $\PRIOR$ should contain all knowledge about the state vector
    before the measurement is evaluated. This knowledge might come from
    a climatology, output from a NWP model, information from other instruments
    and any combination of these sources. The construction of an optimal prior
    is not an easy task and will be discussed in section
    \in[ch:construct_prior]. In any case, due to the assumption that the
    {\PDF} is Gaussian it will have the form:

    \placeformula[eq:optimalest_prior]
    \startformula
        \PRIOR = \GAUSS{\VECX}{\MEANVECA}{\COVMATA}
    \stopformula

    The subscript stands for \quotation{a-priori}.

    The likelihood function $\LIKELIHOOD$ expresses the confidence that the
    observation $\VECY$ is the result of a state $\VECX$. This confidence is
    quantified with the help of the forward model, which can be formulated as:

    \placeformula[eq:forward_model]
    \startformula
        \VECY = \FWD(\VECX) + \ERR
    \stopformula

    Where $\FWD$ is the forward model operator and $\ERR$ is an error term,
    arising from measurement noise, inaccuracies of the forward model,
    interpolation errors and possibly other terms. Again, the assumption is
    that the errors are Gaussian, centered around a systematic bias
    $\MEANVECERR$ and with a convariance matrix $\COVMATERR$:

    \placeformula[eq:optimalest_errpdf]
    \startformula
        \PROB{\ERR} = \GAUSS{\ERR}{\MEANVECERR}{\COVMATERR}
    \stopformula

    A discussion of how $\MEANVECERR$ and $\COVMATERR$ can be determined is
    given in section \in[rtm_errors]. Substituting \ineq{forward_model}
    into \ineq{optimalest_errpdf} and changing the independent variable of
    the {\PDF} results in the likelihood function:
    
    \placesubformula
    \startformula
    \startalign[n=3,align={right,middle,left}]
        \NC \PROB{\VECY - \FWD(\VECX)} = \NC
            \GAUSS{\VECY - \FWD(\VECX)}{\MEANVECERR}{\COVMATERR} \NC \NR
        \NC = \NC \GAUSS{\VECY}{\FWD(\VECX)+\MEANVECERR}{\COVMATERR} \NC
            = \LIKELIHOOD \NR[eq:optimalest_likelihood][]
    \stopalign
    \stopformula

    Knowing \ineq{optimalest_prior} and \ineq{optimalest_likelihood} the
    posterior {\PDF} is given by

    \placeformula[eq:optimalest_posterior]
    \startformula
        \POSTERIOR
        = \frac{\LIKELIHOOD \PRIOR}{\NORMALIZATION}
        = \frac{\GAUSS{\VECY}{\FWD(\VECX) + \MEANVECERR}{\COVMATERR}
            ~\GAUSS{\VECX}{\MEANVECA}{\COVMATA}}{\NORMALIZATION} \EQCOMMA
    \stopformula

    but because the forward model is a nonlinear function, the product of the
    Gaussians cannot be carried out using the results from section
    \in[ch:bayes_gauss] yet.

    \startsubsection[title=Iterative Solutions]

        Taylor series around a state $\ITER{\MEANVEC}{i}$, throw away higher
        order terms

        \startformula
            \FWD(\VECX) \approx \FWD(\ITER{\MEANVEC}{i})
                + \ITER{\FWDJAC}{i} (\VECX - \ITER{\MEANVEC}{i})
        \stopformula

        Insert into \ineq{optimalest_posterior} equating

        \startformula
            \VECB = \FWD(\ITER{\MEANVEC}{i}) + \ITER{\FWDJAC}{i}
            \ITER{\MEANVEC}{i} + \MEANVECERR
        \stopformula

        to obtain an expression for the posterior where the mean of the
        likelihood function is linear in $\VECX$ and \ineq{gausspostcov} and
        \ineq{gausspostmean} can be used to determine the posterior
        distribution parameters. Because of the linearization the solutions for
        $\MEANVEC$ and $\COVMAT$ will not be optimal. Use fixed-point iteration
        to find the optimal solution, leading to an iterative scheme

        \startsubformulas[eq:gausspostiter]
        \placesubformula
        \startformula
        \startalign[n=3,align={right,left,right}]
            \NC \ITER{\MEANVEC}{i+1} = \NC
                \ITER{\COVMAT}{i} (\ITER{\FWDJAC}{i}^\top \COVMATERR^{-1}
                (\VECY - \FWD(\ITER{\MEANVEC}{i} - \MEANVECERR)
                + \ITER{\FWDJAC}{i} \ITER{\MEANVEC}{i}) + \COVMATA^{-1} \MEANVECA)
                \EQCOMMA \NC \NR[eq:gausspostmeaniter][a]
            \NC \ITER{\COVMAT}{i} = \NC
                (\ITER{\FWDJAC}{i}^\top \COVMATERR^{-1} \ITER{\FWDJAC}{i}
                + \COVMATA^{-1})^{-1}
                \EQSTOP \NC \NR[eq:gausspostcoviter][b]
        \stopalign
        \stopformula
        \stopsubformulas

        In practice more robust iteration schemes are needed to obtain good
        convergence properties. The search for the mean of the posterior can be
        translated into a minimization problem, for which many well studied
        numerical methods exist. This is because of the symmetry of Gaussian
        which implies that the mean of the distribution is also its maximum
        \footnote{In Bayesian terminology this is called the maximum
        a-posteriori solution or MAP for short. The equivalence of MAP and
        the expected value is one of the convenient analytic properties of
        the Gaussian distribution which make it such an attractive
        assumption.}. The natural logarithm is a strictly monotonic function
        therefore the maximum distribution is equal to the minimum of its
        negated quadratic form. The task is then to find the root of the
        gradient of the quadratic form of \ineq{optimalest_posterior} with
        respect to $\VECX$:

        \placeformula
        \startformula
        \startalign[n=2,align={right,left}]
            \NC 0 = \NC \nabla_{\VECX} (- 2 \ln(\POSTERIOR)) \NR
            \NC = \NC (\VECY - \FWD(\VECX) - \MEANVECERR)^\top \COVMAT^{-1}
                (\VECY - \FWD(\VECX) - \MEANVECERR) + (\VECX - \MEANVECA)^\top
                \COVMATA (\VECX - \MEANVECX)^\top \EQSTOP \NR[eq:gaussminimization]
        \stopalign
        \stopformula
        
        Usually one settles for a least squares solution to the minimization
        problem as methods for these problems are much simpler to implement
        (e.g. Newton's method for finding the root of a function requires the
        Hessian, which is often expensive to calculate). \cite[Rodgers2000]
        shows resulting iterative schemes for the Gauss-Newton method and
        a more robust version of it, the Levenberg-Marquard iteration. Using
        the identity

        \startformula
            \MEANVECA - \ITER{\COVMAT}{i} \ITER{\FWDJAC}{i}^\top
            \COVMATERR^{-1} \ITER{\FWDJAC}{i} \MEANVECA =
            \ITER{\COVMAT}{i} (\ITER{\COVMAT}{i}^{-1}
            - \ITER{\FWDJAC}{i}^\top \COVMATERR^{-1} \ITER{\FWDJAC}{i})
            \MEANVECA = \ITER{\COVMAT}{i} \COVMATA^{-1} \MEANVECA \EQCOMMA
        \stopformula

        it is easily seen that \ineq{gausspostmeaniter}, the solution obtained
        from fixed point iteration with the linearized forward model, is
        equivalent to equation 5.9 from \cite[Rodgers2000], which he obtained
        by applying the Gauss-Newton method. This underlines the equivalence
        of the two approaches.

        TODO: convergence criterion, cite one or two examples where convergence
        was a problem.

        \placefigure[top][fig:iteralgo]
                {Visualization of the iterative retrieval algorithm. Missing
                is the forward model error distribution, which is used in the
                Levenberg-Marquard step. Square boxes represent numerical
                calculations, rounded boxes data input and octagonal boxes
                data being produced during the retrieval.}
                {\FLOWchart[flow:iteralgo]}

        The flowchart in figure \in[fig:iteralgo] visualizes the resulting
        iterative retrieval scheme. Given the prior knowledge, the measurement
        and an initial guess of the profile, minimization steps with a
        linearized forward model are taken until convergence. The profile
        of the last iteration is then the result of the retrieval.

    \stopsubsection

    \startsubsection[title={1D-VAR, Cost Functions and Regularization}]

        There is another way to derive the optimal estimation retrieval scheme.
        Instead of working directly with probability distributions the
        retrieval problem can be approached as a minimization of the expected
        value of the retrieval error variance. In case of the assumption of
        Gaussian distributions this leads to the same solutions as the Bayesian
        approach. Because of the focus on minimization variance this approach
        is called \quote{variational} often abbreviated as 1D-VAR, since it is
        an applied to an atmospheric profile which represents one dimension of
        the atmosphere. A derivation from the variational perspective is found
        in chapter 4.2 in the book by \cite[Rodgers2000].

        The quadratic form minimized in the MAP solution
        \ineq{gaussminimization} is often referred to as a cost function,
        terminology that is common in regression problems.  It is popular to
        add so-called regularization or penalty terms to this cost function in
        order to make the retrieval favor physically realistic solutions. In
        atmospheric profile retrieval such a penalty might be a term involving
        the lapse rate, raising the \quote{cost} of having superadiabatic
        layers in the atmosphere as these only occur near the surface (CITE).
        REF and \cite[Hewison2006] added a term regulating the amount of LWC,
        with a higher penalty for higher cloud water content.
        
        There is a problem with including information in the retrieval by
        adding regularization terms: while the information is included in the
        retrieved profile, it is absent from its uncertainty estimate given by
        $\ITER{\COVMAT}{i}$. This issue can be resolved with the Bayesian
        perspective. Viewing the cost function as the product of the prior
        and likelihood function it becomes obvious that each additional penalty
        term is nothing else than a multiplication by a distribution
        representing prior knowledge. For a quadratic penalty term the
        associated distribution is Gaussian, making the posterior a Gaussian
        and an analytic form of the uncertainty can be derived as shown in the
        next section. For other terms the distribution might not permit an
        analytic form of the posterior. Then one has to resort to discretized
        distributions and/or Monte Carlo methods which are costly due to the
        high dimensionality of the retrieval problem.

        Regularization is a powerful tool for the inclusion of additional
        knowledge but penalities more complex than quadratic forms generally
        violate the Gaussian assumptions leading to inaccurate uncertainty
        estimates. Often these errors are tolerable since for many end users
        a more accuratly retrieved profile is more valuable than an
        inconsistent uncertainty assessment.

    \stopsubsection

    \startsubsection[title=A Generalization to Multiple Measurements]

        As shown by \cite[Rodgers2000] in chapter 4.1.1, there is a more
        general way to construct the posterior {\PDF} $\POSTERIOR$. If $n$
        measurements $\VECY_i$ are available that can be related to the state
        vector by forward models $\FWD_i$ and the forward model errors are
        pairwise independent, then Bayes' theorem takes the form

        \startformula
        \startalign[n=2,align={right,left}]
            \NC \PROB{\VECX|\VECY_1,\dots,\VECY_n} =
                \NC \frac{\PROB{\VECY_1,\dots,\VECY_n|\VECX}\PRIOR}{\PROB{\VECY_1,\dots,\VECY_n}} \NR
            \NC = \NC \frac{\PRIOR}{\PROB{\VECY_1,\dots,\VECY_n}} \prod_{i=1}^n \PROB{\VECY_i|x} \NR
            \NC = \NC \frac{\PRIOR}{\PROB{\VECY_1,\dots,\VECY_n}}
                \prod_{i=1}^n \GAUSS{\VECY_i}{\FWD_i(\VECX) + \MEANVEC_i}{\COVMAT_i} \EQCOMMA \NR
        \stopalign
        \stopformula

        where the independence property of the forward model errors has been
        used for the second identity. With this formulation, any measurements
        that give information on the state vector can be included in the
        optimal estimation procedure. This includes the a-priori knowledge,
        which might be interpreted as a \quotation{virtual measurement} having
        the identity function as a forward model and the prior state
        uncertainties as errors. In this interpretation, no prior knowledge
        is explicitly included in the model, i.e. $\PRIOR$ is constant, and the
        maximum a-posteriori solution is equivalent to a maximum likelihood
        solution.

        Give formulas for combined map and cov solution from Rodgers, this
        might actually be used for the combination of COSMO7 and Nordkette
        measurements.
        
    \stopsubsection

    \startsubsection[title={Constructing the Prior},reference=ch:construct_prior]

        Choosing the prior (=background) state. Constructing the covariance
        matrix.

    \stopsubsection

\stopsection


\startsection[title=Linear Regression]

    Based on climatology, where to get climatology from. Idea: approximate
    the inverse model from climatology instead of introducting a numerical
    physical model.

    Adding regressors. No explicit forward model or error estimate needed.
    Who has done it? Problem: no constraint, rather a suggestion. Known
    elements of state vector are not enforced.
    
    Quantifying Uncertainty by transformation into eigensystem, diagonal
    covariance.

\stopsection


\startsection[title=Neural Network Regression]

    Who has used it? Non-linearity. Extrapolation problems. No error estimates
    from common formulation. Choice of hidden layer and activation functions.
    Advantage in resolving low level inversions?

\stopsection


\startsection[title=Comparison of Techniques]

    Ease of use, computational efficiency, inclusion of additional information.
    Information content of output: uncertainty.

\stopsection

