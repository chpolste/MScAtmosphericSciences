The retrieval problem of atmospheric remote sensing can be stated as: given
a measurement of a sounding device, derive some part of the atmospheric state,
e.g. a vertical profile of temperature. The difficulty is that this derivation
requires the inversion of radiative transfer which is an ill-posed problem
due to the complex nature of electromagnetic wave propagation in the
atmosphere. When a passive instrument observes atmospheric radiation, its
measurement is affected by many emitting and absorbing layers of the atmosphere
and multiple states may exist which result in the same measurement. Therefore
additional information is needed to arrive at a unique solution which is
as close as possible to the true atmospheric state. Depending on the available
information and recources, different retrieval techniques are applicable. A
great variety of regression models exists which are purely statistical methods.
These directly model the inverse problem, predicting the atmospheric state
from a measurement based on a training data set, often a time series of past
measurements for which the corresponding atmospheric state is known by some
other method. Regression methods are generally easy to set up once such a data
set is available but they carry the risk of bad performance in situations not
adequately represented in the training data. Other methods make use of a
numerical model of the forward problem, which is used in combination with
boundary conditions given by additional measurements and/or statistical
knowledge to iteratively determine a solution that is consitent with both the
boundary conditions and the forward model. Such methods are called optimal
methods or physical retrievals.

Here, an optimal estimation method is derived and compared to the commonly used
linear regression method. This chapter will review the necessary mathematics
for both methods entirely in the framework of Bayesian statistics. All
following results have beed derived before. They are derived here again to
achieve consistent notation and to show the benefits of starting from a purely
Bayesian approach. The calculations are based on books by \cite[Rodgers2000]
and \cite[Bishop2006]. The former treats all aspects of the retrieval problem
specifically for atmospheric sounding from a mixed perspective of classical and
Bayesian statistics. His results are used for the optimal estimation technique.
The book by Bishop is the foundation for general results on Bayesian statistics
and the Gaussian distribution as well as for the Bayesian approach to linear
regression. The introduction to Bayesian statistics is additionally influenced
by \cite[Downey2013].

\startsection[title={Bayesian Statistics}]

    The Bayesian view of statistics is ideal for the formulation of retrieval
    problems. 

    Conditional probability, diachronic interpreation, subjective priors,
    making assumptions explicit, focus on pdf from which values can be
    extracted due to different criteria, pdf carries uncertainty estimates,
    sampling from pdfs, marginalization...

    Central role: Bayes' theorem.
    
    \placeformula[eq:bayes_theorem]
    \startformula
        \PROB{x \GIVEN y} = \frac{\PROB{y \GIVEN x} \PROB{x}}{\PROB{y}}
    \stopformula

    $\PROB{x \GIVEN y}$ likelihood

    $\PROB{y \GIVEN x}$ posterior

    $\PROB{x}$ prior

    $\PROB{y}$ normalization, can be obtained by integrating $\PROB{y \GIVEN x}\PROB{x}$
    over all possible states $x$.

    \placefigure[bottom][fig:bayes_theorem]
        {Visualization of a Bayesian update. A prior {\PDF} is combined
        with the likelihood {\PDF} according to Bayes' theorem resulting in
        a posterior distribution combining the knowledge contained in both.}
        {\externalfigure[bayes_theorem][width=\textwidth]}

    Describe different kinds of solutions: maximum likelihood, MAP, ...

    Mention Monte Carlo methods.

\stopsection

\startsection[title=The Multivariate Gaussian Distribution]

    Bayesian view relies heavily on probablility density functions. In a
    theoretical framework it is desireable to work with pdfs that allow
    analytic solutions for a given problem. The most commonly used pdf is the
    Gaussian distribution:

    \placeformula[eq:gaussian]
    \startformula
        \GAUSS{\VECX}{\MEANVEC}{\COVMAT}
        = \frac{1}{(2 \pi)^{d / 2} \det(\COVMAT)^{1/2}}
          \exp \left( -\frac{1}{2}(\VECX - \MEANVEC)^\top \COVMAT^{-1} (\VECX - \MEANVEC) \right)
    \stopformula

    where $\VECX, \MEANVEC \in \REALS^d$ and $\COVMAT \in \REALS^{d
    \times d}$ and the exponential function is applied component-wise.

    The mean vector $\MEANVEC$ of the Gaussian distribution is the
    location of its maximum. The covariance matrix $\COVMAT$ determines
    the shape of the distribution. For a proper Gaussian distribution, it is
    always a symmetric positive definite matrix. When all off-diagonal elements
    of $\COVMAT$ are zero, each component of the multidimensional
    distribution is independent from the others. If nonzero off-diagonal
    elements exist, some or all componentes are correlated.

    Discuss possibility of determining mean of Gauss by search for extrema.

    Refer to default Bayes/Gaussian plot.

    Because the normalization $((2 \pi)^{d/2} \det(\COVMAT)^{1/2})^{-1}$ can
    be determined from the content of the quadratic form in the exponent,
    i.e. the size of the input $N$ and the covariance matrix $\COVMAT$, a
    Gaussian distribution can be described completely in terms of the exponent.
    A common representation of a Gaussian distribution, that will also be used
    in the following, is

    \startformula
        -2 \ln(\GAUSS{\VECX}{\MEANVEC}{\COVMAT})
        = (\VECX - \MEANVEC)^\top \COVMAT^{-1} (\VECX - \MEANVEC) + c \EQCOMMA % TODO: use \GAUSSEXP
    \stopformula

    where $c$ is a constant that contains the normalization. The right hand
    side of this representation is easier to deal with in analytical
    derivations but contains all the information of the distribution.

    Throughout this thesis it will be assumed again and again that continuous
    variables follow a Gaussian distribution. A major reason for choosing this
    assumption is that Gaussian distributions have very convenient analytic
    properties.  Particularly useful is that the marginalization of a Gaussian
    yields a Gaussian and that a conditional distribution of two Gaussian
    variables is again Gaussian \cite[authoryears][Bishop2006]. TODO
    The assumption that a continuous variable follows a Gaussian distribution
    is not only popular due to its well understood analytic behavior. central
    limit theorem, maximum entropy.

    Because the assumption of Gaussian {\PDF}s is persistent throughout this
    text, some general results are derived in the following sections. These
    will be used in subsequent sections.

    \startsubsection[title=Parameter Estimation]
        
        For the description of a dataset in the form of a Gaussian distribution
        the mean and covariance parameters have to be estimated from the data.
        This is done by finding the maximum likelihood solutions for $\MEANVEC$
        and $\COVMAT$ given a dataset $\{\VECX_1, \dots, \VECX_N\}$, whose
        elements are assumed to be independent and identically distributed.
        The results are found in most textbooks dealing with statistics
        \cite[alternative=authoryears,left={(e.g. }][Bishop2006]. Results
        are shown here for completeness but no derivations.

        The likelihood of a single datapoint $\VECX_i$ is
        $\GAUSS{\VECX_i}{\MEANVEC}{\COVMAT}$ and because of the assumption of
        independent data the likelihood of the entire dataset is

        \startformula
            \PROB{\{\VECX_1, \dots, \VECX_N\} \GIVEN \MEANVEC, \COVMAT}
            = \prod_{i=1}^N \GAUSS{\VECX_i}{\MEANVEC}{\COVMAT} \EQCOMMA
        \stopformula

        i.e. the product of each individual likelihood. The product of
        Gaussians is again Gaussian and mean of a Gaussian is its maximum
        therefore $\MEANVEC$ is found by maximizing the likelihood function of
        the dataset or equivalently minimizing its negative natural logarithm.
        Determination of the covariance matrix is more involved as it has to be
        symmetric and positive definite. The resulting ML estimates are

        \startformula
            \MEANVEC_{ML} = \frac{1}{N} \sum_{i=1}^N \VECX_i
        \stopformula

        and

        \startformula
            \COVMAT_{ML} = \frac{1}{N} \sum_{i=1}^N (\VECX_i - \MEANVEC_{ML})^\top
                (\VECX_i - \MEANVEC_{ML})
        \stopformula

        Notably the mean can be calculated independently from the covariance.
        The ML estimator of the covariance systematically underestimates the
        true covariance but can be corrected by using

        \startformula
            \tilde\COVMAT_{ML} = \frac{1}{N-1} \sum_{i=1}^N (\VECX_i - \MEANVEC_{ML})^\top
                (\VECX_i - \MEANVEC_{ML})
        \stopformula

        instead of $\COVMAT_{ML}$.

    \stopsubsection


    \stopsubsection

    \startsubsection[title={Bayesian Operations for Gaussians},reference=ch:bayes_gauss]

        In this section two important results for Gaussian distributions are
        derived which are used later for the optimal estimation method and
        linear regression with error estimates. A rigorous derivation of these
        results is found in \cite[Bishop2006]. Here, shortcuts are taken
        by taking the facts that the product of two Gaussians is Gaussian and
        that the marginalization of a Gaussian is again Gaussian as a given.

        The goal is to find the missing probability distributions in Bayes'
        Theorem when the prior $\PRIOR$ and likelihood function $\LIKELIHOOD$
        are given by

        \startsubformulas[eq:gaussbayesinit]
        \placesubformula
        \startformula
        \startalign[n=2,align={right,left}]
            \NC \PRIOR = \NC \GAUSS{\VECX}{\VECA}{\MATPI} \EQCOMMA \NR[eq:gaussbayesinitx]
            \NC \LIKELIHOOD = \NC \GAUSS{\VECY}{\MATB \VECX + \VECB}{\MATQI}
                \EQSTOP \NR[eq:gaussbayesinityx]
        \stopalign
        \stopformula
        \stopsubformulas

        The mean of the conditional likelihood function is a linear function
        of $\VECX$ but the covariance is independent of $\VECX$. Covariances
        are given as inverted $\MATP$ and $MATQ$ making the notation of the
        Gaussians in their logarithm form less cluttered:

        \startsubformulas[eq:gaussbayesinitln]
        \placesubformula
        \startformula
        \startalign[n=2,align={right,left}]
            \NC -2 \ln(\PRIOR) = \NC (\VECX - \VECA)^\top \MATP (\VECX - \VECA)
                + \CONST \EQCOMMA \NR[eq:gaussbayesinitlnx]
            \NC -2 \ln(\LIKELIHOOD) = \NC (\VECY - \MATB \VECX - \VECB)^\top
                \MATQ (\VECY - \MATB \VECX - \VECB) + \CONST
                \EQSTOP \NR[eq:gaussbayesinitlnyx]
        \stopalign
        \stopformula
        \stopsubformulas

        The terms denoted \quote{const} contain the respective normalizations.
        Bayes' theorem is derived from the joint probability distribution

        \startformula
            \JOINT = \LIKELIHOOD \PRIOR = \POSTERIOR \NORMALIZATION \EQSTOP
        \stopformula

        Viewing $\LIKELIHOOD$ and $\PRIOR$ as likelihood and prior makes
        $\POSTERIOR$ the posterior distribution. It can be extracted from
        the joint probability distribution or derived by the product of
        $\LIKELIHOOD$ and $\PRIOR$ directly since $\NORMALIZATION$ only
        provides a constant normalization which can also be determined by the
        covariance of $\POSTERIOR$.

        \placesubformula
        \startformula
        \startalign[n=2,align={right,left}]
            \NC -2 \ln(\POSTERIOR) = \NC -2 \ln\left(\frac{\JOINT}{\NORMALIZATION}\right) \NR
            \NC = \NC -2 \ln\left(\frac{\LIKELIHOOD \PRIOR}{\NORMALIZATION}\right) \NR
            \NC = \NC -2 \ln(\LIKELIHOOD) - 2 \ln(\PRIOR) + \CONST \NR
            \NC = \NC \GAUSSEXP{\VECY}{\MATB \VECX - \VECB}{\MATQ}
                + \GAUSSEXP{\VECX}{\VECA}{\MATP} \NR
            \NC = \NC
                \VECYT \MATQ \VECY
                - \VECYT \MATQ \MATB \VECX
                - \VECYT \MATQ \VECB
                - \VECXT \MATBT \MATQ \VECY
                \NR
            \NC \NC
                + \VECXT \MATBT \MATQ \MATB \VECX
                + \VECXT \MATBT \MATQ \VECB
                - \VECBT \MATQ \VECY
                + \VECBT \MATQ \MATB \VECX
                \NR[eq:gaussprodexpand]
            \NC \NC
                + \VECXT \MATP \VECX
                - \VECXT \MATP \VECA
                - \VECAT \MATP \VECX
                + \CONST \EQCOMMA
                \NR
        \stopalign
        \stopformula

        where the last step is expanding the vector-matrix-vector products. The
        posterior is again a Gaussian distribution, therefore an equivalent
        representation of the form

        \placesubformula
        \startformula
        \startalign[n=2,align={right,left}]
            \NC -2 \ln(\POSTERIOR) = \NC
                \GAUSSEXP{\VECX}{\VECC}{\MATS} + \CONST \NR
            \NC = \NC
                 \VECXT \MATS \VECX
                 - \VECXT \MATS \VECC
                 - \VECCT \MATS \VECX
                 + \CONST \NR[eq:gausspostexpand]
        \stopalign
        \stopformula

        must exist. Noting that the equivalence must hold for all $\VECX$, an
        expression for $\COVMAT$ is found by matching terms quadratic in
        $\VECX$ of both forms \ineq{gaussprodexpand} and
        \ineq{gausspostexpand}:

        \startformula
        \startalign[n=3,align={left,right,left}]
            \NC \NC \VECXT \MATS \VECX = \NC
                \VECXT \MATBT \MATQ \MATB \VECX + \VECXT \MATP \VECX \NR
            \NC \NC = \NC
                \VECXT (\MATBT \MATQ \MATB + \MATP) \VECX \NR
            \NC \Rightarrow~~ \NC \MATS = \NC
                \MATBT \MATQ \MATB + \MATP
                \EQSTOP
        \stopalign
        \stopformula

        Matching the linear terms involving $\VECXT$ of both forms
        \ineq{gaussprodexpand} and \ineq{gausspostexpand}
        results in an expression for $\VECC$:

        \startformula
        \startalign[n=3,align={left,right,left}]
            \NC \NC - \VECXT \MATS \VECC = \NC
                - \VECXT \MATBT \MATQ \VECY
                + \VECXT \MATBT \MATQ \VECB
                - \VECXT \MATP \VECA \NR
            \NC \NC  = \NC
                - \VECXT (\MATBT \MATQ (\VECY - \VECB) + \MATP \VECA) \NR
            \NC \Rightarrow~~ \NC \VECC = \NC
                \MATSI (\MATBT \MATQ (\VECY - \VECB) + \MATP \VECA) \EQSTOP \NR
        \stopalign
        \stopformula

        Equating terms linear in $\VECX$ yields the same result. With $\VECC$
        ( mean)and $\MATS$ (inverse covariance) determined,

        \placeformula[eq:gaussbayespost]
        \startformula
            \POSTERIOR = \GAUSS{\VECX}{\MATSI(\MATBT \MATQ (\VECY - \VECB) + \MATP \VECA)}{\MATSI}
        \stopformula

        is known. $\NORMALIZATION$ could now be obtained by

        \startformula
            \NORMALIZATION = \frac{\JOINT}{\POSTERIOR} \EQCOMMA
        \stopformula

        making the assumption that it is a Gaussian distribution

        \startformula
            -2 \ln(\NORMALIZATION) = \GAUSSEXP{\VECY}{\VECD}{\MATT}
        \stopformula

        and matching terms between both expressions as done for the posterior.
        Due to the rather complex mean and covariance of $\POSTERIOR$ this is
        tedious. It is also possible to directly integrate the joint
        distribution with respect to $\VECX$
        
        \startformula
            \NORMALIZATION = \int \JOINT \, \diff \VECX = \int \LIKELIHOOD \, \PRIOR \, \diff \VECX \EQCOMMA
        \stopformula
        
        again a quite tedious procedure.  Starting from the joint distribution
        and rewriting it in blocked form, an intuitive argument is made which
        can be reinforced by rigorous derivation given by \cite[Bishop2006].
        Let $\VECZ$ be the vector obtained by stacking $\VECX$ and $\VECY$:

        \startformula
            \VECZ = \startpmatrix \VECX \NR \VECY \NR \stoppmatrix \EQSTOP
        \stopformula

        The joint distribution is Gaussian

        \placeformula
        \startformula
        \startalign
            \NC \JOINTZ = \NC \GAUSS{\VECZ}{\MEANVEC}{\MATRI} \EQCOMMA \NR
            \NC -2 \ln(\JOINTZ) = \NC \GAUSSEXP{\VECZ}{\MEANVEC}{\MATR} + \CONST \NR
            \NC = \NC \VECZT \MATR \VECZ - \VECZT \MATR \MEANVEC
                - \MEANVECT \MATR \VECZ + \CONST \NR[eq:gaussbayesjoint]
        \stopalign
        \stopformula

        and by matching terms with $\JOINT = \LIKELIHOOD \PRIOR$
        \footnote{Which are the same terms as in \ineq{gaussprodexpand} with
        exception of the constant term containing the normalization $2
        \ln(\NORMALIZATION)$.}, $\MATR$ and $\MEANVEC$ can be determined. As
        before, the quadratic terms are used for the inverse of the covariance

        \placeformula
        \startformula
        \startalign[n=2,align={right,left}]
            \NC \NC
                \VECYT \MATQ \VECY
                - \VECYT \MATQ \MATB \VECX
                - \VECXT \MATBT \MATQ \VECY
                + \VECXT \MATBT \MATQ \MATB \VECX
                + \VECXT \MATP \VECX \NR
            \NC = \NC
                \VECZT \startpmatrix[n=2,align={middle,middle}]
                    \NC \MATP + \MATBT \MATQ \MATB \NC - \MATBT \MATQ \NR
                    \NC - \MATQ \MATB \NC \MATQ \NR
                \stoppmatrix \VECZ \NR[eq:gaussbayesjointcovi]
            \NC = \NC
                \VECZT \MATR \VECZ \NR
        \stopalign
        \stopformula

        and linear terms for determination of the mean (here shown with the
        transposed terms)

        \startformula
        \startalign[n=2,align={right,left}]
            \NC \NC
                - \VECYT \MATQ \VECB
                + \VECXT \MATBT \MATQ \VECB
                - \VECXT \MATP \VECA \NR
            \NC = \NC
                - \VECZT \startpmatrix
                    \MATP \VECA - \MATBT \MATQ \VECB \NR
                    \MATQ \VECB \NR
                \stoppmatrix \NR
            \NC = \NC
                - \VECZT \MATR \MEANVEC \EQSTOP \NR
        \stopalign
        \stopformula

        An expression for the inverse of a block matrix is given e.g. by
        \cite[Petersen2012] yielding

        \placeformula[eq:gaussbayesjointcov]
        \startformula
            \MATRI = \startpmatrix[n=2,align={middle,middle}]
                \NC \MATPI \NC \MATPI \MATBT \NR
                \NC \MATB \MATPI \NC \MATQI + \MATB \MATPI \MATBT \NR
            \stoppmatrix \EQCOMMA
        \stopformula
        for \ineq{gaussbayesjointcovi} while using the fact that covariance
        matrices are symmetric. Knowing \ineq{gaussbayesjointcov}, the mean is
        given by

        \placeformula[eq:gaussbayesjointmean]
        \startformula
            \MEANVEC = \MATRI \startpmatrix
                    \MATP \VECA - \MATBT \MATQ \VECB \NR
                    \MATQ \VECB \NR
                \stoppmatrix = \startpmatrix
                    \VECA \NR
                    \MATB \VECA + \VECB \NR
                \stoppmatrix \EQSTOP
        \stopformula

        Comparing \ineq{gaussbayesjointcov} and \ineq{gaussbayesjointmean}
        with the probability distribution $\PRIOR
        = \GAUSS{\VECX}{\VECA}{\MATPI}$ from \ineq{gaussbayesinit} one finds
        that the upper left block of the covariance matrix and the upper block
        of the mean vector of the joint distribution correspond directly to the
        covariance and mean of $\PRIOR$. This makes intuitive sense: When the
        distribution is evaluated, these are the blocks that interact with
        $\VECX$ exclusively while the off-diagonal blocks of $\MATRI$ model the
        interaction between $\VECX$ and $\VECY$. It would make sense that the
        former blocks are then the only ones neccesary to describe the
        (marginalized) distribution with respect to $\VECX$ alone. The same
        idea leads to the expectation that the lower left block of $\MATRI$ and
        the lower block of $\MEANVEC$ are the only ones neccessary to describe
        the (marginalized) distribution with respect to $\VECY$ only. For
        symmetry $\NORMALIZATION$ should therefore be given by
        
        \placeformula[eq:gaussbayesnorm]
        \startformula
            \NORMALIZATION = \GAUSS{\VECY}{\MATB \VECA + \VECB}{\MATQI + \MATB \MATPI \MATBT} \EQCOMMA
        \stopformula

        and this is indeed the result found in a rigorous derivation.

    \stopsubsection

\stopsection


\startsection[title={Optimal Estimation},reference=ch:optimalestimation]

    Reference Rodgers, explain how it fits together.

    The optimal estimation approach directly uses Bayes' theorem
    \ineq{bayes_theorem} to obtain the posterior probability distribution
    $\POSTERIOR$ using a forward model and prior information of the state
    vector.

    Call it physical retrieval because it uses the forward model.

    The assumptions for the prior {\PDF} and the likelihood function
    are that they are Gaussian.

    Justify assumptions.

    The prior $\PRIOR$ should contain all knowledge about the state vector
    before the measurement is evaluated. This knowledge might come from
    a climatology, output from a NWP model, information from other instruments
    and any combination of these sources. The construction of an optimal prior
    is not an easy task and will be discussed in section
    \in[ch:construct_prior]. In any case, due to the assumption that the
    {\PDF} is Gaussian it is of the form:

    \placeformula[eq:optimalest_prior]
    \startformula
        \PRIOR = \GAUSS{\VECX}{\MEANVECA}{\COVMATA} \EQCOMMA
    \stopformula

    where the subscript \quote{a} stands for \quote{a-priori}.  The likelihood
    function $\LIKELIHOOD$ expresses the confidence that the observation
    $\VECY$ is the result of a state $\VECX$. This confidence is quantified
    with the help of the forward model, formulated as

    \placeformula[eq:forward_model]
    \startformula
        \VECY = \FWD(\VECX) + \ERR \EQCOMMA
    \stopformula

    where $\FWD$ is the forward model operator and $\ERR$ is an error term,
    arising from measurement noise, inaccuracies of the forward model,
    discretization errors and possibly other terms. Again, the assumption is
    that the error term follows a Gaussian distribution, centered around
    a systematic bias $\MEANVECERR$ with a convariance matrix $\COVMATERR$

    \placeformula[eq:optimalest_errpdf]
    \startformula
        \PROB{\ERR} = \GAUSS{\ERR}{\MEANVECERR}{\COVMATERR} \EQSTOP
    \stopformula

    A discussion of how $\MEANVECERR$ and $\COVMATERR$ can be determined is
    given in section \in[rtm_errors]. Substituting \ineq{forward_model}
    into \ineq{optimalest_errpdf} and changing the independent variable of
    the {\PDF} to $\VECY$ results in the likelihood function:
    
    \placesubformula
    \startformula
    \startalign[n=3,align={right,middle,left}]
        \NC \PROB{\VECY - \FWD(\VECX)} = \NC
            \GAUSS{\VECY - \FWD(\VECX)}{\MEANVECERR}{\COVMATERR} \NC \NR
        \NC = \NC \GAUSS{\VECY}{\FWD(\VECX)+\MEANVECERR}{\COVMATERR} \NC
            = \LIKELIHOOD \EQSTOP \NR[eq:optimalest_likelihood]
    \stopalign
    \stopformula

    Knowing \ineq{optimalest_prior} and \ineq{optimalest_likelihood}, the
    posterior {\PDF} is given by

    \placeformula[eq:optimalest_posterior]
    \startformula
        \POSTERIOR
        = \frac{\LIKELIHOOD \PRIOR}{\NORMALIZATION}
        = \frac{\GAUSS{\VECY}{\FWD(\VECX) + \MEANVECERR}{\COVMATERR}
            ~\GAUSS{\VECX}{\MEANVECA}{\COVMATA}}{\NORMALIZATION} \EQSTOP
    \stopformula

    It expresses the probability of an atmospheric state $\VECX$ given a
    measurement $\VECY$ and incorporates the forward model with its error
    characteristic and any a-priori knowledge of the atmospheric state before
    the measurement. As all involved distributions are assumed to be Gaussian,
    the $\POSTERIOR$ is also a Gaussian distribution therefore the
    normalization $\NORMALIZATION$ does not have to be determined explicitly
    but follows from the product in the numerator. The forward model operator
    $\FWD$ is non-linear, therefore this product cannot be carried out
    analytically. To use the results from section \in[ch:bayes_gauss], the
    model has to be linearized and an iterative approach is taken.

    \startsubsection[title=Iterative Solutions]

        To linearize $\FWD$, a reference state to linearize at must be chosen.
        Let this state be $\ITER{\VECX}{i}$. Applying a Taylor series
        expansion to the forward model and chopping off all higher-than-linear
        order terms yields the approximation

        \startformula
            \FWD(\VECX) \approx \FWD(\ITER{\VECX}{i})
                + \ITER{\FWDJAC}{i} (\VECX - \ITER{\VECX}{i}) \EQSTOP
        \stopformula

        $\ITER{\FWDJAC}{i}$ is the Jacobian of $\FWD$ at $\ITER{\VECX}{i}$
        containing the derivatives of the forward model output with respect
        to each state vector variable. After replacing $\FWD$ in
        \ineq{optimalest_posterior} by this linearization, the likelihood
        function

        \startformula
            \LIKELIHOOD = \GAUSS{\VECY}{\ITER{\FWDJAC}{i} \VECX
                + \FWD(\ITER{\VECX}{i}) - \ITER{\FWDJAC}{i}
                \ITER{\VECX}{i} + \MEANVECERR}{\COVMATERR}
        \stopformula

        and the prior \ineq{optimalest_prior} fulfill the assumptions
        \ineq{gaussbayesinit} and the result \ineq{gaussbayespost} can be used
        with the substitutions

        \startformula
            \{ \VECA \rightarrow \MEANVECA,~
            \MATPI \rightarrow \COVMATA,~
            \MATB \rightarrow \ITER{\FWDJAC}{i},~
            \VECB \rightarrow \FWD(\ITER{\VECX}{i})
                - \ITER{\FWDJAC}{i} \ITER{\VECX}{i}
                + \MEANVECERR,~
            \MATQI \rightarrow \COVMATERR \}
        \stopformula

        to obtain an expression for the posterior:
        
        \startformula
        \startalign[n=2,align={right,left}]
            \NC \POSTERIOR = \NC \GAUSS{\VECX}{\MEANVEC}{\COVMAT} \NR
            \NC \MEANVEC = \NC \COVMAT (\ITER{\FWDJAC}{i}^\top
                \COVMATERR^{-1} (\VECY - \FWD(\ITER{\VECX}{i})
                + \ITER{\FWDJAC}{i} \ITER{\VECX}{i} - \MEANVECERR)
                + \COVMATA^{-1} \MEANVECA) \NR
            \NC \COVMAT = \NC 
                (\ITER{\FWDJAC}{i} \COVMATERR^{-1} \ITER{\FWDJAC}{i}^\top
                \COVMATA^{-1})^{-1} \NR
        \stopalign
        \stopformula
        
        Because of the linearization the solution for $\MEANVEC$ and $\COVMAT$
        will not be optimal. Starting from a first guess $\ITER{\MEANVEC}{0}$,
        use fixed-point iteration to find the optimal solution, using the mean
        of the previous iteration as the linearization point

        \startsubformulas[eq:gausspostiter]
        \placesubformula
        \startformula
        \startalign[n=3,align={right,left,right}]
            \NC \ITER{\MEANVEC}{i+1} = \NC
                \ITER{\COVMAT}{i} (\ITER{\FWDJAC}{i}^\top
                \COVMATERR^{-1} (\VECY - \FWD(\ITER{\MEANVEC}{i})
                + \ITER{\FWDJAC}{i} \ITER{\MEANVEC}{i} - \MEANVECERR)
                + \COVMATA^{-1} \MEANVECA) \EQCOMMA \NC \NR[eq:gausspostmeaniter][a]
            \NC \ITER{\COVMAT}{i} = \NC
                (\ITER{\FWDJAC}{i}^\top \COVMATERR^{-1} \ITER{\FWDJAC}{i}
                + \COVMATA^{-1})^{-1}
                \EQSTOP \NC \NR[eq:gausspostcoviter][b]
        \stopalign
        \stopformula
        \stopsubformulas

        In practice more robust iteration schemes are needed to obtain good
        convergence properties. The search for the mean of the posterior can be
        translated into a minimization problem, for which many well studied
        numerical methods exist. This is because of the symmetry of Gaussian
        which implies that the mean of the distribution is also its maximum
        \footnote{In Bayesian terminology this is called the maximum
        a-posteriori solution or MAP for short. The equivalence of MAP and
        the expected value is one of the convenient analytic properties of
        the Gaussian distribution which make it such an attractive
        assumption.}. The natural logarithm is a strictly monotonic function
        therefore the maximum distribution is equal to the minimum of its
        negated quadratic form. The task is then to find the root of the
        gradient of the quadratic form of \ineq{optimalest_posterior} with
        respect to $\VECX$:

        \placeformula
        \startformula
        \startalign[n=2,align={right,left}]
            \NC 0 = \NC \nabla_{\VECX} (- 2 \ln(\POSTERIOR)) \NR
            \NC = \NC (\VECY - \FWD(\VECX) - \MEANVECERR)^\top \COVMAT^{-1}
                (\VECY - \FWD(\VECX) - \MEANVECERR) + (\VECX - \MEANVECA)^\top
                \COVMATA (\VECX - \MEANVECX)^\top \EQSTOP \NR[eq:gaussminimization]
        \stopalign
        \stopformula
        
        Usually one settles for a least squares solution to the minimization
        problem as methods for these problems are much simpler to implement
        (e.g. Newton's method for finding the root of a function requires the
        Hessian, which is often expensive to calculate). \cite[Rodgers2000]
        shows resulting iterative schemes for the Gauss-Newton method and
        a more robust version of it, the Levenberg-Marquard iteration. Using
        the identity

        \startformula
            \MEANVECA - \ITER{\COVMAT}{i} \ITER{\FWDJAC}{i}^\top
            \COVMATERR^{-1} \ITER{\FWDJAC}{i} \MEANVECA =
            \ITER{\COVMAT}{i} (\ITER{\COVMAT}{i}^{-1}
            - \ITER{\FWDJAC}{i}^\top \COVMATERR^{-1} \ITER{\FWDJAC}{i})
            \MEANVECA = \ITER{\COVMAT}{i} \COVMATA^{-1} \MEANVECA \EQCOMMA
        \stopformula

        it is easily seen that \ineq{gausspostmeaniter}, the solution obtained
        from fixed point iteration with the linearized forward model, is
        equivalent to equation 5.9 from \cite[Rodgers2000], which he obtained
        by applying the Gauss-Newton method. This underlines the equivalence
        of the two approaches.

        TODO: convergence criterion, cite one or two examples where convergence
        was a problem.

        \placefigure[top][fig:iteralgo]
                {Visualization of the iterative retrieval algorithm. Missing
                is the forward model error distribution, which is used in the
                Levenberg-Marquard step. Square boxes represent numerical
                calculations, rounded boxes data input and octagonal boxes
                data being produced during the retrieval.}
                {\FLOWchart[flow:iteralgo]}

        The flowchart in figure \in[fig:iteralgo] visualizes the resulting
        iterative retrieval scheme. Given the prior knowledge, the measurement
        and an initial guess of the profile, minimization steps with a
        linearized forward model are taken until convergence. The profile
        of the last iteration is then the result of the retrieval.

    \stopsubsection

    \startsubsection[title={1D-VAR, Cost Functions and Regularization}]

        There is another way to derive the optimal estimation retrieval scheme.
        Instead of working directly with probability distributions the
        retrieval problem can be approached as a minimization of the expected
        value of the retrieval error variance. In case of the assumption of
        Gaussian distributions this leads to the same solutions as the Bayesian
        approach. Because of the focus on minimization variance this approach
        is called \quote{variational} often abbreviated as 1D-VAR, since it is
        an applied to an atmospheric profile which represents one dimension of
        the atmosphere. A derivation from the variational perspective is found
        in chapter 4.2 in the book by \cite[Rodgers2000].

        The quadratic form minimized in the MAP solution
        \ineq{gaussminimization} is often referred to as a cost function,
        terminology that is common in regression problems.  It is popular to
        add so-called regularization or penalty terms to this cost function in
        order to make the retrieval favor physically realistic solutions. In
        atmospheric profile retrieval such a penalty might be a term involving
        the lapse rate, raising the \quote{cost} of having superadiabatic
        layers in the atmosphere as these only occur near the surface (CITE).
        REF and \cite[Hewison2006] added a term regulating the amount of LWC,
        with a higher penalty for higher cloud water content.
        
        There is a problem with including information in the retrieval by
        adding regularization terms: while the information is included in the
        retrieved profile, it is absent from its uncertainty estimate given by
        $\ITER{\COVMAT}{i}$. This issue can be resolved with the Bayesian
        perspective. Viewing the cost function as the product of the prior
        and likelihood function it becomes obvious that each additional penalty
        term is nothing else than a multiplication by a distribution
        representing prior knowledge. For a quadratic penalty term the
        associated distribution is Gaussian, making the posterior a Gaussian
        and an analytic form of the uncertainty can be derived as shown in the
        next section. For other terms the distribution might not permit an
        analytic form of the posterior. Then one has to resort to discretized
        distributions and/or Monte Carlo methods which are costly due to the
        high dimensionality of the retrieval problem.

        Regularization is a powerful tool for the inclusion of additional
        knowledge but penalities more complex than quadratic forms generally
        violate the Gaussian assumptions leading to inaccurate uncertainty
        estimates. Often these errors are tolerable since for many end users
        a more accuratly retrieved profile is more valuable than an
        inconsistent uncertainty assessment.

    \stopsubsection

    \startsubsection[title=A Generalization to Multiple Measurements]

        As shown by \cite[Rodgers2000] in chapter 4.1.1, there is a more
        general way to construct the posterior {\PDF} $\POSTERIOR$. If $n$
        measurements $\VECY_i$ are available that can be related to the state
        vector by forward models $\FWD_i$ and the forward model errors are
        pairwise independent, then Bayes' theorem takes the form

        \startformula
        \startalign[n=2,align={right,left}]
            \NC \PROB{\VECX \GIVEN \VECY_1,\dots,\VECY_n} =
                \NC \frac{\PROB{\VECY_1,\dots,\VECY_n \GIVEN \VECX}\PRIOR}{\PROB{\VECY_1,\dots,\VECY_n}} \NR
            \NC = \NC \frac{\PRIOR}{\PROB{\VECY_1,\dots,\VECY_n}} \prod_{i=1}^n \PROB{\VECY_i \GIVEN x} \NR
            \NC = \NC \frac{\PRIOR}{\PROB{\VECY_1,\dots,\VECY_n}}
                \prod_{i=1}^n \GAUSS{\VECY_i}{\FWD_i(\VECX) + \MEANVEC_i}{\COVMAT_i} \EQCOMMA \NR
        \stopalign
        \stopformula

        where the independence property of the forward model errors has been
        used for the second identity. With this formulation, any measurements
        that give information on the state vector can be included in the
        optimal estimation procedure. This includes the a-priori knowledge,
        which might be interpreted as a \quotation{virtual measurement} having
        the identity function as a forward model and the prior state
        uncertainties as errors. In this interpretation, no prior knowledge
        is explicitly included in the model, i.e. $\PRIOR$ is constant, and the
        maximum a-posteriori solution is equivalent to a maximum likelihood
        solution.

        Give formulas for combined map and cov solution from Rodgers, this
        might actually be used for the combination of COSMO7 and Nordkette
        measurements.
        
    \stopsubsection

    \startsubsection[title={Constructing the Prior},reference=ch:construct_prior]

        Choosing the prior (=background) state. Constructing the covariance
        matrix.

    \stopsubsection

\stopsection


\startsection[title=Linear Regression]

    TODO: Introduction. Adding regressors by adding elements to $\VECY$. No
    explicit forward model or error estimate needed.  Who has done it? Problem:
    no constraint, rather a suggestion. Known elements of state vector are not
    enforced.

    In retrieval applications the data pairs are atmospheric state observations
    $x$ and simultaneous radiometer measurements $\VECY$. The atmospheric state
    usually comes from a radiosonde climatology or NWP output, while the
    radiometer measurements are often simulated with a numerical radiative
    transfer model since extensive time series of radiometer measurements are
    rare.

    The classical approach to linear regression is the least squares framework
    but the Bayesian framework provides an equivalent derivation presented
    here. The Bayesian approach results in a predictive distribution instead
    of a single target value but this distribution as an important caveat which
    is shown as well.

    Consider a model $f$ of a linear combination of $n$ fixed basis functions
    $\VECPHI = \startpmatrix[n=3] \NC \phi_1 \NC \dots \NC \phi_n \NR
    \stoppmatrix$ (a row vector) approximating a scalar
    function $t$ of an input $\VECY \in \REALS^d$

    \startformula
        t(\VECY) \approx f(\VECY, \VECW) = w_0 + \sum_{i=1}^n w_i \phi_i(\VECY)
            = \sum_{i=0}^n w_i \phi_i(\VECY) = \VECPHI(\VECY) \VECW
    \stopformula

    where an additional constant basis function $\phi_0(\VECY) = 1$ is
    introduced to $\VECPHI$ for modelling the constant term without the need to
    explicitly consider it in the following derivations. A common choice for
    basis functions in retrievals are indentity functions for individual
    components, i.e. $n = d$ and

    \startformula
        \phi_0(\VECY) = 1,~ \phi_1(\VECY) = y_1, ~ \dots, \phi_d = y_d
    \stopformula

    with $\VECY^\top = \startpmatrix[n=3] \NC y_1 \NC \dots \NC y_d \NR \stoppmatrix$,
    but basis is not restricted to linear functions\footnote{The \quote{linear}
    in \quote{linear regression} refers to the linearity in the parameter
    vector $\VECW$.}. The objective of regression is to find a
    a parameter vector $\VECW$ from a given set of $(t(\VECY), \VECY)$ pairs
    that is in some sense optimal. The Bayesian approach even results in
    an entire probability distribution for $\VECW$ which provides error
    estimates for predictions from the regression model.

    As usual, the assumption is that the deviations between the true function
    $t$ and its approximation $f$ follows a Gaussian distribution with zero
    mean and covariance $\BETAI$

    \placeformula
    \startformula
    \startalign[n=3,align={left,right,left}]
        \NC \NC \PROB{t(\VECY) - f(\VECY, \VECW) \GIVEN \beta} = \NC
                \GAUSS{t(\VECY) - f(\VECY, \VECW)}{0}{\BETAI} \NR
        \NC \Rightarrow~~ \NC \PROB{x \GIVEN \VECY, \VECW, \beta} = \NC
            \GAUSS{x}{\VECPHI(\VECY) \VECW}{\BETAI} \EQCOMMA \NR[eq:reg_indlikelihood]
    \stopalign
    \stopformula

    where $x = t(\VECY)$ is now understood as the target variable and its
    dependence on $\VECY$ is implied by the conditional variables of the
    probability distribution. Under the assumption that data pairs from
    corresponding datasets $\SETY = \{ \VECY_1, \dots, \VECY_m \}$ and $\SETX
    = \{ x_1, \dots, x_m \}$ are independent, the likelihood function of
    is given by

    \startformula
        \RLIKELIHOOD = \prod_{i=1}^{m} \GAUSS{x_i}{\VECPHI(\VECY_i) \VECW}{\BETAI}
    \stopformula
    
    using \ineq{reg_indlikelihood} or alternatively by

    \startformula
        \RLIKELIHOOD = \GAUSS{\VECX}{\MATPHI \VECW}{\BETAI \MATID}
    \stopformula
    
    with the definitions

    \startformula
        \VECX = \startpmatrix x_1 \NR \vdots\NR x_m \NR \stoppmatrix
        {\rm ~~and~~}
        \MATPHI = \startpmatrix[n=3,align={middle,middle,middle}]
            \NC \phi_0(\VECY_1) \NC \dots \NC \phi_n(\VECY_1) \NR
            \NC \vdots \NC \ddots \NC \vdots \NR
            \NC \phi_0(\VECY_m) \NC \dots \NC \phi_n(\VECY_m) \NR
        \stoppmatrix = \startpmatrix
            \VECPHI(\VECY_1) \NR \vdots \NR \VECPHI(\VECY_m) \NR
        \stoppmatrix
        \EQSTOP
    \stopformula
    
    In order to use Bayes' theorem to obtain the probability distribution of
    $\VECW$ given the data pairs, a prior assumption for the parameter vector
    is necessary. One could just assume that any value of $\VECW$ is equally
    likely since no information about $\VECW$ are available at this point,
    resulting in a maximum likelihood solution.  However, experience shows that
    it is useful to make the regression favor smaller values of $\VECW$ in
    order to obtain a model that is less prone to overfitting and numerically
    more stable. An isotropic Gaussian with mean $\VECZERO$ is therefore
    considered

    \placeformula[eq:reg_prior]
    \startformula
        \RPRIOR = \GAUSS{\VECW}{\VECZERO}{\ALPHAI \MATID}
    \stopformula

    where $\alpha$ governs the covariance of the distribution. With likelihood
    function and prior determined, Bayes' theorem reads

    \startformula
        \RPOSTERIOR = \frac{\RLIKELIHOOD \, \RPRIOR}{\RNORMALIZATION} \EQSTOP
    \stopformula

    The denominator is only a constant normalization, applying the result
    \ineq{gaussbayespost} with substitutions

    \startformula
        \{ \VECX \rightarrow \VECW,~
            \VECA \rightarrow \VECZERO,~
            \MATP \rightarrow \alpha \MATID,~
            \VECY \rightarrow \VECX,~
            \MATB \rightarrow \MATPHI,~
            \VECB \rightarrow \VECZERO,~
            \MATQ \rightarrow \beta \MATID
            \} \EQCOMMA
    \stopformula

    identified from \ineq{gaussbayesinit}, yields the desired posterior
    probability distribution for the parameter vector:

    \startsubformulas[eq:reg_posterior]
    \placesubformula
    \startformula
    \startalign[n=2,align={right,left}]
        \NC \RPOSTERIOR = \NC \GAUSS{\VECW}{\MEANVEC}{\COVMAT} \NR[eq:reg_posterior_gauss]
        \NC \COVMAT = \NC (\beta \MATPHIT \MATPHI + \alpha \MATID)^{-1} \NR[eq:reg_posterior_cov]
        \NC \MEANVEC = \NC \beta \COVMAT \MATPHIT \VECX \EQSTOP \NR[eq:reg_posterior_mean]
    \stopalign
    \stopformula
    \stopsubformulas

    Once the regression model is trained predictions for new measurements
    $\VECY$ are made by evaluating the predictive distribution

    \startformula
    \startalign[n=2,align={right,left}]
        \NC \RPREDICT = \NC
            \int_{\REALS^{n+1}} \PROB{x \GIVEN \VECY, \VECW, \beta} \,
            \RPOSTERIOR \, \diff \VECW \NR
        \NC = \NC
            \int_{\REALS^{n+1}} \GAUSS{x}{\VECPHI(\VECY) \VECW}{\BETAI} \,
                \GAUSS{\VECW}{\MEANVEC}{\COVMAT} \,\diff \VECW \NR
    \stopalign
    \stopformula
    
    which is the likelihood of the measurement given the model
    \ineq{reg_indlikelihood}, marginalized with respect to the posterior
    parameter vector distribution \ineq{reg_posterior_gauss}. With the
    substitution

    \startformula
        \{ \VECX \rightarrow \VECW,~
            \VECA \rightarrow \MEANVEC,~
            \MATP \rightarrow \COVMATI,~
            \VECY \rightarrow x,~
            \MATB \rightarrow \VECPHI(\VECY),~
            \VECB \rightarrow 0,~
            \MATQ \rightarrow \beta \}
    \stopformula

    result \ineq{gaussbayesnorm} is applicable\footnote{The reduction in
    dimension in \ineq{gaussbayesinityx} does not invalidate the result.},
    yielding

    \startformula
        \RPREDICT = \GAUSS{x}{\VECPHI(\VECY) \MEANVEC}{\BETAI + \VECPHI(\VECY) \COVMAT \VECPHIT(\VECY)}
    \stopformula

    as the predictive distribution. The prediction mean and therefore most
    probable value is the sum of products between the basis functions evaluated
    for the predictors and the the parameter vector's MAP solution, a result
    that is consistent with the expectation from a non-Bayesian approach to
    linear regression. The covariance of the predicted value is a combination
    of the uncertainty inherent in the target variable and uncertainty in the
    model parameters.

    \placefigure[top][fig:bayesian_regression]
        {Predictive distributions of Bayesian regression models trained on 12
        data points obtained from a sine signal with added Gaussian noise of
        standard deviation 0.2 (circles). The line is the mean of the
        predictive distribution, the gray and light gray shaded areas mark
        one and three standard deviations around this mean. The left and center
        models are fifth and first order polynomials respectively, the right
        model uses seven Gaussian basis functions centered between -3 and 3.}
        {\externalfigure[bayesian_regression][width=\textwidth]}

    On first sight, a full description of the predictive distribution seems
    useful for the estimation of uncertainty in predictions.  But care has to
    be taken in its interpretation because the distribution describes
    uncertainty in the model parameters but not the choice of model itself,
    i.e. the choice of basis functions. The uncertainty obtained from the
    predictive distribution is just one term in the overall error estimate and
    on its own only really useful if the chosen model can accurately describe
    the variability in the data.  Figure \in[fig:bayesian_regression]
    illustrates how the choice of basis functions affects the predictive
    distribution. The model shown in the left frame is a polynomial able
    to describe the variability of the data and gives a good fit inside the
    data domain and an appropriate uncertainty range outside the domain. The
    model in the middle is a line with just slope and offset as parameters. It
    is not only a bad fit for the data in- and outside the domain but the
    predictive distribution massively overstates its accuracy. For the given
    model this is a good fit and the parameters can be determined with high
    accuracy but considering all possible models it is a bad fit because the
    model cannot express variability sufficiently. On the right seven Gaussian
    basis functions are used, giving a good fit inside the data domain with
    appropriate uncertainty estimates. Because the basis functions are
    localized, the error estimate from the predictive distribution however
    reduces to the inherent uncertainty of the target data and no contribution
    from the model exists anymore (the covariance is dominated by $\BETAI$
    since $\VECPHI(\VECY)$ vanishes away from the basis functions).

    The treatment so far only considered multiple predictors but only a scalar
    target. For the retrieval of an atmospheric profile from radiometer
    measurements, each atmospheric layer is a target variable resulting in a
    one dimensional target vector. The common approach in this situation is to
    use the same basis functions for each target component and derive a
    parameter matrix instead of a parameter vector $\VECW$. In the derivation
    of this matrix it becomes apparent that the problem decouples into
    independent, scalar regression problems for each target component
    \cite[Bishop2006]. The results from above are therefore directly applicable
    to multiple targets if the assumption holds that the errors of each target
    variable are identically and independently distributed, an assumption that
    is reasonable for the random errors of the sensors of a radiosonde.

\stopsection


\startsection[title=Comparison of Techniques]

    Ease of use, computational efficiency, inclusion of additional information.
    Information content of output: uncertainty.

\stopsection

