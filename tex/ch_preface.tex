For me, the most difficult aspect of scientific writing is the adequate
presentation of the temporal evolution of research. Knowledge and expectations
change with time, experiments fail and not all theories work out. In the end
one has to decide which results of this continuous process make it into the
final text. Should only positive results be included to make the presentation
of the content as efficient as possible or are negative results just as useful?
Is there any value in retracing the evolution of the research process or are
only final results of interest? It seems to me that current publications are
mainly focused on the presentation of final, positive results and encountered
complications are often somewhat hidden. With that, I do not accuse any authors
of purposefully hiding information but rather want to state my view on the
general culture around the scientific publication system. I have tried to
include some (negative) experiences from the \quote{journey} that I took with
this work at appropriate locations in this thesis. Because I refrain from
writing in first person perspective in the text, I would like to provide some
personal thoughts and additional context here:

This thesis was originally about the implementation of a neural network
technique for the retrieval of temperature and humidity as an alternative to
the linear regression method which has been used in the past at the Institute
for Atmospheric and Cryospheric Sciences in Innsbruck. In the end it turned
into an implementation of the optimal estimation method. This change of topic
occured during the initial literature review. I believe that despite its
complexity this technique provides the best approach to the retrieval problem
currently known and hope to argue in favor of this belief with the following
discussions and results.  I have chosen to present all derivations in the
Bayesian framework after seeing how universally Bayes' theorem can be applied
to the construction of machine learning models and estimation problems in the
books by \cite[Bishop2006] and \cite[Downey2013]. Initially it was not planned
to also implement a radiative transfer model, but after some frustration with
the interface of MonoRTM, I decided to write a model able to provide an exact
linearization of itself. I have been fascinated by the simple but powerful idea
of automatic differentiation since I first read about it in the context of
neural network training. Judging from the short presentation of radiative
transfer in most publications, I naively assumed that the implementation of
a numerical model would be a rather simple task but I soon found that the devil
is in the details. Getting the model right took a substantial amout of time,
much more than I would have needed to set up a mature solution like ARTS, but
I learned a lot in that time and that justifies the effort for me personally.
That actually seems to be the theme of this work: unnecessarily reinventing the
wheel in order to get to know all aspects of the retrieval problem in depth.
While this is not best scientific practice, I think a Master's thesis is a good
place for such an approach without wasting much of anyone's time except
possibly mine.

Finally, I must mention that not all publications about retrievals and
radiative transfer found during my literature research have been referenced
here. This is not to purposefully hide anything but rather due to time
constraints which did not allow me to read all material with the appropriate
attention. The included references are nonetheless plentiful and should provide
a decent point of origin for anyone trying to find more articles related to
retrievals.

